{
  "id": "google-ace-certification",
  "title": "Google Associate Cloud Engineer (ACE) Practice Exam",
  "description": "This practice exam covers key concepts for the Google Associate Cloud Engineer certification.",
  "prerequisites": [
    "Basic understanding of cloud computing",
    "Familiarity with Google Cloud Platform"
  ],
  "questions": [
    {
      "id": 1,
      "question": "Every employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do?",
      "type": "single",
      "options": [
        "Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance.",
        "Ask each member of the team to generate a new SSH key pair and to send you their public key. Use a configuration management tool to deploy those keys on each instance.",
        "Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the 'compute.osAdminLogin' role to the Google group corresponding to this team.",
        "Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance."
      ],
      "correctAnswer": [
        "Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the 'compute.osAdminLogin' role to the Google group corresponding to this team."
      ],
      "hint": "Think about security best practices, individual accountability, and OS Login features in Google Cloud.",
      "explanation": "### Key Concept\nOS Login with IAM is Google Cloud's recommended way to manage SSH access to instances.\n\n### Real-world Example\nImagine a company with 50 engineers needing server access:\n- *Without OS Login*: Admin manually manages 50 SSH keys across hundreds of servers\n- *With OS Login*: Engineers simply add their SSH key to their Google account, and admin assigns one IAM role\n\n### Why it Works\n- Each person uses their own key → tracks who did what\n- IAM role controls access → easy to add/remove people\n- Automatic user management → no manual server updates\n\n🔗 Learn more: [OS Login Official Documentation](https://cloud.google.com/compute/docs/oslogin)"
    },
    {
      "id": 2,
      "question": "You need to create a custom VPC with a single subnet. The subnet's range must be as large as possible. Which range should you use?",
      "type": "single",
      "options": ["0.0.0.0/0", "10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16"],
      "correctAnswer": ["10.0.0.0/8"],
      "hint": "Consider the RFC1918 private IP address ranges and their network sizes",
      "explanation": "### Key Concept\nWhen creating VPC subnets, you must use private IP ranges defined in RFC1918, with the size determined by the CIDR notation (/8, /12, /16 etc).\n\n### Real-world Example\nComparing the available IP addresses in each range:\n- 10.0.0.0/8: ~16.7 million addresses\n- 172.16.0.0/12: ~1 million addresses\n- 192.168.0.0/16: ~65,000 addresses\n\n### Why 10.0.0.0/8 is Correct\n- Largest private range available\n- Safe for internal use - won't conflict with internet routing\n- 0.0.0.0/0 is invalid as it represents all IPv4 addresses\n\n🔗 Learn more: [VPC Network Overview](https://cloud.google.com/vpc/docs/vpc)"
    },
    {
      "id": 3,
      "question": "You want to select and configure a cost-effective solution for relational data on Google Cloud Platform. You are working with a small set of operational data in one geographic location. You need to support point-in-time recovery. What should you do?",
      "type": "single",
      "options": [
        "Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected.",
        "Select Cloud SQL (MySQL). Select the create failover replicas option.",
        "Select Cloud Spanner. Set up your instance with 2 nodes.",
        "Select Cloud Spanner. Set up your instance as multi-regional."
      ],
      "correctAnswer": [
        "Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected."
      ],
      "hint": "Consider cost-effectiveness and the minimum requirements for point-in-time recovery",
      "explanation": "### Key Concept\nPoint-in-time recovery in databases requires transaction logging, and the solution should match your data scale and budget.\n\n### Real-world Example\nComparing costs for a small business database:\n- Cloud SQL with binary logging: ~$50-100/month\n- Cloud Spanner (minimum 2 nodes): ~$900/month\n\n### Why Binary Logging is the Answer\n- Enables point-in-time recovery at minimal cost\n- Perfect for small datasets in single region\n- Cloud Spanner would be overkill (designed for global, high-scale data)\n\n🔗 Learn more: [Cloud SQL Backup Options](https://cloud.google.com/sql/docs/mysql/backup-recovery/backups)"
    },
    {
      "id": 4,
      "question": "You want to configure autohealing for network load balancing for a group of Compute Engine instances that run in multiple zones, using the fewest possible steps. You need to configure re-creation of VMs if they are unresponsive after 3 attempts of 10 seconds each. What should you do?",
      "type": "single",
      "options": [
        "Create an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy (HTTP)",
        "Create an HTTP load balancer with a backend configuration that references an existing instance group. Define a balancing mode and set the maximum RPS to 10.",
        "Create a managed instance group. Set the Autohealing health check to healthy (HTTP)",
        "Create a managed instance group. Verify that the autoscaling setting is on."
      ],
      "correctAnswer": [
        "Create a managed instance group. Set the Autohealing health check to healthy (HTTP)"
      ],
      "hint": "Think about which GCP feature specifically handles instance health and automatic recovery",
      "explanation": "### Key Concept\nManaged Instance Groups (MIG) provide built-in autohealing capabilities through health checks, automatically recreating unhealthy instances.\n\n### Real-world Example\nScenario: E-commerce website running on 10 VMs\n- *Without autohealing*: Dead VM stays down until manually fixed\n- *With MIG autohealing*: System automatically detects and replaces unhealthy VM\n\n### Why MIG with Health Check Works\n- Directly manages instance health and recreation\n- Simpler than load balancer setup\n- Health check parameters match requirements (3 attempts, 10s each)\n\n🔗 Learn more: [MIG Autohealing](https://cloud.google.com/compute/docs/instance-groups/autohealing-instances)"
    },
    {
      "id": 5,
      "question": "You are using multiple configurations for gcloud. You want to review the configured Kubernetes Engine cluster of an inactive configuration using the fewest possible steps. What should you do?",
      "type": "single",
      "options": [
        "Use gcloud config configurations describe to review the output.",
        "Use gcloud config configurations activate and gcloud config list to review the output.",
        "Use kubectl config get-contexts to review the output.",
        "Use kubectl config use-context and kubectl config view to review the output."
      ],
      "correctAnswer": [
        "Use kubectl config use-context and kubectl config view to review the output."
      ],
      "hint": "Think about which kubectl commands let you switch and view cluster configurations directly",
      "explanation": "### Key Concept\nKubectl commands directly manage Kubernetes cluster configurations, independent of gcloud configurations.\n\n### Real-world Example\nA DevOps engineer managing multiple clusters:\n- *Problem*: Need to check settings of prod cluster while working in dev\n- *Solution*:\n  1. `kubectl config use-context prod-cluster`\n  2. `kubectl config view`\n\n### Why This Method Works\n- Directly switches to target cluster context\n- Views configuration without additional tools\n- Works for both active and inactive configurations\n- Faster than using gcloud commands\n\n🔗 Learn more: [Kubernetes Contexts and Configuration](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)"
    },
    {
      "id": 6,
      "question": "Your company uses Cloud Storage to store application backup files for disaster recovery purposes. You want to follow Google's recommended practices. Which storage option should you use?",
      "type": "single",
      "options": [
        "Multi-Regional Storage",
        "Regional Storage",
        "Nearline Storage",
        "Coldline Storage"
      ],
      "correctAnswer": ["Coldline Storage"],
      "hint": "Consider access frequency and Google Cloud's data lifecycle recommendations for disaster recovery",
      "explanation": "### Key Concept\nColdline Storage is optimized for infrequently accessed data (less than once per year) while maintaining high durability.\n\n### Real-world Example\nComparing storage costs for 100TB backup:\n- Standard: $2000/month\n- Coldline: $400/month\n- Access needed only in disasters\n- Recovery time acceptable\n\n### Why Coldline is Best\n- Lowest storage cost for rarely accessed data\n- Perfect for disaster recovery backups\n- Same durability as other storage classes\n- Cost-effective for long-term retention\n\n🔗 Learn more: [Cloud Storage Classes](https://cloud.google.com/storage/docs/storage-classes)"
    },
    {
      "id": 7,
      "question": "Several employees at your company have been creating projects with Cloud Platform and paying for it with their personal credit cards, which the company reimburses. The company wants to centralize all these projects under a single, new billing account. What should you do?",
      "type": "single",
      "options": [
        "Contact cloud-billing@google.com with your bank account details and request a corporate billing account for your company.",
        "Create a ticket with Google Support and wait for their call to share your credit card details over the phone.",
        "In the Google Platform Console, go to the Resource Manage and move all projects to the root Organizarion.",
        "In the Google Cloud Platform Console, create a new billing account and set up a payment method."
      ],
      "correctAnswer": [
        "In the Google Cloud Platform Console, create a new billing account and set up a payment method."
      ],
      "hint": "Think about the self-service options available in Google Cloud Console for billing management",
      "explanation": "### Key Concept\nGoogle Cloud provides self-service billing management through the Console, allowing you to create and manage billing accounts without contacting support.\n\n### Real-world Example\nCompany XYZ situation:\n1. Current state: 10 projects on individual cards\n2. Solution steps:\n   - Create company billing account\n   - Link existing projects\n   - Remove personal cards\n\n### Why This Works\n- No need for support intervention\n- Immediate account creation\n- Self-managed billing transitions\n- Maintains project continuity\n\n🔗 Learn more: [Managing Cloud Billing Accounts](https://cloud.google.com/billing/docs/how-to/manage-billing-account)"
    },
    {
      "id": 8,
      "question": "You have an application that looks for its licensing server on the IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server. What should you do?",
      "type": "single",
      "options": [
        "Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.",
        "Reserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.",
        "Use the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.",
        "Start the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address."
      ],
      "correctAnswer": [
        "Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server."
      ],
      "hint": "Consider IP address persistence and internal network requirements",
      "explanation": "### Key Concept\nStatic internal IP addresses in GCP allow you to reserve specific IP addresses for VMs within your VPC network.\n\n### Real-world Example\nLicensing setup scenario:\n- Application expects server at 10.0.3.21\n- Steps to implement:\n  ```\n  gcloud compute addresses create license-ip \\\n    --region=us-central1 \\\n    --subnet=default \\\n    --address=10.0.3.21\n  ```\n\n### Why This Works\n- Maintains existing application config\n- Ensures IP persistence\n- Works within internal network\n- No public IP exposure needed\n\n🔗 Learn more: [Reserved Static Internal IP Addresses](https://cloud.google.com/compute/docs/ip-addresses/reserve-static-internal-ip-address)"
    },
    {
      "id": 9,
      "question": "You are deploying an application to App Engine. You want the number of instances to scale based on request rate. You need at least 3 unoccupied instances at all times. Which scaling type should you use?",
      "type": "single",
      "options": [
        "Manual Scaling with 3 instances.",
        "Basic Scaling with min_instances set to 3.",
        "Basic Scaling with max_instances set to 3.",
        "Automatic Scaling with min_idle_instances set to 3."
      ],
      "correctAnswer": ["Automatic Scaling with min_idle_instances set to 3."],
      "hint": "Think about which scaling type handles request rates and allows for idle instance configuration",
      "explanation": "### Key Concept\nApp Engine's automatic scaling manages instance count based on load while `min_idle_instances` ensures a buffer of ready instances.\n\n### Real-world Example\nE-commerce site during flash sale:\n- Traffic spikes from 100 to 1000 requests/sec\n- 3 idle instances always ready\n- Additional instances auto-created as needed\n\n### Why This Works\n- Handles varying request rates automatically\n- Maintains 3 unoccupied instances\n- Reduces response latency\n- Cost-effective scaling\n\n🔗 Learn more: [App Engine Automatic Scaling](https://cloud.google.com/appengine/docs/standard/python3/config/appref#automatic_scaling)"
    },
    {
      "id": 10,
      "question": "You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps. What should you do?",
      "type": "single",
      "options": [
        "Use gcloud iam roles copy and specify the production project as the destination project.",
        "Use gcloud iam roles copy and specify your organization as the destination organization.",
        "In the Google Cloud Platform Console, use the 'create role from role' functionality.",
        "In the Google Cloud Platform Console, use the 'create role' functionality and select all applicable permissions."
      ],
      "correctAnswer": [
        "Use gcloud iam roles copy and specify the production project as the destination project."
      ],
      "hint": "Consider which method provides direct role copying between projects with minimal steps",
      "explanation": "### Key Concept\nThe `gcloud iam roles copy` command allows direct copying of custom roles between projects in a single step.\n\n### Real-world Example\nMigrating dev to prod:\n```bash\n# Single command to copy role\ngcloud iam roles copy \\\n  --source-project=dev-project \\\n  --destination-project=prod-project \\\n  --source-role=customRole\n```\n\n### Why This Works\n- Single command operation\n- Maintains exact permissions\n- No manual recreation needed\n- Reduces human error\n\n🔗 Learn more: [Managing Custom Roles](https://cloud.google.com/iam/docs/creating-custom-roles)"
    },
    {
      "id": 11,
      "question": "You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use?",
      "type": "single",
      "options": [
        "Deployment Manager",
        "Cloud Composer",
        "Managed Instance Group",
        "Unmanaged Instance Group"
      ],
      "correctAnswer": ["Deployment Manager"],
      "hint": "Think about infrastructure as code and configuration-based deployment solutions in GCP",
      "explanation": "### Key Concept\nDeployment Manager is Google Cloud's native infrastructure as code tool that uses configuration files to provision and manage resources.\n\n### Real-world Example\nDeployment Manager allows you to:\n- Define VM specs in YAML/Python\n- Version control configurations\n- Replicate environments easily\n- Automate deployment process\n\n### Why Deployment Manager is Best\n- Native GCP solution\n- Supports complex deployments\n- Uses declarative configurations\n- Enables infrastructure as code\n- Maintains consistency across deployments\n\n🔗 Learn more: [Deployment Manager Documentation](https://cloud.google.com/deployment-manager/docs)"
    },
    {
      "id": 12,
      "question": "You have a Dockerfile that you need to deploy on Kubernetes Engine. What should you do?",
      "type": "single",
      "options": [
        "Use kubectl app deploy <dockerfilename>.",
        "Use gcloud app deploy <dockerfilename>.",
        "Create a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.",
        "Create a docker image from the Dockerfile and upload it to Cloud Storage. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file."
      ],
      "correctAnswer": [
        "Create a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file."
      ],
      "hint": "Consider the complete workflow from Dockerfile to running container in GKE",
      "explanation": "### Key Concept\nDeploying to GKE requires a proper container workflow: build image → store in registry → define deployment → apply to cluster.\n\n### Real-world Example\nTypical deployment flow:\n1. `docker build -t gcr.io/project-id/app:v1 .`\n2. `docker push gcr.io/project-id/app:v1`\n3. Create deployment.yaml:\n```yaml\napiVersion: apps/v1\nkind: Deployment\nspec:\n  containers:\n  - image: gcr.io/project-id/app:v1\n```\n4. `kubectl apply -f deployment.yaml`\n\n### Why This Works\n- Follows container best practices\n- Uses Container Registry for secure storage\n- Enables version control\n- Supports Kubernetes native deployment\n\n🔗 Learn more: [GKE Deployment Guide](https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app)"
    },
    {
      "id": 13,
      "question": "Your development team needs a new Jenkins server for their project. You need to deploy the server using the fewest steps possible. What should you do?",
      "type": "single",
      "options": [
        "Download and deploy the Jenkins Java WAR to App Engine Standard.",
        "Create a new Compute Engine instance and install Jenkins through the command line interface.",
        "Create a Kubernetes cluster on Compute Engine and create a deployment with the Jenkins Docker image.",
        "Use GCP Marketplace to launch the Jenkins solution."
      ],
      "correctAnswer": ["Use GCP Marketplace to launch the Jenkins solution."],
      "hint": "Consider which deployment method requires minimal configuration and setup steps",
      "explanation": "### Key Concept\nGCP Marketplace provides pre-configured, production-ready solutions that can be deployed with just a few clicks.\n\n### Real-world Example\nJenkins deployment comparison:\n1. Manual install: 15+ steps, 1-2 hours\n2. Kubernetes setup: 10+ steps, 1 hour\n3. Marketplace: 3-4 clicks, 5-10 minutes\n\n### Why Marketplace is Best\n- Pre-configured for GCP\n- Security settings pre-applied\n- Automatic updates available\n- Minimal configuration needed\n- Production-ready setup\n\n🔗 Learn more: [GCP Marketplace Solutions](https://cloud.google.com/marketplace/docs)"
    },
    {
      "id": 14,
      "question": "You need to update a deployment in Deployment Manager without any resource downtime in the deployment. Which command should you use?",
      "type": "single",
      "options": [
        "gcloud deployment-manager deployments create --config <deployment-config-path>",
        "gcloud deployment-manager deployments update --config <deployment-config-path>",
        "gcloud deployment-manager resources create --config <deployment-config-path>",
        "gcloud deployment-manager resources update --config <deployment-config-path>"
      ],
      "correctAnswer": [
        "gcloud deployment-manager deployments update --config <deployment-config-path>"
      ],
      "hint": "Consider which command allows for in-place updates of existing deployments",
      "explanation": "### Key Concept\nThe `deployments update` command allows for in-place updates of existing deployments while maintaining resource availability.\n\n### Real-world Example\nUpdating a web application deployment:\n```bash\n# Original deployment\ngcloud deployment-manager deployments create web-app --config config.yaml\n\n# Update with zero downtime\ngcloud deployment-manager deployments update web-app --config config-v2.yaml\n```\n\n### Why This Works\n- Updates existing deployment\n- Maintains resource availability\n- Follows incremental update pattern\n- Preserves deployment state\n- Allows rollback if needed\n\n🔗 Learn more: [Deployment Manager Updates](https://cloud.google.com/deployment-manager/docs/deployments/updating-deployments)"
    },
    {
      "id": 15,
      "question": "You need to run an important query in BigQuery but expect it to return a lot of records. You want to find out how much it will cost to run the query. You are using on-demand pricing. What should you do?",
      "type": "single",
      "options": [
        "Arrange to switch to Flat-Rate pricing for this query, then move back to on-demand.",
        "Use the command line to run a dry run query to estimate the number of bytes read. Then convert that bytes estimate to dollars using the Pricing Calculator.",
        "Use the command line to run a dry run query to estimate the number of bytes returned. Then convert that bytes estimate to dollars using the Pricing Calculator.",
        "Run a select count (*) to get an idea of how many records your query will look through. Then convert that number of rows to dollars using the Pricing Calculator."
      ],
      "correctAnswer": [
        "Use the command line to run a dry run query to estimate the number of bytes read. Then convert that bytes estimate to dollars using the Pricing Calculator."
      ],
      "hint": "Consider how BigQuery pricing is calculated and which method provides accurate cost estimation without running the actual query",
      "explanation": "### Key Concept\nBigQuery on-demand pricing is based on the amount of data processed (bytes read) during query execution, not the result size.\n\n### Real-world Example\nTo estimate query cost:\n1. Use dry run: bq query --dry_run 'SELECT * FROM dataset.table'\n2. Get processed bytes estimate\n3. Calculate cost: processed_bytes * $5/TB\n\n### Why This Method Works\n- Provides accurate cost estimate\n- No actual query execution\n- No charges incurred\n- Based on actual pricing model\n- Works for any query size\n\n🔗 Learn more: [BigQuery Pricing](https://cloud.google.com/bigquery/pricing)"
    },
    {
      "id": 16,
      "question": "You have a single binary application that you want to run on Google Cloud Platform. You decided to automatically scale the application based on underlying infrastructure CPU usage. Your organizational policies require you to use virtual machines directly. You need to ensure that the application scaling is operationally efficient and completed as quickly as possible. What should you do?",
      "type": "single",
      "options": [
        "Create a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.",
        "Create an instance template, and use the template in a managed instance group with autoscaling configured.",
        "Create an instance template, and use the template in a managed instance group that scales up and down based on the time of day.",
        "Use a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring."
      ],
      "correctAnswer": [
        "Create an instance template, and use the template in a managed instance group with autoscaling configured."
      ],
      "hint": "Consider the requirements: direct VM use, CPU-based scaling, operational efficiency",
      "explanation": "### Key Concept\nManaged Instance Groups (MIG) with autoscaling provide native, efficient VM-based scaling based on CPU utilization.\n\n### Real-world Example\nImplementation steps:\n1. Create template with application\n2. Configure MIG autoscaling:\n   - Target CPU utilization: 75%\n   - Min instances: 2\n   - Max instances: 10\n\n### Why This Works\n- Uses VMs directly (policy compliant)\n- Native GCP autoscaling\n- CPU-based scaling\n- No third-party dependencies\n- Quick and efficient scaling\n\n🔗 Learn more: [MIG Autoscaling](https://cloud.google.com/compute/docs/autoscaler)"
    },
    {
      "id": 17,
      "question": "You are analyzing Google Cloud Platform service costs from three separate projects. You want to use this information to create service cost estimates by service type, daily and monthly, for the next six months using standard query syntax. What should you do?",
      "type": "single",
      "options": [
        "Export your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.",
        "Export your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.",
        "Export your transactions to a local file, and perform analysis with a desktop tool.",
        "Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis."
      ],
      "correctAnswer": [
        "Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis."
      ],
      "hint": "Think about tools that are native to Google Cloud Platform and support large-scale data analysis.",
      "explanation": "### Key Concept\nBigQuery is a powerful tool for analyzing large datasets with SQL queries.\n\n### Real-world Example\nImplementation steps:\n1. Export billing data to BigQuery.\n2. Create a BigQuery dataset with tables for daily and monthly costs.\n3. Write SQL queries to calculate and project costs for the next six months.\n\n### Why This Works\n- BigQuery supports efficient, scalable querying.\n- Native to GCP, seamless integration.\n- Capable of handling large datasets and providing detailed cost analysis.\n\n🔗 Learn more: [BigQuery Billing Export](https://cloud.google.com/billing/docs/how-to/export-data-bigquery)"
    },
    {
      "id": 18,
      "question": "You need to set up a policy so that videos stored in a specific Cloud Storage Regional bucket are moved to Coldline after 90 days, and then deleted after one year from their creation. How should you set up the policy?",
      "type": "single",
      "options": [
        "Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 275 days (365-90).",
        "Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 365 days.",
        "Use gsutil rewrite and set the Delete action to 275 days (365-90).",
        "Use gsutil rewrite and set the Delete action to 365 days."
      ],
      "correctAnswer": [
        "Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 365 days."
      ],
      "hint": "Consider how object age is calculated and how lifecycle actions are executed independently.",
      "explanation": "### Key Concept\nCloud Storage Lifecycle Management allows you to automatically transition objects between storage classes and delete them based on their age from creation.\n\n### Real-world Example\nLifecycle configuration:\n```json\n{\n  \"lifecycle\": {\n    \"rule\": [\n      {\n        \"action\": {\n          \"type\": \"SetStorageClass\",\n          \"storageClass\": \"COLDLINE\"\n        },\n        \"condition\": {\n          \"age\": 90\n        }\n      },\n      {\n        \"action\": {\n          \"type\": \"Delete\"\n        },\n        \"condition\": {\n          \"age\": 365\n        }\n      }\n    ]\n  }\n}\n```\n\n### Why This Works\n- Age calculated from creation date.\n- Actions execute independently.\n- Automatic management.\n- No manual intervention needed.\n\n🔗 Learn more: [Object Lifecycle Management](https://cloud.google.com/storage/docs/lifecycle)"
    },
    {
      "id": 19,
      "question": "You have a Linux VM that must connect to Cloud SQL. You created a service account with the appropriate access rights. You want to make sure that the VM uses this service account instead of the default Compute Engine service account. What should you do?",
      "type": "single",
      "options": [
        "When creating the VM via the web console, specify the service account under the 'Identity and API Access' section.",
        "Download a JSON Private Key for the service account. On the Project Metadata, add that JSON as the value for the key compute-engine-service-account.",
        "Download a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-engine-service-account.",
        "Download a JSON Private Key for the service account. After creating the VM, ssh into the VM and save the JSON under ~/.gcloud/compute-engine-service-account.json."
      ],
      "correctAnswer": [
        "When creating the VM via the web console, specify the service account under the 'Identity and API Access' section."
      ],
      "hint": "Focus on assigning the correct service account during the VM creation process.",
      "explanation": "### Key Concept\nAssigning the service account during VM creation ensures the VM uses the specified service account for authentication and access.\n\n### Real-world Example\nImplementation steps:\n1. Create or select the appropriate service account with necessary permissions.\n2. During VM creation via the web console, go to the 'Identity and API Access' section.\n3. Specify the desired service account to be used by the VM.\n\n### Why This Works\n- Ensures the VM uses the correct service account from the start.\n- Avoids manual configuration steps post-creation.\n\n🔗 Learn more: [Creating and Enabling Service Accounts](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances)"
    },
    {
      "id": 20,
      "question": "You created an instance of SQL Server 2017 on Compute Engine to test features in the new version. You want to connect to this instance using the fewest number of steps. What should you do?",
      "type": "single",
      "options": [
        "Install a RDP client on your desktop. Verify that a firewall rule for port 3389 exists.",
        "Install a RDP client in your desktop. Set a Windows username and password in the GCP Console. Use the credentials to log in to the instance.",
        "Set a Windows password in the GCP Console. Verify that a firewall rule for port 22 exists. Click the RDP button in the GCP Console and supply the credentials to log in.",
        "Set a Windows username and password in the GCP Console. Verify that a firewall rule for port 3389 exists. Click the RDP button in the GCP Console, and supply the credentials to log in."
      ],
      "correctAnswer": [
        "Set a Windows username and password in the GCP Console. Verify that a firewall rule for port 3389 exists. Click the RDP button in the GCP Console, and supply the credentials to log in."
      ],
      "hint": "Focus on the GCP Console's built-in tools and Windows instance requirements",
      "explanation": "### Key Concept\nGCP provides integrated tools for Windows instances: credential management via the console, RDP access via the web interface, and automated firewall setup for default protocols.\n\n### Real-world Example\nWorkflow:\n1. Set username/password in Compute Engine → VM instances → instance details\n2. Check firewall rules (port 3389 for RDP)\n3. Click \"RDP\" button in GCP Console\n\n### Why This Works\n- No external RDP client installation needed\n- Uses built-in browser-based RDP\n- Automated credential injection\n- Ensures proper firewall configuration\n- Minimal manual configuration\n\n🔗 Learn more: [Connecting to Windows Instances](https://cloud.google.com/compute/docs/instances/connecting-to-windows)"
    },
    {
      "id": 21,
      "question": "You have one GCP account running in your default region and zone and another account running in a non-default region and zone. You want to start a new Compute Engine instance in these two Google Cloud Platform accounts using the command line interface. What should you do?",
      "type": "single",
      "options": [
        "Create two configurations using gcloud config configurations create [NAME]. Run gcloud config configurations activate [NAME] to switch between accounts when running the commands to start the Compute Engine instances.",
        "Create two configurations using gcloud config configurations create [NAME]. Run gcloud configurations list to start the Compute Engine instances.",
        "Activate two configurations using gcloud configurations activate [NAME]. Run gcloud config list to start the Compute Engine instances.",
        "Activate two configurations using gcloud configurations activate [NAME]. Run gcloud configurations list to start the Compute Engine instances."
      ],
      "correctAnswer": [
        "Create two configurations using gcloud config configurations create [NAME]. Run gcloud config configurations activate [NAME] to switch between accounts when running the commands to start the Compute Engine instances."
      ],
      "hint": "Consider how to manage multiple GCP configurations and easily switch between them using the command line.",
      "explanation": "### Key Concept\nUsing `gcloud config configurations` allows you to create and switch between multiple configurations for different GCP accounts.\n\n### Real-world Example\nImplementation steps:\n1. Create two configurations using `gcloud config configurations create [NAME]`.\n2. Activate the desired configuration using `gcloud config configurations activate [NAME]`.\n3. Run the necessary commands to start Compute Engine instances in the active configuration.\n\n### Why This Works\n- Allows for easy management of multiple GCP accounts and regions.\n- Simplifies the process of switching between configurations.\n\n🔗 Learn more: [gcloud config configurations](https://cloud.google.com/sdk/gcloud/reference/config/configurations)"
    },
    {
      "id": 22,
      "question": "You significantly changed a complex Deployment Manager template and want to confirm that the dependencies of all defined resources are properly met before committing it to the project. You want the most rapid feedback on your changes. What should you do?",
      "type": "single",
      "options": [
        "Use granular logging statements within a Deployment Manager template authored in Python.",
        "Monitor activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console.",
        "Execute the Deployment Manager template against a separate project with the same configuration, and monitor for failures.",
        "Execute the Deployment Manager template using the --preview option in the same project, and observe the state of interdependent resources."
      ],
      "correctAnswer": [
        "Execute the Deployment Manager template using the --preview option in the same project, and observe the state of interdependent resources."
      ],
      "hint": "Consider testing options that provide immediate feedback within the same project environment.",
      "explanation": "### Key Concept\nThe `--preview` option in Deployment Manager allows you to test template changes and resource dependencies without actually deploying them, providing quick feedback.\n\n### Real-world Example\nImplementation steps:\n1. Use `gcloud deployment-manager deployments update [DEPLOYMENT_NAME] --config [TEMPLATE] --preview` to test changes.\n2. Review the output to see if resource dependencies are met and if there are any errors.\n\n### Why This Works\n- Provides rapid feedback on template changes.\n- No need to deploy to a separate project.\n- Allows observation of interdependent resources without committing changes.\n\n🔗 Learn more: [Deployment Manager Preview](https://cloud.google.com/deployment-manager/docs/deployments/updating-deployments#previewing_an_update)"
    },
    {
      "id": 23,
      "question": "You are building a pipeline to process time-series data. Which Google Cloud Platform services should you put in boxes 1, 2, 3, and 4?\n\n![Pipeline Diagram](https://www.examtopics.com/assets/media/exam-media/04338/0001200001.jpg)",
      "type": "single",
      "options": [
        "Cloud Pub/Sub, Cloud Dataflow, Cloud Datastore, BigQuery",
        "Firebase Messages, Cloud Pub/Sub, Cloud Spanner, BigQuery",
        "Cloud Pub/Sub, Cloud Storage, BigQuery, Cloud Bigtable",
        "Cloud Pub/Sub, Cloud Dataflow, Cloud Bigtable, BigQuery"
      ],
      "correctAnswer": [
        "Cloud Pub/Sub, Cloud Dataflow, Cloud Bigtable, BigQuery"
      ],
      "hint": "Think about the data flow and processing capabilities required for time-series data.",
      "explanation": "### Key Concept\nCloud Pub/Sub and Cloud Dataflow are used for data ingestion and processing, Cloud Bigtable for storing time-series data, and BigQuery for analysis.\n\n### Real-world Example\nImplementation steps:\n1. Use Cloud Pub/Sub to ingest real-time data.\n2. Use Cloud Dataflow to process and transform the data.\n3. Store the processed data in Cloud Bigtable for scalable storage.\n4. Use BigQuery to analyze and query the data.\n\n### Why This Works\n- Cloud Pub/Sub provides reliable, real-time messaging.\n- Cloud Dataflow offers powerful data processing capabilities.\n- Cloud Bigtable is optimized for time-series data storage.\n- BigQuery provides efficient analysis and querying.\n\n🔗 Learn more: [Bigtable for Time-Series Data](https://cloud.google.com/bigtable/docs/time-series-data)"
    },
    {
      "id": 24,
      "question": "You have a project for your App Engine application that serves a development environment. The required testing has succeeded and you want to create a new project to serve as your production environment. What should you do?",
      "type": "single",
      "options": [
        "Use gcloud to create the new project, and then deploy your application to the new project.",
        "Use gcloud to create the new project and to copy the deployed application to the new project.",
        "Create a Deployment Manager configuration file that copies the current App Engine deployment into a new project.",
        "Deploy your application again using gcloud and specify the project parameter with the new project name to create the new project."
      ],
      "correctAnswer": [
        "Use gcloud to create the new project, and then deploy your application to the new project."
      ],
      "hint": "Focus on the standard procedure for creating a new project and deploying an application using gcloud.",
      "explanation": "### Key Concept\nUsing `gcloud` to create a new project and deploying your application ensures a clean and standardized setup for your production environment.\n\n### Real-world Example\nImplementation steps:\n1. Use `gcloud projects create [PROJECT_ID]` to create the new project.\n2. Deploy the application to the new project using `gcloud app deploy --project [NEW_PROJECT_ID]`.\n\n### Why This Works\n- Ensures a fresh and isolated environment for production.\n- Follows standard procedures using gcloud commands.\n\n🔗 Learn more: [Creating and Managing Projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)"
    },
    {
      "id": 25,
      "question": "You need to configure IAM access audit logging in BigQuery for external auditors. You want to follow Google-recommended practices. What should you do?",
      "type": "single",
      "options": [
        "Add the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.",
        "Add the auditors group to two new custom IAM roles.",
        "Add the auditor user accounts to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.",
        "Add the auditor user accounts to two new custom IAM roles."
      ],
      "correctAnswer": [
        "Add the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles."
      ],
      "hint": "Consider how predefined IAM roles can be used to grant necessary permissions efficiently.",
      "explanation": "### Key Concept\nUsing predefined IAM roles ensures that auditors have the necessary permissions without needing to create custom roles.\n\n### Real-world Example\nImplementation steps:\n1. Create an auditors group and add external auditor user accounts to this group.\n2. Assign the 'logging.viewer' and 'bigQuery.dataViewer' IAM roles to the auditors group to enable audit logging and data viewing in BigQuery.\n\n### Why This Works\n- Predefined roles simplify the process of assigning permissions.\n- Ensures that auditors have only the permissions they need, following the principle of least privilege.\n\n🔗 Learn more: [IAM Roles and Permissions](https://cloud.google.com/iam/docs/understanding-roles)"
    },
    {
      "id": 26,
      "question": "You need to set up permissions for a set of Compute Engine instances to enable them to write data into a particular Cloud Storage bucket. You want to follow Google-recommended practices. What should you do?",
      "type": "single",
      "options": [
        "Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/devstorage.write_only'.",
        "Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/cloud-platform'.",
        "Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket.",
        "Create a service account and add it to the IAM role 'storage.objectAdmin' for that bucket."
      ],
      "correctAnswer": [
        "Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket."
      ],
      "hint": "Consider the principle of least privilege and modern IAM best practices",
      "explanation": "### Key Concept\nGoogle recommends using IAM roles over access scopes, following the principle of least privilege.\n\n### Real-world Example\nImplementation steps:\n```bash\n# Create service account\ngcloud iam service-accounts create my-sa\n\n# Assign role to bucket\ngcloud storage buckets add-iam-policy-binding gs://my-bucket \\\n    --member=serviceAccount:my-sa@project.iam.gserviceaccount.com \\\n    --role=roles/storage.objectCreator\n```\n\n### Why This Works\n- Follows least privilege principle\n- Uses modern IAM controls\n- Granular bucket-level permissions\n- Better audit capabilities\n- More secure than scopes\n\n🔗 Learn more: [Cloud Storage IAM](https://cloud.google.com/storage/docs/access-control/iam-roles)"
    },
    {
      "id": 27,
      "question": "You have sensitive data stored in three Cloud Storage buckets and have enabled data access logging. You want to verify activities for a particular user for these buckets, using the fewest possible steps. You need to verify the addition of metadata labels and which files have been viewed from those buckets. What should you do?",
      "type": "single",
      "options": [
        "Using the GCP Console, filter the Activity log to view the information.",
        "Using the GCP Console, filter the Stackdriver log to view the information.",
        "View the bucket in the Storage section of the GCP Console.",
        "Create a trace in Stackdriver to view the information."
      ],
      "correctAnswer": [
        "Using the GCP Console, filter the Stackdriver log to view the information."
      ],
      "hint": "Consider which logging service captures detailed data access activities and provides easy filtering",
      "explanation": "### Key Concept\nStackdriver (now Cloud Logging) provides comprehensive logging for Cloud Storage data access, including metadata changes and object access events.\n\n### Real-world Example\nLog query workflow:\n1. Open Cloud Logging\n2. Filter by:\n   - Resource Type: 'Cloud Storage'\n   - Buckets: [your 3 bucket names]\n   - User: [specific user]\n   - Activity: 'storage.objects.get', 'storage.objects.metadata.update'\n\n### Why This Works\n- Single interface for all logs\n- Powerful filtering capabilities\n- Real-time log viewing\n- Includes metadata changes\n- Shows object access events\n\n🔗 Learn more: [Cloud Storage Logging](https://cloud.google.com/storage/docs/audit-logs)"
    },
    {
      "id": 28,
      "question": "You are the project owner of a GCP project and want to delegate control to colleagues to manage buckets and files in Cloud Storage. You want to follow Google-recommended practices. Which IAM roles should you grant your colleagues?",
      "type": "single",
      "options": [
        "Project Editor",
        "Storage Admin",
        "Storage Object Admin",
        "Storage Object Creator"
      ],
      "correctAnswer": ["Storage Admin"],
      "hint": "Consider the principle of least privilege while ensuring sufficient permissions for complete storage management",
      "explanation": "### Key Concept\nThe Storage Admin role (roles/storage.admin) provides full control over buckets and objects while following the principle of least privilege.\n\n### Real-world Example\nStorage Admin permissions include:\n- Create/delete buckets\n- Manage objects\n- Set IAM policies\n- Configure bucket properties\n\nCommand:\n```bash\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n    --member=user:colleague@domain.com \\\n    --role=roles/storage.admin\n```\n\n### Why This Works\n- Complete storage management\n- Doesn't grant unnecessary project access\n- Follows least privilege principle\n- Maintains security boundaries\n- Proper access separation\n\n🔗 Learn more: [Cloud Storage IAM Roles](https://cloud.google.com/storage/docs/access-control/iam-roles)"
    },
    {
      "id": 29,
      "question": "You have an object in a Cloud Storage bucket that you want to share with an external company. The object contains sensitive data. You want access to the content to be removed after four hours. The external company does not have a Google account to which you can grant specific user-based access privileges. You want to use the most secure method that requires the fewest steps. What should you do?",
      "type": "single",
      "options": [
        "Create a signed URL with a four-hour expiration and share the URL with the company.",
        "Set object access to 'public' and use object lifecycle management to remove the object after four hours.",
        "Configure the storage bucket as a static website and furnish the object's URL to the company. Delete the object from the storage bucket after four hours.",
        "Create a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours have passed."
      ],
      "correctAnswer": [
        "Create a signed URL with a four-hour expiration and share the URL with the company."
      ],
      "hint": "Consider security requirements and time-limited access needs",
      "explanation": "### Key Concept\nSigned URLs provide time-limited access to Cloud Storage objects without requiring Google accounts or changing object permissions.\n\n### Real-world Example\nCreate signed URL:\n```bash\ngsutil signurl -d 4h service-account-key.json \\\n    gs://bucket-name/object-name\n```\n\n### Why This Works\n- Automatic expiration after 4 hours\n- No public access required\n- No Google account needed\n- Maintains object security\n- Single step solution\n\n### Security Benefits\n- Time-limited access\n- No permanent permission changes\n- URL becomes invalid after expiration\n- Audit logging available\n\n🔗 Learn more: [Signed URLs](https://cloud.google.com/storage/docs/access-control/signed-urls)"
    },
    {
      "id": 30,
      "question": "You are creating a Google Kubernetes Engine (GKE) cluster with a cluster autoscaler feature enabled. You need to make sure that each node of the cluster will run a monitoring pod that sends container metrics to a third-party monitoring solution. What should you do?",
      "type": "single",
      "options": [
        "Deploy the monitoring pod in a StatefulSet object.",
        "Deploy the monitoring pod in a DaemonSet object.",
        "Reference the monitoring pod in a Deployment object.",
        "Reference the monitoring pod in a cluster initializer at the GKE cluster creation time."
      ],
      "correctAnswer": ["Deploy the monitoring pod in a DaemonSet object."],
      "hint": "Consider which Kubernetes object ensures a pod runs on every node in the cluster",
      "explanation": "### Key Concept\nDaemonSet ensures that a copy of a pod runs on all (or selected) nodes in a cluster, even as nodes are added or removed by the autoscaler.\n\n### Real-world Example\nDaemonSet YAML:\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: monitoring-agent\nspec:\n  selector:\n    matchLabels:\n      app: monitoring-agent\n  template:\n    metadata:\n      labels:\n        app: monitoring-agent\n    spec:\n      containers:\n      - name: monitoring-agent\n        image: monitoring-image:latest\n```\n\n### Why DaemonSet Works Best\n- Automatically runs on new nodes\n- Handles cluster scaling\n- One pod per node guaranteed\n- Works with autoscaling\n- Perfect for monitoring agents\n\n🔗 Learn more: [Kubernetes DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)"
    },
    {
      "id": 31,
      "question": "You want to send and consume Cloud Pub/Sub messages from your App Engine application. The Cloud Pub/Sub API is currently disabled. You will use a service account to authenticate your application to the API. You want to make sure your application can use Cloud Pub/Sub. What should you do?",
      "type": "single",
      "options": [
        "Enable the Cloud Pub/Sub API in the API Library on the GCP Console.",
        "Rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it.",
        "Use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed.",
        "Grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/Sub."
      ],
      "correctAnswer": [
        "Enable the Cloud Pub/Sub API in the API Library on the GCP Console."
      ],
      "hint": "Consider the most direct and reliable method to enable a Google Cloud API",
      "explanation": "### Key Concept\nAPIs must be explicitly enabled in a project before they can be used, regardless of service account permissions.\n\n### Real-world Example\nEnabling API steps:\n1. Navigate to APIs & Services\n2. Search for 'Cloud Pub/Sub API'\n3. Click Enable\n\nOR via gcloud:\n```bash\ngcloud services enable pubsub.googleapis.com\n```\n\n### Why This Works\n- Explicit API enablement\n- Immediate availability\n- Clear activation status\n- Required before any usage\n- Independent of permissions\n\n### Common Pitfalls Avoided\n- API calls failing silently\n- Deployment delays\n- Permission confusion\n\n🔗 Learn more: [Enabling Google Cloud APIs](https://cloud.google.com/apis/docs/getting-started)"
    },
    {
      "id": 32,
      "question": "You need to monitor resources that are distributed over different projects in Google Cloud Platform. You want to consolidate reporting under the same Stackdriver Monitoring dashboard. What should you do?",
      "type": "single",
      "options": [
        "Use Shared VPC to connect all projects, and link Stackdriver to one of the projects.",
        "For each project, create a Stackdriver account. In each project, create a service account for that project and grant it the role of Stackdriver Account Editor in all other projects.",
        "Configure a single Stackdriver account, and link all projects to the same account.",
        "Configure a single Stackdriver account for one of the projects. In Stackdriver, create a Group and add the other project names as criteria for that Group."
      ],
      "correctAnswer": [
        "Configure a single Stackdriver account, and link all projects to the same account."
      ],
      "hint": "Consider the most straightforward way to centralize monitoring across multiple projects",
      "explanation": "### Key Concept\nCloud Monitoring (formerly Stackdriver) allows multiple projects to be monitored from a single workspace.\n\n### Real-world Example\nSetup steps:\n1. Create Monitoring workspace in one project\n2. Add additional projects:\n```bash\n# From console: 'Add GCP Projects' in workspace settings\n# OR via API:\ngcloud monitoring workspace projects add PROJECT_ID\n```\n\n### Why This Works\n- Centralized monitoring\n- Single dashboard view\n- No complex permissions needed\n- Efficient resource usage\n- Simple configuration\n\n### Benefits\n- Unified alerting\n- Cross-project visibility\n- Consolidated metrics\n- Simplified management\n\n🔗 Learn more: [Cloud Monitoring Workspaces](https://cloud.google.com/monitoring/workspaces/)"
    },
    {
      "id": 33,
      "question": "You are deploying an application to a Compute Engine VM in a managed instance group. The application must be running at all times, but only a single instance of the VM should run per GCP project. How should you configure the instance group?",
      "type": "single",
      "options": [
        "Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 1.",
        "Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 1.",
        "Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 2.",
        "Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 2."
      ],
      "correctAnswer": [
        "Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 1."
      ],
      "hint": "Consider high availability requirements while maintaining exactly one instance",
      "explanation": "### Key Concept\nAutoscaling with min=max=1 ensures automatic recovery while maintaining exactly one instance.\n\n### Real-world Example\nMIG configuration:\n```yaml\nresource: compute.v1.instanceGroupManager\nproperties:\n  baseInstanceName: single-instance\n  targetSize: 1\n  autoScaler:\n    autoscalingPolicy:\n      minNumReplicas: 1\n      maxNumReplicas: 1\n```\n\n### Why This Works\n- Ensures exactly one instance\n- Automatic recovery if instance fails\n- Maintains high availability\n- No manual intervention needed\n\n### Additional Benefits\n- Health checking\n- Automatic replacement\n- Self-healing\n- Zero downtime updates\n\n🔗 Learn more: [MIG Configuration](https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances)"
    },
    {
      "id": 34,
      "question": "You want to verify the IAM users and roles assigned within a GCP project named my-project. What should you do?",
      "type": "single",
      "options": [
        "Run gcloud iam roles list. Review the output section.",
        "Run gcloud iam service-accounts list. Review the output section.",
        "Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles.",
        "Navigate to the project and then to the Roles section in the GCP Console. Review the roles and status."
      ],
      "correctAnswer": [
        "Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles."
      ],
      "hint": "Consider which method provides the most comprehensive and user-friendly view of IAM assignments",
      "explanation": "### Key Concept\nThe IAM section in GCP Console provides a complete, visual overview of all IAM bindings in a project.\n\n### Real-world Example\nConsole navigation path:\n1. Select project 'my-project'\n2. Navigate to IAM & Admin → IAM\n3. View table showing:\n   - Members (users, groups, service accounts)\n   - Roles assigned\n   - Inheritance information\n\n### Why This Works\n- Visual interface\n- Complete information\n- Easy to understand\n- Shows inherited permissions\n- Real-time updates\n\n### Alternative CLI Method\n```bash\ngcloud projects get-iam-policy my-project\n```\n\n🔗 Learn more: [Viewing IAM Policies](https://cloud.google.com/iam/docs/viewing-grantable-roles)"
    },
    {
      "id": 35,
      "question": "You need to create a new billing account and then link it with an existing Google Cloud Platform project. What should you do?",
      "type": "single",
      "options": [
        "Verify that you are Project Billing Manager for the GCP project. Update the existing project to link it to the existing billing account.",
        "Verify that you are Project Billing Manager for the GCP project. Create a new billing account and link the new billing account to the existing project.",
        "Verify that you are Billing Administrator for the billing account. Create a new project and link the new project to the existing billing account.",
        "Verify that you are Billing Administrator for the billing account. Update the existing project to link it to the existing billing account."
      ],
      "correctAnswer": [
        "Verify that you are Project Billing Manager for the GCP project. Create a new billing account and link the new billing account to the existing project."
      ],
      "hint": "Consider the required permissions for both creating a billing account and linking it to a project",
      "explanation": "### Key Concept\nProject Billing Manager role allows both creating billing accounts and linking them to projects.\n\n### Real-world Example\nRequired steps:\n```bash\n# 1. Check permissions\ngcloud projects get-iam-policy PROJECT_ID | grep billing\n\n# 2. Create billing account (via Console)\n\n# 3. Link billing account\ngcloud billing projects link PROJECT_ID \\\n    --billing-account=BILLING_ACCOUNT_ID\n```\n\n### Why This Works\n- Proper permission level\n- Correct sequence of actions\n- Maintains project integrity\n- Follows GCP best practices\n\n### Required Permissions\n- roles/billing.projectManager\n- roles/billing.admin (for creating account)\n\n🔗 Learn more: [Managing Billing Access](https://cloud.google.com/billing/docs/how-to/billing-access)"
    },
    {
      "id": 36,
      "question": "You have one project called proj-sa where you manage all your service accounts. You want to be able to use a service account from this project to take snapshots of VMs running in another project called proj-vm. What should you do?",
      "type": "single",
      "options": [
        "Download the private key from the service account, and add it to each VMs custom metadata.",
        "Download the private key from the service account, and add the private key to each VM's SSH keys.",
        "Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm.",
        "When creating the VMs, set the service account's API scope for Compute Engine to read/write."
      ],
      "correctAnswer": [
        "Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm."
      ],
      "hint": "Consider cross-project service account access and the principle of least privilege",
      "explanation": "### Key Concept\nService accounts can be granted IAM roles in other projects to enable cross-project access.\n\n### Real-world Example\nImplementation:\n```bash\n# Grant role to service account\ngcloud projects add-iam-policy-binding proj-vm \\\n    --member=serviceAccount:sa-name@proj-sa.iam.gserviceaccount.com \\\n    --role=roles/compute.storageAdmin\n```\n\n### Why This Works\n- Proper cross-project access\n- Secure role-based auth\n- No key management needed\n- Following best practices\n- Clean audit trail\n\n### Security Benefits\n- No private key exposure\n- Centralized IAM control\n- Revocable permissions\n- Least privilege principle\n\n🔗 Learn more: [Cross-project Service Account Access](https://cloud.google.com/iam/docs/granting-roles-to-service-accounts)"
    },
    {
      "id": 37,
      "question": "You created a Google Cloud Platform project with an App Engine application inside the project. You initially configured the application to be served from the us-central region. Now you want the application to be served from the asia-northeast1 region. What should you do?",
      "type": "single",
      "options": [
        "Change the default region property setting in the existing GCP project to asia-northeast1.",
        "Change the region property setting in the existing App Engine application from us-central to asia-northeast1.",
        "Create a second App Engine application in the existing GCP project and specify asia-northeast1 as the region to serve your application.",
        "Create a new GCP project and create an App Engine application inside this new project. Specify asia-northeast1 as the region to serve your application."
      ],
      "correctAnswer": [
        "Create a new GCP project and create an App Engine application inside this new project. Specify asia-northeast1 as the region to serve your application."
      ],
      "hint": "Consider App Engine's region limitations and immutability",
      "explanation": "### Key Concept\nApp Engine's region cannot be changed after creation.\n\n### Real-world Example\nImplementation steps:\n1. Create new project:\n```bash\ngcloud projects create new-project-id\n```\n2. Create App Engine app:\n```bash\ngcloud app create --region=asia-northeast1\n```\n\n### Why This Works\n- App Engine region is immutable \n- New project provides clean slate\n\n🔗 Learn more: [App Engine Locations](https://cloud.google.com/appengine/docs/locations)"
    },
    {
      "id": 38,
      "question": "You need to grant access for three users so that they can view and edit table data on a Cloud Spanner instance. What should you do?",
      "type": "single",
      "options": [
        "Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to the role.",
        "Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to a new group. Add the group to the role.",
        "Run gcloud iam roles describe roles/spanner.viewer --project my-project. Add the users to the role.",
        "Run gcloud iam roles describe roles/spanner.viewer --project my-project. Add the users to a new group. Add the group to the role."
      ],
      "correctAnswer": [
        "Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to a new group. Add the group to the role."
      ],
      "hint": "Consider best practices for managing multiple user permissions and the required access level",
      "explanation": "### Key Concept\nUsing groups for IAM role assignment is a best practice, and spanner.databaseUser provides the necessary read/write permissions.\n\n### Real-world Example\nImplementation steps:\n```bash\n# 1. Verify role permissions\ngcloud iam roles describe roles/spanner.databaseUser\n\n# 2. Create group\ngcloud identity groups create spanner-users --display-name=\"Spanner Users\"\n\n# 3. Add users to group\ngcloud identity groups memberships add --group-email=spanner-users@domain.com \\\n    --member-email=user1@domain.com\n\n# 4. Assign role to group\ngcloud spanner instances add-iam-policy-binding INSTANCE_ID \\\n    --member=\"group:spanner-users@domain.com\" \\\n    --role=\"roles/spanner.databaseUser\"\n```\n\n### Why This Works\n- Proper permission level\n- Follows best practices\n- Scalable management\n- Easy to audit\n- Simplified administration\n\n🔗 Learn more: [Cloud Spanner IAM](https://cloud.google.com/spanner/docs/iam)"
    },
    {
      "id": 39,
      "question": "You create a new Google Kubernetes Engine (GKE) cluster and want to make sure that it always runs a supported and stable version of Kubernetes. What should you do?",
      "type": "single",
      "options": [
        "Enable the Node Auto-Repair feature for your GKE cluster.",
        "Enable the Node Auto-Upgrades feature for your GKE cluster.",
        "Select the latest available cluster version for your GKE cluster.",
        "Select 'Container-Optimized OS (cos)' as a node image for your GKE cluster."
      ],
      "correctAnswer": [
        "Enable the Node Auto-Upgrades feature for your GKE cluster."
      ],
      "hint": "Consider which feature automatically keeps Kubernetes version up-to-date",
      "explanation": "### Key Concept\nNode Auto-Upgrades automatically upgrades node version when the control plane is upgraded, ensuring version compatibility and security \n- Minimal maintenance overhead\n\n### Benefits\n- Always running supported version\n- Automatic security updates\n- Reduced maintenance work\n- Consistent environment\n\n🔗 Learn more: [GKE Auto-upgrades](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades)"
    },
    {
      "id": 40,
      "question": "You have an instance group that you want to load balance. You want the load balancer to terminate the client SSL session. The instance group is used to serve a public web application over HTTPS. You want to follow Google-recommended practices. What should you do?",
      "type": "single",
      "options": [
        "Configure an HTTP(S) load balancer.",
        "Configure an internal TCP load balancer.",
        "Configure an external SSL proxy load balancer.",
        "Configure an external TCP proxy load balancer."
      ],
      "correctAnswer": ["Configure an HTTP(S) load balancer."],
      "hint": "Consider Google's recommendations for SSL termination and public web applications",
      "explanation": "### Key Concept\nHTTP(S) load balancer is Google's recommended solution for SSL termination and public web applications \n- Global load balancing\n- Advanced features available\n\n🔗 Learn more: [HTTP(S) Load Balancing](https://cloud.google.com/load-balancing/docs/https)"
    },
    {
      "id": 41,
      "question": "You have 32 GB of data in a single file that you need to upload to a Nearline Storage bucket. The WAN connection you are using is rated at 1 Gbps, and you are the only one on the connection. You want to use as much of the rated 1 Gbps as possible to transfer the file rapidly. How should you upload the file?",
      "type": "single",
      "options": [
        "Use the GCP Console to transfer the file instead of gsutil.",
        "Enable parallel composite uploads using gsutil on the file transfer.",
        "Decrease the TCP window size on the machine initiating the transfer.",
        "Change the storage class of the bucket from Nearline to Multi-Regional."
      ],
      "correctAnswer": [
        "Enable parallel composite uploads using gsutil on the file transfer."
      ],
      "hint": "Consider which method allows for maximum throughput by using multiple connections",
      "explanation": "### Key Concept\nParallel composite uploads split large files into chunks that can be uploaded simultaneously, maximizing bandwidth usage.\n\n### Real-world Example\nEnable parallel composite uploads:\n```bash\n# Enable parallel composite uploads\ngsutil -o GSUtil:parallel_composite_upload_threshold=150M cp largefile.dat gs://bucket-name/\n\n# Can also be set in .boto config:\n[GSUtil]\nparallel_composite_upload_threshold=150M\n```\n\n### Why This Works\n- Splits file into chunks\n- Uploads chunks in parallel\n- Maximizes bandwidth usage\n- Improves transfer speed\n- Perfect for large files\n\n### Performance Benefits\n- Multiple simultaneous connections\n- Better network utilization\n- Faster upload times\n- Automatic chunk management\n\n### Best Practices\n- Use for files >150MB\n- Monitor network usage\n- Adjust chunk size if needed\n- Consider available memory\n\n🔗 Learn more: [Cloud Storage Transfer](https://cloud.google.com/storage/docs/gsutil/commands/cp)"
    },
    {
      "id": 42,
      "question": "You need to refactor this configuration so that the database password is not stored in plain text. You want to follow Google-recommended practices. What should you do?\n\n![Kubernetes YAML Configuration](https://www.examtopics.com/assets/media/exam-media/04338/0002100001.jpg)",
      "type": "single",
      "options": [
        "Store the database password inside the Docker image of the container, not in the YAML file.",
        "Store the database password inside a Secret object. Modify the YAML file to populate the DB_PASSWORD environment variable from the Secret.",
        "Store the database password inside a ConfigMap object. Modify the YAML file to populate the DB_PASSWORD environment variable from the ConfigMap.",
        "Store the database password in a file inside a Kubernetes persistent volume, and use a persistent volume claim to mount the volume to the container."
      ],
      "correctAnswer": [
        "Store the database password inside a Secret object. Modify the YAML file to populate the DB_PASSWORD environment variable from the Secret."
      ],
      "hint": "Consider which Kubernetes object is specifically designed for sensitive data storage",
      "explanation": "### Key Concept\nKubernetes Secrets are specifically designed to store sensitive information like passwords and keys.\n\n### Real-world Example\nImplementation:\n```yaml\n# 1. Create Secret\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\ndata:\n  password: base64encodedpassword\n\n# 2. Modified Deployment\napiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: web\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: password\n```\n\n### Why This Works\n- Encrypted at rest\n- Base64 encoded\n- Kubernetes managed\n- Access controlled\n- Separate from code\n\n### Security Benefits\n- No plain text passwords\n- RBAC integration\n- Namespace scoped\n- Easy to rotate\n- Version controlled\n\n🔗 Learn more: [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/)"
    },
    {
      "id": 43,
      "question": "You are running an application on multiple virtual machines within a managed instance group and have autoscaling enabled. The autoscaling policy is configured so that additional instances are added to the group if the CPU utilization of instances goes above 80%. VMs are added until the instance group reaches its maximum limit of five VMs or until CPU utilization of instances lowers to 80%. The initial delay for HTTP health checks against the instances is set to 30 seconds. The virtual machine instances take around three minutes to become available for users. You observe that when the instance group autoscales, it adds more instances then necessary to support the levels of end-user traffic. You want to properly maintain instance group sizes when autoscaling. What should you do?",
      "type": "single",
      "options": [
        "Set the maximum number of instances to 1.",
        "Decrease the maximum number of instances to 3.",
        "Use a TCP health check instead of an HTTP health check.",
        "Increase the initial delay of the HTTP health check to 200 seconds."
      ],
      "correctAnswer": [
        "Increase the initial delay of the HTTP health check to 200 seconds."
      ],
      "hint": "Consider the startup time of instances and how it relates to health check timing",
      "explanation": "### Key Concept\nHealth check initial delay should align with actual instance startup time to prevent premature scaling decisions.\n\n### Real-world Example\nConfig update:\n```yaml\nhealthCheck:\n  type: HTTP\n  initialDelaySec: 200\n  checkIntervalSec: 10\n  timeoutSec: 5\n  healthyThreshold: 2\n  unhealthyThreshold: 3\n```\n\n### Why This Works\n- Matches actual startup time (3min)\n- Prevents premature scaling\n- Allows proper initialization\n- Accurate health status\n- Better resource utilization\n\n### Problem It Solves\n- Prevents over-provisioning\n- Reduces unnecessary scaling\n- More accurate capacity\n- Cost optimization\n- Better performance\n\n### Best Practices\n- Set delay > instance startup\n- Account for application init\n- Monitor actual startup times\n- Adjust based on metrics\n\n🔗 Learn more: [MIG Health Checks](https://cloud.google.com/compute/docs/instance-groups/autohealing-instances)"
    },
    {
      "id": 44,
      "question": "You need to select and configure compute resources for a set of batch processing jobs. These jobs take around 2 hours to complete and are run nightly. You want to minimize service costs. What should you do?",
      "type": "single",
      "options": [
        "Select Google Kubernetes Engine. Use a single-node cluster with a small instance type.",
        "Select Google Kubernetes Engine. Use a three-node cluster with micro instance types.",
        "Select Compute Engine. Use preemptible VM instances of the appropriate standard machine type.",
        "Select Compute Engine. Use VM instance types that support micro bursting."
      ],
      "correctAnswer": [
        "Select Compute Engine. Use preemptible VM instances of the appropriate standard machine type."
      ],
      "hint": "Consider cost optimization for short-duration workloads that can handle interruptions",
      "explanation": "### Key Concept\nPreemptible VMs offer significant cost savings (up to 80%) for interruptible workloads that run less than 24 hours.\n\n### Real-world Example\nImplementation:\n```bash\n# Create preemptible VM for batch job\ngcloud compute instances create batch-processor \\\n    --machine-type=n1-standard-4 \\\n    --preemptible \\\n    --metadata startup-script='#!/bin/bash\n        # Run batch processing\n        ./run_batch_job.sh\n    '\n```\n\n### Why This Works\n- Much lower cost (60-80% cheaper)\n- Perfect for batch jobs\n- 2-hour jobs fit well\n- Nightly schedule ideal\n- Auto-termination acceptable\n\n### Cost Benefits\n- Significant savings vs standard VMs\n- No long-term commitment\n- Pay only for processing time\n- Optimal for scheduled jobs\n\n### Best Practices\n- Handle preemption gracefully\n- Use standard machine types\n- Schedule during low-demand times\n- Implement retry logic\n- Monitor completion status\n\n🔗 Learn more: [Preemptible VM Instances](https://cloud.google.com/compute/docs/instances/preemptible)"
    },
    {
      "id": 45,
      "question": "You recently deployed a new version of an application to App Engine and then discovered a bug in the release. You need to immediately revert to the prior version of the application. What should you do?",
      "type": "single",
      "options": [
        "Run gcloud app restore.",
        "On the App Engine page of the GCP Console, select the application that needs to be reverted and click Revert.",
        "On the App Engine Versions page of the GCP Console, route 100% of the traffic to the previous version.",
        "Deploy the original version as a separate application. Then go to App Engine settings and split traffic between applications so that the original version serves 100% of the requests."
      ],
      "correctAnswer": [
        "On the App Engine Versions page of the GCP Console, route 100% of the traffic to the previous version."
      ],
      "hint": "Consider the fastest way to switch traffic without needing to redeploy",
      "explanation": "### Key Concept\nApp Engine maintains multiple versions and allows immediate traffic routing between them without redeployment.\n\n### Real-world Example\nVia gcloud:\n```bash\n# List versions\ngcloud app versions list\n\n# Migrate traffic\ngcloud app services set-traffic --splits=version1=1.0\n\n# Or gradually:\ngcloud app services set-traffic --splits=version1=1.0 --migrate\n```\n\nVia YAML:\n```yaml\nenv: standard\nversion: version1\ntraffic_split: 1.0\n```\n\n### Why This Works\n- Immediate effect\n- No deployment needed\n- Zero downtime\n- Easily reversible\n- Maintains all versions\n\n### Benefits\n- Quick rollback\n- No code changes\n- No deployment delay\n- Simple UI operation\n- Version preservation\n\n### Best Practices\n- Keep previous versions\n- Test before full routing\n- Monitor after switch\n- Document changes\n- Clean up old versions\n\n🔗 Learn more: [App Engine Traffic Splitting](https://cloud.google.com/appengine/docs/standard/python3/splitting-traffic)"
    },
    {
      "id": 46,
      "question": "You deployed an App Engine application using 'gcloud app deploy', but it did not deploy to the intended project. You want to find out why this happened and where the application deployed. What should you do?",
      "type": "single",
      "options": [
        "Check the app.yaml file for your application and check project settings.",
        "Check the web-application.xml file for your application and check project settings.",
        "Go to Deployment Manager and review settings for deployment of applications.",
        "Go to Cloud Shell and run 'gcloud config list' to review the Google Cloud configuration used for deployment."
      ],
      "correctAnswer": [
        "Go to Cloud Shell and run 'gcloud config list' to review the Google Cloud configuration used for deployment."
      ],
      "hint": "Consider which command shows the current configuration settings including project ID",
      "explanation": "### Key Concept\ngcloud config list shows all active configuration settings, including the currently set project.\n\n### Real-world Example\n```bash\n# Check current config\ngcloud config list\n\n# Sample output:\n[core]\nproject = my-project-id\naccount = user@example.com\ndisable_usage_reporting = False\n\n# To change project if needed\ngcloud config set project correct-project-id\n```\n\n### Why This Works\n- Shows active project\n- Displays all settings\n- Quick verification\n- Easy to diagnose\n- No file checking needed\n\n### Best Practices\n- Verify project before deploy\n- Set default project\n- Use project flag when needed\n- Document configurations\n- Regular config checks\n\n🔗 Learn more: [Gcloud Configuration](https://cloud.google.com/sdk/gcloud/reference/config)"
    },
    {
      "id": 47,
      "question": "You want to configure 10 Compute Engine instances for availability when maintenance occurs. Your requirements state that these instances should attempt to automatically restart if they crash. Also, the instances should be highly available including during system maintenance. What should you do?",
      "type": "single",
      "options": [
        "Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group.",
        "Create an instance template for the instances. Set 'Automatic Restart' to off. Set 'On-host maintenance' to Terminate VM instances. Add the instance template to an instance group.",
        "Create an instance group for the instances. Set the 'Autohealing' health check to healthy (HTTP).",
        "Create an instance group for the instance. Verify that the 'Advanced creation options' setting for 'do not retry machine creation' is set to off."
      ],
      "correctAnswer": [
        "Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group."
      ],
      "hint": "Consider settings that handle both system crashes and maintenance events while maintaining availability",
      "explanation": "### Key Concept\nInstance availability policies and templates work together to ensure high availability during both crashes and maintenance.\n\n### Real-world Example\n```bash\n# Create instance template with HA settings\ngcloud compute instance-templates create ha-template \\\n    --maintenance-policy=MIGRATE \\\n    --restart-on-failure \\\n    --machine-type=n1-standard-2\n\n# Create managed instance group\ngcloud compute instance-groups managed create ha-group \\\n    --template=ha-template \\\n    --size=10 \\\n    --zone=us-central1-a\n```\n\n### Why This Works\n- Automatic restart handles crashes\n- Live migration during maintenance\n- No downtime during updates\n- Consistent configuration\n- Scalable management\n\n### Best Practices\n- Use instance templates\n- Enable automatic restarts\n- Configure live migration\n- Group similar instances\n- Regular health checks\n\n🔗 Learn more: [Compute Engine High Availability](https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options)"
    },
    {
      "id": 48,
      "question": "You host a static website on Cloud Storage. Recently, you began to include links to PDF files on this site. Currently, when users click on the links to these PDF files, their browsers prompt them to save the file onto their local system. Instead, you want the clicked PDF files to be displayed within the browser window directly, without prompting the user to save the file locally. What should you do?",
      "type": "single",
      "options": [
        "Enable Cloud CDN on the website frontend.",
        "Enable 'Share publicly' on the PDF file objects.",
        "Set Content-Type metadata to application/pdf on the PDF file objects.",
        "Add a label to the storage bucket with a key of Content-Type and value of application/pdf."
      ],
      "correctAnswer": [
        "Set Content-Type metadata to application/pdf on the PDF file objects."
      ],
      "hint": "Consider how browsers determine how to handle different types of files based on MIME types",
      "explanation": "### Key Concept\nThe Content-Type (MIME type) metadata tells browsers how to handle files when accessed.\n\n### Real-world Example\n```bash\n# Set content-type for existing file\ngsutil setmeta -h \"Content-Type:application/pdf\" gs://bucket-name/document.pdf\n\n# Upload with correct content-type\ngsutil cp -h \"Content-Type:application/pdf\" ./document.pdf gs://bucket-name/\n\n# Set content-type for multiple PDFs\ngsutil -m setmeta -h \"Content-Type:application/pdf\" gs://bucket-name/*.pdf\n```\n\n### Why This Works\n- Browser recognizes PDF format\n- Inline display enabled\n- No download prompt\n- Standard MIME type\n- Proper file handling\n\n### Best Practices\n- Set correct MIME types\n- Verify metadata after upload\n- Batch process existing files\n- Test in different browsers\n- Document content types\n\n🔗 Learn more: [Cloud Storage Object Metadata](https://cloud.google.com/storage/docs/metadata)"
    },
    {
      "id": 49,
      "question": "You have a virtual machine that is currently configured with 2 vCPUs and 4 GB of memory. It is running out of memory. You want to upgrade the virtual machine to have 8 GB of memory. What should you do?",
      "type": "single",
      "options": [
        "Rely on live migration to move the workload to a machine with more memory.",
        "Use 'gcloud' to add metadata to the VM. Set the key to required-memory-size and the value to 8 GB.",
        "Stop the VM, change the machine type to n1-standard-8, and start the VM.",
        "Stop the VM, increase the memory to 8 GB, and start the VM."
      ],
      "correctAnswer": [
        "Stop the VM, increase the memory to 8 GB, and start the VM."
      ],
      "hint": "Consider the process of modifying VM resources while maintaining the same number of vCPUs",
      "explanation": "### Key Concept\nChanging VM memory requires stopping the instance but allows for custom memory configurations without changing the entire machine type.\n\n### Real-world Example\n```bash\n# 1. Stop the VM\ngcloud compute instances stop my-instance --zone=us-central1-a\n\n# 2. Set custom machine type with new memory\ngcloud compute instances set-machine-type my-instance --custom-memory=8 --custom-cpu=2 --zone=us-central1-a\n\n# 3. Start the VM\ngcloud compute instances start my-instance --zone=us-central1-a\n```\n\n### Why This Works\n- Maintains existing vCPUs\n- Custom memory sizing\n- No data loss\n- Cost-effective\n- Simple process\n\n### Best Practices\n- Schedule maintenance window\n- Backup before stopping\n- Verify applications\n- Document changes\n- Monitor after restart\n\n### Common Mistakes to Avoid\n- Changing machine type unnecessarily\n- Live modification attempts\n- Metadata modifications\n- Relying on auto-migration\n\n🔗 Learn more: [Changing Machine Type](https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance)"
    },
    {
      "id": 50,
      "question": "You have production and test workloads that you want to deploy on Compute Engine. Production VMs need to be in a different subnet than the test VMs. All the VMs must be able to reach each other over Internal IP without creating additional routes. You need to set up VPC and the 2 subnets. Which configuration meets these requirements?",
      "type": "single",
      "options": [
        "Create a single custom VPC with 2 subnets. Create each subnet in a different region and with a different CIDR range.",
        "Create a single custom VPC with 2 subnets. Create each subnet in the same region and with the same CIDR range.",
        "Create 2 custom VPCs, each with a single subnet. Create each subnet in a different region and with a different CIDR range.",
        "Create 2 custom VPCs, each with a single subnet. Create each subnet in the same region and with the same CIDR range."
      ],
      "correctAnswer": [
        "Create a single custom VPC with 2 subnets. Create each subnet in a different region and with a different CIDR range."
      ],
      "hint": "Consider network isolation requirements while maintaining internal connectivity",
      "explanation": "### Key Concept\nA single VPC with multiple subnets allows internal communication while maintaining network separation.\n\n### Real-world Example\n```bash\n# Create VPC\ngcloud compute networks create my-vpc --subnet-mode=custom\n\n# Create production subnet\ngcloud compute networks subnets create prod-subnet \\\n    --network=my-vpc \\\n    --region=us-central1 \\\n    --range=10.0.0.0/20\n\n# Create test subnet\ngcloud compute networks subnets create test-subnet \\\n    --network=my-vpc \\\n    --region=us-east1 \\\n    --range=10.1.0.0/20\n```\n\n### Why This Works\n- Single VPC connectivity\n- Subnet isolation\n- No additional routes\n- Regional distribution\n- Clean IP separation\n\n### Best Practices\n- Non-overlapping CIDR\n- Logical subnet naming\n- Document IP ranges\n- Plan for growth\n- Consider future routing\n\n🔗 Learn more: [VPC Network Design](https://cloud.google.com/vpc/docs/vpc)"
    },
    {
      "id": 51,
      "question": "You need to create an autoscaling managed instance group for an HTTPS web application. You want to make sure that unhealthy VMs are recreated. What should you do?",
      "type": "single",
      "options": [
        "Create a health check on port 443 and use that when creating the Managed Instance Group.",
        "Select Multi-Zone instead of Single-Zone when creating the Managed Instance Group.",
        "In the Instance Template, add the label 'health-check'.",
        "In the Instance Template, add a startup script that sends a heartbeat to the metadata server."
      ],
      "correctAnswer": [
        "Create a health check on port 443 and use that when creating the Managed Instance Group."
      ],
      "hint": "Consider how HTTPS traffic works and what port it uses by default",
      "explanation": "### Key Concept\nHealth checks are essential for maintaining high availability in managed instance groups by automatically detecting and replacing unhealthy instances.\n\n### Technical Details\n- HTTPS uses port 443 by default\n- Health checks actively monitor instance health\n- MIG uses health check results to maintain service availability\n\n### Implementation Example\n```bash\n# Create health check\ngcloud compute health-checks create https my-health-check \\\n    --port=443 \\\n    --check-interval=30s \\\n    --healthy-threshold=2 \\\n    --timeout=10s \\\n    --unhealthy-threshold=3\n\n# Create MIG with health check\ngcloud compute instance-groups managed create my-mig \\\n    --template=my-instance-template \\\n    --health-check=my-health-check \\\n    --initial-delay=300 \\\n    --region=us-central1 \\\n    --size=2\n```\n\n### Why This Works\n- Automatic detection of failures\n- Proactive instance replacement\n- Minimal downtime\n- Native integration with GCP\n\n### Best Practices\n- Set appropriate check intervals\n- Configure meaningful thresholds\n- Use appropriate ports for service type\n- Configure initial delay for startup\n- Monitor health check metrics\n\n🔗 Learn more: [Health Checks in GCP](https://cloud.google.com/load-balancing/docs/health-checks)"
    },
    {
      "id": 52,
      "question": "Your company has a Google Cloud Platform project that uses BigQuery for data warehousing. Your data science team changes frequently and has few members. You need to allow members of this team to perform queries. You want to follow Google-recommended practices. What should you do?",
      "type": "single",
      "options": [
        "Create an IAM entry for each data scientist's user account. Assign the BigQuery jobUser role to the group.",
        "Create an IAM entry for each data scientist's user account. Assign the BigQuery dataViewer role to the group.",
        "Create a dedicated Google group in Cloud Identity. Add each data scientist's user account to the group. Assign the BigQuery jobUser role to the group.",
        "Create a dedicated Google group in Cloud Identity. Add each data scientist's user account to the group. Assign the BigQuery dataViewer role to the group."
      ],
      "correctAnswer": [
        "Create a dedicated Google group in Cloud Identity. Add each data scientist's user account to the group. Assign the BigQuery jobUser role to the group."
      ],
      "hint": "Consider grouping users for scalable permissions and the roles needed for running queries.",
      "explanation": "### Key Concept\nProperly managing access via groups and roles in IAM (Identity and Access Management) aligns with Google Cloud best practices, ensuring streamlined access management and segregation of duties.\n\n### Why the Correct Answer Works\n1. **Google Groups and Role Management**:\n   - Grouping users in a single Google group simplifies permission assignment as the members of the group inherit the permissions.\n   - If team members change frequently, only the group's membership needs updating rather than reconfiguring IAM roles for individual accounts.\n2. **BigQuery jobUser Role**:\n   - Members assigned the 'jobUser' role can run queries on datasets.\n   - This role is ideal for enabling data science workflows without assigning unnecessary permissions.\n\n### Description of Other Roles\n- **dataViewer Role**:\n  - This grants read-only access to datasets and tables.\n  - It does not allow execution of queries, which makes it incorrect in this scenario.\n\n### Implementation Example\n```bash\n# 1. Create a Google group (use Admin Console or API).\ngcloud iam groups create my-data-scientists --description 'Group for data scientists'\n\n# 2. Add each data scientist's user account to the Google group.\n# Example process via Admin Console.\n\n# 3. Assign the BigQuery jobUser role to the group.\ngcloud projects add-iam-policy-binding my-project-id \\\n  --member='group:my-data-scientists@example.com' \\\n  --role='roles/bigquery.jobUser'\n```\n\n### Best Practices for Adding New Team Members\n- Add new users to the **Google group**.\n- Ensure group permissions are reviewed periodically.\n- Use the 'least privilege' principle when assigning roles.\n\n🔗 Learn more: [IAM Best Practices](https://cloud.google.com/iam/docs/overview) | [BigQuery Roles](https://cloud.google.com/bigquery/docs/access-control)"
    },
    {
      "id": 53,
      "question": "Your company has a 3-tier solution running on Compute Engine. The configuration of the current infrastructure is shown below. Each tier has a service account that is associated with all instances within it. You need to enable communication on TCP port 8080 between tiers as follows:\n\n- Instances in tier #1 must communicate with tier #2.\n- Instances in tier #2 must communicate with tier #3.\n\n![3-tier Compute Engine Solution](https://www.examtopics.com/assets/media/exam-media/04338/0002700001.png)",
      "type": "single",
      "options": [
        "Create an ingress firewall rule with the following settings:\n- Targets: all instances\n- Source filter: IP ranges (with the range set to 10.0.2.0/24)\n- Protocols: allow all.\n\nCreate another ingress firewall rule with the following settings:\n- Targets: all instances\n- Source filter: IP ranges (with the range set to 10.0.1.0/24)\n- Protocols: allow all.",
        "Create an ingress firewall rule with the following settings:\n- Targets: all instances with the tier #2 service account\n- Source filter: all instances with the tier #1 service account\n- Protocols: allow TCP:8080.\n\nCreate another ingress firewall rule with the following settings:\n- Targets: all instances with the tier #3 service account\n- Source filter: all instances with the tier #2 service account\n- Protocols: allow TCP:8080.",
        "Create an ingress firewall rule with the following settings:\n- Targets: all instances with the tier #2 service account\n- Source filter: all instances with the tier #1 service account\n- Protocols: allow all.\n\nCreate another ingress firewall rule with the following settings:\n- Targets: all instances with the tier #3 service account\n- Source filter: all instances with the tier #2 service account\n- Protocols: allow all.",
        "Create an egress firewall rule with the following settings:\n- Targets: all instances\n- Source filter: IP ranges (with the range set to 10.0.2.0/24)\n- Protocols: allow TCP:8080.\n\nCreate another egress firewall rule with the following settings:\n- Targets: all instances\n- Source filter: IP ranges (with the range set to 10.0.1.0/24)\n- Protocols: allow TCP:8080."
      ],
      "correctAnswer": [
        "Create an ingress firewall rule with the following settings:\n- Targets: all instances with the tier #2 service account\n- Source filter: all instances with the tier #1 service account\n- Protocols: allow TCP:8080.\n\nCreate another ingress firewall rule with the following settings:\n- Targets: all instances with the tier #3 service account\n- Source filter: all instances with the tier #2 service account\n- Protocols: allow TCP:8080."
      ],
      "hint": "Focus on linking tiers through service accounts and use ingress firewall rules to enable secure communication.",
      "explanation": "### Key Concept\nTo enable communication between instances in different tiers, you should configure **ingress firewall rules** targeting specific service accounts. This ensures that only instances associated with the intended tier can send or receive traffic on the specified port.\n\n### Why the Correct Answer Works\n1. **Service Account Targeting**:\n   - Using service accounts to target specific instances allows precise control over inbound traffic. For example, tier #2 instances will only accept traffic from instances in tier #1, and tier #3 instances will only accept traffic from tier #2.\n2. **Protocol and Port**:\n   - Allowing only TCP traffic on port 8080 ensures that the communication is specific to the application requirements and avoids exposing unnecessary protocols or ports.\n\n### Implementation Example\n```bash\n# Rule to allow Tier #1 to communicate with Tier #2 on TCP:8080\ngcloud compute firewall-rules create allow-tier1-to-tier2 \\\n    --direction=INGRESS \\\n    --action=ALLOW \\\n    --rules=tcp:8080 \\\n    --source-tags=tier-1-instances \\\n    --target-service-accounts=tier-2-service-account\n\n# Rule to allow Tier #2 to communicate with Tier #3 on TCP:8080\ngcloud compute firewall-rules create allow-tier2-to-tier3 \\\n    --direction=INGRESS \\\n    --action=ALLOW \\\n    --rules=tcp:8080 \\\n    --source-tags=tier-2-instances \\\n    --target-service-accounts=tier-3-service-account\n```\n\n### Why Other Options Are Incorrect\n1. **Option A**:\n   - Allowing all protocols for all instances is too broad and violates the principle of least privilege.\n2. **Option C**:\n   - Allowing all protocols is unnecessary when only TCP:8080 is required.\n3. **Option D**:\n   - Egress rules do not specifically control incoming traffic and are not sufficient for restricting access between tiers.\n\n### Best Practices\n- Use **service accounts** to apply permissions specific to workloads.\n- Define **ingress rules** for restricting access between tiers based on source and target.\n- Configure **TCP port filter** only for the necessary port (in this case, 8080).\n- Regularly audit firewall rules for compliance and security.\n\n🔗 Learn more: [Firewall Rules in Google Cloud](https://cloud.google.com/vpc/docs/firewalls)"
    },
    {
      "id": 54,
      "question": "You are given a project with a single Virtual Private Cloud (VPC) and a single subnetwork in the us-central1 region. There is a Compute Engine instance hosting an application in this subnetwork. You need to deploy a new instance in the same project in the europe-west1 region. This new instance needs access to the application. You want to follow Google-recommended practices. What should you do?",
      "type": "single",
      "options": [
        "Create a subnetwork in the same VPC, in europe-west1. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.",
        "Create a VPC and a subnetwork in europe-west1. Expose the application with an internal load balancer. Create the new instance in the new subnetwork and use the load balancer's address as the endpoint.",
        "Create a subnetwork in the same VPC, in europe-west1. Use Cloud VPN to connect the two subnetworks. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.",
        "Create a VPC and a subnetwork in europe-west1. Peer the 2 VPCs. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint."
      ],
      "correctAnswer": [
        "Create a subnetwork in the same VPC, in europe-west1. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint."
      ],
      "hint": "Consider using the same VPC to simplify connectivity between regions and avoid additional networking services.",
      "explanation": "### Key Concept\nUsing the same VPC across multiple regions simplifies network management and ensures seamless connectivity between resources within those regions. By creating a subnetwork in the **same VPC**, the private IP addresses of instances can communicate without addition of VPNs or VPC peering.\n\n### Why the Correct Answer Works\n1. **Single VPC Architecture**: Google Cloud allows global connectivity within a single VPC. Instances in different regions under the same VPC can communicate with one another via internal IP addresses.\n2. **No Additional Services Needed**: By extending the same VPC instead of creating a new one, you avoid the complexity of setting up additional networking components (e.g., Cloud VPN or VPC peering).\n3. **Recommended Best Practice**: Google Cloud recommends using a single VPC to simplify networking whenever possible.\n\n### Implementation Example\n```bash\n# Step 1: Create a subnetwork in europe-west1 under the same VPC\ngcloud compute networks subnets create europe-west1-subnet \\\n    --network=my-vpc \\\n    --region=europe-west1 \\\n    --range=10.1.0.0/20\n\n# Step 2: Create a new instance in the europe-west1 region using the new subnetwork\ngcloud compute instances create new-instance \\\n    --zone=europe-west1-b \\\n    --subnet=europe-west1-subnet\n\n# Step 3: Use the internal IP of the us-central1 instance as the application endpoint for connectivity.\n```\n\n### Why Other Options Are Incorrect\n1. **Option B**:\n   - Creating a separate VPC goes against the simplicity and flexibility provided by single VPCs.\n   - Exposing the application via an internal load balancer is unnecessary when the same VPC can be used.\n2. **Option C**:\n   - Cloud VPN adds redundant complexity since a single VPC already provides connectivity across regions for internal IPs.\n3. **Option D**:\n   - VPC peering is unnecessary when the resources are part of the same project and can share a single VPC for connectivity.\n\n### Best Practices\n- **Use non-overlapping CIDR blocks** for subnetworks in different regions.\n- Plan your VPC and subnetwork architecture to support potential growth and regional expansion.\n- Test connectivity between instances across subnetworks and regions before deploying production workloads.\n\n🔗 Learn more: [VPC Design Overview](https://cloud.google.com/vpc/docs/vpc) | [Subnets in Google Cloud](https://cloud.google.com/vpc/docs/subnets)"
    },
    {
      "id": 55,
      "question": "Your projects incurred more costs than you expected last month. Your research reveals that a development GKE container emitted a huge number of logs, which resulted in higher costs. You want to disable the logs quickly using the minimum number of steps. What should you do?",
      "type": "single",
      "options": [
        "Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource.",
        "Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE Cluster Operations resource.",
        "Go to the GKE console, and delete existing clusters. Recreate a new cluster. Clear the option to enable legacy Stackdriver Logging.",
        "Go to the GKE console, and delete existing clusters. Recreate a new cluster. Clear the option to enable legacy Stackdriver Monitoring."
      ],
      "correctAnswer": [
        "Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource."
      ],
      "hint": "Focus on identifying the log source that caused the unexpected cost and disabling it without recreating existing clusters.",
      "explanation": "### Key Concept\nLogs generated by Google Kubernetes Engine (GKE) clusters can cause unexpected costs if excessive detailed logs are emitted. Controlling the log ingestion settings allows you to manage and reduce these costs without impacting live resources.\n\n### Why the Correct Answer Works\n1. **Stackdriver Logging**:\n   - Logs ingestion is managed in Stackdriver Logging (Google Cloud's Operations Suite).\n   - You can disable logging for specific resources (e.g., GKE container resource) to reduce costs instantly.\n2. **Direct Action**:\n   - Disabling logs for the GKE container resource is the quickest action you can take. There is no need to delete and recreate clusters, which would be time-consuming and disruptive.\n\n### Implementation Example\n```bash\n# Steps to disable logs ingestion:\n# 1. Go to the Google Cloud Console.\n# 2. Navigate to 'Logging' > 'Logs Explorer'.\n# 3. Open 'Logs ingestion'.\n# 4. Find the logs related to GKE container resource.\n# 5. Disable logging for the specific log source to stop excessive data collection.\n```\n\n### Why Other Options Are Incorrect\n1. **Option B**:\n   - Disabling logs for GKE Cluster Operations only prevents logs related to the underlying infrastructure, not application logs, which is not relevant in this case.\n2. **Option C**:\n   - Deleting and recreating clusters is disruptive and unnecessary when the issue can be resolved by disabling logs at the ingestion level.\n3. **Option D**:\n   - Legacy Stackdriver Monitoring settings are not relevant; the issue is specific to logs ingestion, not monitoring.\n\n### Best Practices\n- Regularly audit **logging configurations** for all services to ensure that unnecessary logs are not retained.\n- Use **log exclusions** for noisy logs that are not required for analysis.\n- Monitor **logs ingestion metrics** to identify unusual growth patterns and adjust accordingly.\n\n🔗 Learn more: [Managing Logs Exclusions](https://cloud.google.com/logging/docs/exclusions) | [GKE Logging and Monitoring](https://cloud.google.com/kubernetes-engine/docs/how-to/logging)"
    },
    {
      "id": 56,
      "question": "You have a website hosted on App Engine standard environment. You want 1% of your users to see a new test version of the website. You want to minimize complexity. What should you do?",
      "type": "single",
      "options": [
        "Deploy the new version in the same application and use the --migrate option.",
        "Deploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version.",
        "Create a new App Engine application in the same project. Deploy the new version in that application. Use the App Engine library to proxy 1% of the requests to the new version.",
        "Create a new App Engine application in the same project. Deploy the new version in that application. Configure your network load balancer to send 1% of the traffic to that new application."
      ],
      "correctAnswer": [
        "Deploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version."
      ],
      "hint": "Leverage App Engine's native traffic splitting capabilities for controlled rollouts",
      "explanation": "### Key Concept\nApp Engine's traffic splitting allows percentage-based distribution between versions without complex infrastructure changes.\n\n### Command Example\n```bash\ngcloud app services set-traffic --splits=v2023=99,test-version=1 --migrate\n```\n\n### Why This Works\n- Minimal operational complexity\n- No duplicate deployments\n- Instant rollout adjustment\n- Built-in version management\n\n🔗 Learn more: [App Engine Traffic Splitting](https://cloud.google.com/appengine/docs/standard/python3/splitting-traffic)"
    },
    {
      "id": 57,
      "question": "You have a web application deployed as a managed instance group. You have a new version of the application to gradually deploy. Your web application is currently receiving live web traffic. You want to ensure that the available capacity does not decrease during the deployment. What should you do?",
      "type": "single",
      "options": [
        "Perform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1.",
        "Perform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0.",
        "Create a new managed instance group with an updated instance template. Add the group to the backend service for the load balancer. When all instances in the new managed instance group are healthy, delete the old managed instance group.",
        "Create a new instance template with the new application version. Update the existing managed instance group with the new instance template. Delete the instances in the managed instance group to allow the managed instance group to recreate the instance using the new instance template."
      ],
      "correctAnswer": [
        "Perform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0."
      ],
      "hint": "Zero downtime deployment requires maintaining instance availability during updates",
      "explanation": "### Key Concept\nMaxSurge=1 creates buffer capacity before removing old instances, ensuring service availability.\n\n### Deployment Config\n```yaml\nupdatePolicy:\n  type: PROACTIVE\n  maxSurge: 1\n  maxUnavailable: 0\n```\n\n### Why This Works\n- 100% capacity maintained\n- Seamless transition\n- Automatic health checks\n- Gradual instance replacement\n\n🔗 Learn more: [Managed Instance Groups Updates](https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups)"
    },
    {
      "id": 58,
      "question": "You are building an application that stores relational data from users. Users across the globe will use this application. Your CTO is concerned about the scaling requirements because the size of the user base is unknown. You need to implement a database solution that can scale with your user growth with minimum configuration changes. Which storage solution should you use?",
      "type": "single",
      "options": [
        "Cloud SQL",
        "Cloud Spanner",
        "Cloud Firestore",
        "Cloud Datastore"
      ],
      "correctAnswer": ["Cloud Spanner"],
      "hint": "Global scale + relational model + automatic scaling",
      "explanation": "### Key Comparison\n| Feature        | Cloud Spanner | Cloud SQL       |\n|----------------|---------------|------------------|\n| Scalability    | Petabyte-scale| Vertical scaling |\n| Global         | Yes           | Regional         |\n| Maintenance    | Fully managed | Partial          |\n\n### Why Cloud Spanner\n- Horizontal scaling\n- Strong consistency\n- 99.999% availability\n- No manual sharding\n\n🔗 Learn more: [Cloud Spanner Documentation](https://cloud.google.com/spanner/docs)"
    },
    {
      "id": 59,
      "question": "You are the organization and billing administrator for your company. The engineering team has the Project Creator role on the organization. You do not want the engineering team to be able to link projects to the billing account. Only the finance team should be able to link a project to a billing account, but they should not be able to make any other changes to projects. What should you do?",
      "type": "single",
      "options": [
        "Assign the finance team only the Billing Account User role on the billing account.",
        "Assign the engineering team only the Billing Account User role on the billing account.",
        "Assign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.",
        "Assign the engineering team the Billing Account User role on the billing account and the Project Billing Manager role on the organization."
      ],
      "correctAnswer": [
        "Assign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization."
      ],
      "hint": "Separation of duties between project creation and billing association",
      "explanation": "### Role Permissions\n1. **Billing Account User**: Link projects to billing\n2. **Project Billing Manager**: Manage billing association\n3. **Project Creator**: Create projects only\n\n### Configuration Flow\n1. Finance gets billing roles\n2. Engineering keeps Project Creator\n3. IAM policies prevent cross-role access\n\n🔗 Reference: [GCP IAM Roles](https://cloud.google.com/iam/docs/understanding-roles)"
    },
    {
      "id": 60,
      "question": "You have an application running in Google Kubernetes Engine (GKE) with cluster autoscaling enabled. The application exposes a TCP endpoint. There are several replicas of this application. You have a Compute Engine instance in the same region, but in another Virtual Private Cloud (VPC), called gce-network, that has no overlapping IP ranges with the first VPC. This instance needs to connect to the application on GKE. You want to minimize effort. What should you do?",
      "type": "single",
      "options": [
        "In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. Set the service's externalTrafficPolicy to Cluster. Configure the Compute Engine instance to use the address of the load balancer that has been created.",
        "In GKE, create a Service of type NodePort that uses the application's Pods as backend. Create a Compute Engine instance called proxy with 2 network interfaces, one in each VPC. Use iptables on this instance to forward traffic from gce-network to the GKE nodes. Configure the Compute Engine instance to use the address of proxy in gce-network as endpoint.",
        "In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal Peer the two VPCs together. Configure the Compute Engine instance to use the address of the load balancer that has been created.",
        "In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. Add a Cloud Armor Security Policy to the load balancer that whitelists the internal IPs of the MIG's instances. Configure the Compute Engine instance to use the address of the load balancer that has been created."
      ],
      "correctAnswer": [
        "In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal Peer the two VPCs together. Configure the Compute Engine instance to use the address of the load balancer that has been created."
      ],
      "hint": "Cross-VPC connectivity with managed networking services",
      "explanation": "### Implementation Steps\n1. **Internal LB**: \n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: internal-lb\n  annotations:\n    cloud.google.com/load-balancer-type: \"Internal\"\nspec:\n  type: LoadBalancer\n```\n2. **VPC Peering**:\n```bash\ngcloud compute networks peerings create peer-gke-to-gce \\\n  --network=default \\\n  --peer-network=gce-network\n```\n3. **Connectivity Test**: \n```bash\nping <internal-lb-ip>\n```\n\n### Why This Works\n- Private IP connectivity\n- No public exposure\n- Automatic routing\n- Minimal configuration\n\n🔗 Reference: [Internal Load Balancing](https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing)"
    },
    [
      {
        "id": 61,
        "question": "Your organization is a financial company that needs to store audit log files for 3 years. Your organization has hundreds of Google Cloud projects. You need to implement a cost-effective approach for log file retention. What should you do?",
        "type": "single",
        "options": [
          "Create an export to the sink that saves logs from Cloud Audit to BigQuery.",
          "Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.",
          "Write a custom script that uses logging API to copy the logs from Stackdriver logs to BigQuery.",
          "Export these logs to Cloud Pub/Sub and write a Cloud Dataflow pipeline to store logs to Cloud SQL."
        ],
        "correctAnswer": [
          "Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket."
        ],
        "hint": "Prioritize archival storage tiers for long-term retention",
        "explanation": "### Key Concept\nColdline Storage offers the lowest cost for archival data with 30-day minimum retention.\n\n### Implementation Steps\n1. Create log sink:\n```bash\ngcloud logging sinks create audit-coldline \\\n  storage.googleapis.com/my-coldline-bucket \\\n  --log-filter='logName:\"logs/cloudaudit.googleapis.com\"'\n```\n2. Set bucket lifecycle policy\n\n### Why This Works\n- 1/5th of Standard Storage cost\n- Automated log exports\n- No scripting required\n- Multi-project support\n\n🔗 Reference: [Coldline Storage Pricing](https://cloud.google.com/storage/pricing#archival)"
      },
      {
        "id": 62,
        "question": "You want to run a single caching HTTP reverse proxy on GCP for a latency-sensitive website. This specific reverse proxy consumes almost no CPU. You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes. You want to minimize cost. How should you run this reverse proxy?",
        "type": "single",
        "options": [
          "Create a Cloud Memorystore for Redis instance with 32-GB capacity.",
          "Run it on Compute Engine, and choose a custom instance type with 6 vCPUs and 32 GB of memory.",
          "Package it in a container image, and run it on Kubernetes Engine, using n1-standard-32 instances as nodes.",
          "Run it on Compute Engine, choose the instance type n1-standard-1, and add an SSD persistent disk of 32 GB."
        ],
        "correctAnswer": [
          "Create a Cloud Memorystore for Redis instance with 32-GB capacity."
        ],
        "hint": "Managed service vs custom instance memory optimization",
        "explanation": "### Cost Analysis\n| Option | Monthly Cost (EST)* |\n|--------|---------------------|\n| Memorystore | ~$1,900 |\n| Custom VM (6vCPU+32GB) | ~$157 |\n| n1-standard-1 + SSD | ~$83 |\n\n*Costs approximate for comparison\n\n### Reality Check\n- Actual correct answer contradicts cost minimization\n- **Note**: This question appears to require domain knowledge about Cloud Memorystore being preferred for caching use cases despite higher cost\n\n🔗 Reference: [Memorystore Pricing](https://cloud.google.com/memorystore/pricing)"
      },
      {
        "id": 63,
        "question": "You are hosting an application on bare-metal servers in your own data center. The application needs access to Cloud Storage. However, security policies prevent the servers hosting the application from having public IP addresses or access to the internet. You want to follow Google-recommended practices to provide the application with access to Cloud Storage. What should you do?",
        "type": "single",
        "options": [
          "Use nslookup to get the IP address for storage.googleapis.com. Negotiate with the security team to be able to give a public IP address to the servers. Only allow egress traffic from those servers to the IP addresses for storage.googleapis.com.",
          "Using Cloud VPN, create a VPN tunnel to a Virtual Private Cloud (VPC) in Google Cloud. In this VPC, create a Compute Engine instance and install the Squid proxy server on this instance. Configure your servers to use that instance as a proxy to access Cloud Storage.",
          "Use Migrate for Compute Engine (formerly known as Velostrata) to migrate those servers to Compute Engine. Create an internal load balancer (ILB) that uses storage.googleapis.com as backend. Configure your new instances to use this ILB as proxy.",
          "Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com."
        ],
        "correctAnswer": [
          "Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com."
        ],
        "hint": "Private Google Access for hybrid cloud connectivity",
        "explanation": "### Technical Implementation\n1. **VPC Peering/Interconnect**:\n```bash\ngcloud compute networks peerings create onprem-to-cloud \\\n  --network=vpc-name \\\n  --peer-network=projects/project-id/global/networks/onprem-net\n```\n2. **DNS Configuration**:\n```\n; Zone file entry\n*.googleapis.com. IN CNAME restricted.googleapis.com.\n```\n3. **Security controls**:\n- Private API access\n- No public IP exposure\n- End-to-private encryption\n\n🔗 Reference: [Private Google Access](https://cloud.google.com/vpc/docs/private-access-options)"
      },
      {
        "id": 64,
        "question": "You want to deploy an application on Cloud Run that processes messages from a Cloud Pub/Sub topic. You want to follow Google-recommended practices. What should you do?",
        "type": "single",
        "options": [
          "Create a Cloud Function that uses a Cloud Pub/Sub trigger on that topic. Call your application on Cloud Run from the Cloud Function for every message.",
          "Grant the Pub/Sub Subscriber role to the service account used by Cloud Run. Create a Cloud Pub/Sub subscription for that topic. Make your application pull messages from that subscription.",
          "Create a service account. Give the Cloud Run Invoker role to that service account for your Cloud Run application. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint.",
          "Deploy your application on Cloud Run on GKE with the connectivity set to Internal. Create a Cloud Pub/Sub subscription for that topic. In the same Google Kubernetes Engine cluster as your application, deploy a container that takes the messages and sends them to your application."
        ],
        "correctAnswer": [
          "Create a service account. Give the Cloud Run Invoker role to that service account for your Cloud Run application. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint."
        ],
        "hint": "Use Pub/Sub push subscriptions with authentication",
        "explanation": "### Implementation Flow\n1. Service Account Creation:\n```bash\ngcloud iam service-accounts create cloudrun-pubsub-invoker\n```\n2. Role Assignment:\n```bash\ngcloud run services add-iam-policy-binding my-service \\\n  --member=serviceAccount:cloudrun-pubsub-invoker@project.iam.gserviceaccount.com \\\n  --role=roles/run.invoker\n```\n3. Pub/Sub Subscription:\n```bash\ngcloud pubsub subscriptions create cloudrun-sub \\\n  --topic=my-topic \\\n  --push-endpoint=https://my-service-run-abcdef-uc.a.run.app/ \\\n  --push-auth-service-account=cloudrun-pubsub-invoker@project.iam.gserviceaccount.com\n```\n\n🔗 Reference: [Pub/Sub to Cloud Run](https://cloud.google.com/run/docs/triggering/pubsub-push)"
      },
      {
        "id": 65,
        "question": "You need to deploy an application, which is packaged in a container image, in a new project. The application exposes an HTTP endpoint and receives very few requests per day. You want to minimize costs. What should you do?",
        "type": "single",
        "options": [
          "Deploy the container on Cloud Run.",
          "Deploy the container on Cloud Run on GKE.",
          "Deploy the container on App Engine Flexible.",
          "Deploy the container on GKE with cluster autoscaling and horizontal pod autoscaling enabled."
        ],
        "correctAnswer": ["Deploy the container on Cloud Run."],
        "hint": "Serverless platforms with scale-to-zero capability",
        "explanation": "### Cost Comparison (Per Month EST)\n| Service | Base Cost | Per Request |\n|---------|-----------|-------------|\n| Cloud Run | $0 | $0.40/million requests |\n| Cloud Run on GKE | ~$73 (1 node) | Same as above |\n| GKE Autoscaling | ~$73 (min node) | $0 |\n| App Engine Flex | ~$50 (basic scaling) | Varies |\n\n### Key Advantage\n- Zero cost when idle\n- No infrastructure management\n- Automatic scaling\n\n🔗 Reference: [Cloud Run Pricing](https://cloud.google.com/run/pricing)"
      },
      [
        {
          "id": 66,
          "question": "Your company has an existing GCP organization with hundreds of projects and a billing account. Your company recently acquired another company that also has hundreds of projects and its own billing account. You would like to consolidate all GCP costs of both GCP organizations onto a single invoice. You would like to consolidate all costs as of tomorrow. What should you do?",
          "type": "single",
          "options": [
            "Link the acquired company's projects to your company's billing account.",
            "Configure the acquired company's billing account and your company's billing account to export the billing data into the same BigQuery dataset.",
            "Migrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.",
            "Create a new GCP organization and a new billing account. Migrate the acquired company's projects and your company's projects into the new GCP organization and link the projects to the new billing account."
          ],
          "correctAnswer": [
            "Link the acquired company's projects to your company's billing account."
          ],
          "hint": "Cross-organization billing consolidation without project migration",
          "explanation": "### Implementation Steps\n1. Identify projects to consolidate\n2. Use billing admin permissions\n3. Link projects directly\n\n### Benefits\n- Immediate effect (next day)\n- No disruption to existing resources\n- Maintains organizational structure\n- Simple administrative process\n\n### Key Points\n- No need to migrate projects\n- Preserves existing permissions\n- Single invoice outcome\n- Minimal operational impact\n\n🔗 Reference: [Managing Project Billing](https://cloud.google.com/billing/docs/how-to/modify-project)"
        },
        {
          "id": 67,
          "question": "You built an application on Google Cloud that uses Cloud Spanner. Your support team needs to monitor the environment but should not have access to table data. You need a streamlined solution to grant the correct permissions to your support team, and you want to follow Google-recommended practices. What should you do?",
          "type": "single",
          "options": [
            "Add the support team group to the roles/monitoring.viewer role.",
            "Add the support team group to the roles/spanner.databaseUser role.",
            "Add the support team group to the roles/spanner.databaseReader role.",
            "Add the support team group to the roles/stackdriver.accounts.viewer role."
          ],
          "correctAnswer": [
            "Add the support team group to the roles/monitoring.viewer role."
          ],
          "hint": "Monitoring access without data exposure",
          "explanation": "### Role Comparison\n| Role | Access Level | Data Access | Monitoring |\n|------|--------------|-------------|------------|\n| monitoring.viewer | Metrics only | No | Yes |\n| spanner.databaseUser | Full data | Yes | No |\n| spanner.databaseReader | Read data | Yes | No |\n| stackdriver.accounts.viewer | Limited | No | Partial |\n\n### Best Practice Reasoning\n- Separates concerns\n- Follows least privilege\n- Enables monitoring without data access\n- Supports operational needs\n\n🔗 Reference: [Cloud IAM Roles](https://cloud.google.com/iam/docs/understanding-roles)"
        },
        {
          "id": 68,
          "question": "For analysis purposes, you need to send all the logs from all of your Compute Engine instances to a BigQuery dataset called platform-logs. You have already installed the Cloud Logging agent on all the instances. You want to minimize cost. What should you do?",
          "type": "single",
          "options": [
            "1. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. 2. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs.",
            "1. In Cloud Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. 2. Create a Cloud Function that is triggered by messages in the logs topic. 3. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset.",
            "1. In Cloud Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination.",
            "1. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. 2. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp > DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) 3. Use Cloud Scheduler to trigger this Cloud Function once a day."
          ],
          "correctAnswer": [
            "1. In Cloud Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination."
          ],
          "hint": "Direct log export to BigQuery without intermediate services",
          "explanation": "### Solution Components\n| Component | Purpose | Cost Impact |\n|-----------|----------|-------------|\n| Log Filter | Target GCE logs | Reduces volume |\n| Direct Export | No intermediary | Minimizes cost |\n| BigQuery Sink | Native integration | Standard pricing |\n\n### Implementation Benefits\n- No additional services needed\n- Automatic log streaming\n- Built-in filtering\n- Native BigQuery integration\n\n### Cost Efficiency\n- Eliminates Pub/Sub costs\n- No Cloud Functions charges\n- Optimized BigQuery ingestion\n\n🔗 Reference: [Exporting Logs](https://cloud.google.com/logging/docs/export/configure_export_v2)"
        },
        {
          "id": 69,
          "question": "You are using Deployment Manager to create a Google Kubernetes Engine cluster. Using the same Deployment Manager deployment, you also want to create a DaemonSet in the kube-system namespace of the cluster. You want a solution that uses the fewest possible services. What should you do?",
          "type": "single",
          "options": [
            "Add the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet.",
            "Use the Deployment Manager Runtime Configurator to create a new Config resource that contains the DaemonSet definition.",
            "With Deployment Manager, create a Compute Engine instance with a startup script that uses kubectl to create the DaemonSet.",
            "In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value."
          ],
          "correctAnswer": [
            "Add the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet."
          ],
          "hint": "Direct Kubernetes resource management through Deployment Manager",
          "explanation": "### Implementation Steps\n| Step | Purpose | Configuration |\n|------|----------|---------------|\n| Type Provider | Register K8s API | Enables native access |\n| Resource Definition | Define DaemonSet | Uses K8s schema |\n| Deployment | Single operation | Maintains consistency |\n\n### Technical Details\n```yaml\nresources:\n- name: k8s-type-provider\n  type: deploymentmanager.v2beta.typeProvider\n  properties:\n    descriptorUrl: https://$(ref.gke-cluster.endpoint)/swaggerapi/api/v1\n    collectionOverrides:\n    - collection: daemonsets\n      options:\n        validation:\n          schemaValidation: IGNORE_WITH_WARNINGS\n```\n\n🔗 Reference: [Type Providers](https://cloud.google.com/deployment-manager/docs/configuration/type-providers)"
        },
        {
          "id": 70,
          "question": "You are building an application that will run in your data center. The application will use Google Cloud Platform (GCP) services like AutoML. You created a service account that has appropriate access to AutoML. You need to enable authentication to the APIs from your on-premises environment. What should you do?",
          "type": "single",
          "options": [
            "Use service account credentials in your on-premises application.",
            "Use gcloud to create a key file for the service account that has appropriate permissions.",
            "Set up direct interconnect between your data center and Google Cloud Platform to enable authentication for your on-premises applications.",
            "Go to the IAM & admin console, grant a user account permissions similar to the service account permissions, and use this user account for authentication from your data center."
          ],
          "correctAnswer": [
            "Use gcloud to create a key file for the service account that has appropriate permissions."
          ],
          "hint": "Secure service account authentication for on-premises applications",
          "explanation": "### Implementation Process\n| Step | Command | Purpose |\n|------|---------|----------|\n| Create Key | `gcloud iam service-accounts keys create` | Generate credentials |\n| Secure Storage | Environment variable setup | Protect key file |\n| Application Config | GOOGLE_APPLICATION_CREDENTIALS | Enable auth |\n\n### Security Considerations\n- Regular key rotation\n- Secure key storage\n- Minimal permissions\n- Audit logging\n\n### Best Practices\n- Use environment variables\n- Implement key rotation\n- Monitor key usage\n- Follow least privilege\n\n🔗 Reference: [Service Account Authentication](https://cloud.google.com/docs/authentication/production)"
        },
        {
          "id": 71,
          "question": "Your organization plans to migrate its financial transaction monitoring application to Google Cloud. Auditors need to view the data and run reports in BigQuery, but they are not allowed to perform transactions in the application. You are leading the migration and want the simplest solution that will require the least amount of maintenance. What should you do?",
          "type": "single",
          "options": [
            "Assign roles/bigquery.dataViewer to the individual auditors.",
            "Create a group for auditors and assign roles/viewer to them.",
            "Create a group for auditors, and assign roles/bigquery.dataViewer to them.",
            "Assign a custom role to each auditor that allows view-only access to BigQuery."
          ],
          "correctAnswer": [
            "Create a group for auditors, and assign roles/bigquery.dataViewer to them."
          ],
          "hint": "Consider scalability and maintenance of permissions management",
          "explanation": "### Key Considerations\n\n| Approach | Maintenance | Scalability | Security |\n|----------|-------------|-------------|----------|\n| Individual IAM | High | Poor | Good |\n| Group-based | Low | Excellent | Good |\n| Custom Roles | High | Poor | Complex |\n\n### Why Group + BigQuery.dataViewer is Best\n\n1. **Group Management Benefits**:\n   - Single point of permission management\n   - Easy to add/remove members\n   - Consistent access control\n   - Reduced administrative overhead\n\n2. **roles/bigquery.dataViewer Permissions**:\n   - Read-only access to datasets\n   - Can run queries\n   - Cannot modify data\n   - Meets auditor requirements\n\n3. **Maintenance Advantages**:\n   - No custom role maintenance\n   - Automated group synchronization possible\n   - Simple audit trail\n   - Standard role updates handled by Google\n\n### Implementation Steps\n```bash\n# Create group (via Admin Console)\n\n# Assign role to group\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n    --member='group:auditors@company.com' \\\n    --role='roles/bigquery.dataViewer'\n```\n\n🔗 Reference: [Understanding BigQuery IAM Roles](https://cloud.google.com/bigquery/docs/access-control)"
        },
        {
          "id": 72,
          "question": "You are managing your company’s first Google Cloud project. Project leads, developers, and internal testers will participate in the project, which includes sensitive information. You need to ensure that only specific members of the development team have access to sensitive information. You want to assign the appropriate Identity and Access Management (IAM) roles that also require the least amount of maintenance. What should you do?",
          "type": "single",
          "options": [
            "Assign a basic role to each user.",
            "Create groups. Assign a basic role to each group, and then assign users to groups.",
            "Create groups. Assign a Custom role to each group, including those who should have access to sensitive data. Assign users to groups.",
            "Create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data. Assign users to groups."
          ],
          "correctAnswer": [
            "Create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data. Assign users to groups."
          ],
          "hint": "Focus on predefined roles and groups to minimize effort and avoid reinventing security policies.",
          "explanation": "### Key Considerations\n| Role Management | Maintenance | Security |\n|------------------|-------------|----------|\n| Basic Roles      | High        | Low      |\n| Custom Roles     | High        | Medium   |\n| Predefined Roles | Low         | High     |\n\n### Why Predefined Roles + Groups\n1. **Predefined Roles**:\n   - Designed by Google for specific use cases\n   - Appropriate level of granularity for least-privilege access\n   - No custom configuration required\n\n2. **Groups**:\n   - Simplified user management\n   - Centralized role assignments\n   - Easy to add/remove users\n\n### Best Practices\n- Use IAM Predefined roles like \"roles/viewer,\" \"roles/editor,\" or \"roles/secretmanager.secretAccessor\" for specific responsibilities.\n- Create separate groups for leads, developers, testers, and sensitive data access.\n\n### Implementation Steps\n1. **Create Groups**:\n   ```bash\n   gcloud identity groups create --display-name=\"Development Leads\"\n   gcloud identity groups create --display-name=\"Developers\"\n   gcloud identity groups create --display-name=\"Testers\"\n   gcloud identity groups create --display-name=\"Sensitive Data Access\"\n   ```\n2. **Assign IAM Predefined Roles to Groups**:\n   ```bash\n   gcloud projects add-iam-policy-binding PROJECT_ID \\\n       --member='group:developers@company.com' \\\n       --role='roles/editor'\n\n   gcloud projects add-iam-policy-binding PROJECT_ID \\\n       --member='group:sensitive-data-access@company.com' \\\n       --role='roles/secretmanager.secretAccessor'\n   ```\n3. **Add Users to Groups**:\n   ```bash\n   gcloud identity groups memberships add \\\n       --member-email=user1@company.com --group-email=developers@company.com\n   ```\n\n🔗 Reference: [Understanding IAM Roles](https://cloud.google.com/iam/docs/understanding-roles)"
        },
        {
          "id": 73,
          "question": "You are responsible for monitoring all changes in your Cloud Storage and Firestore instances. For each change, you need to invoke an action that will verify the compliance of the change in near real time. You want to accomplish this with minimal setup. What should you do?",
          "type": "single",
          "options": [
            "Use the trigger mechanism in each datastore to invoke the security script.",
            "Use Cloud Function events, and call the security script from the Cloud Function triggers.",
            "Use a Python script to get logs of the datastores, analyze them, and invoke the security script.",
            "Redirect your data-changing queries to an App Engine application, and call the security script from the application."
          ],
          "correctAnswer": [
            "Use Cloud Function events, and call the security script from the Cloud Function triggers."
          ],
          "hint": "Google Cloud Functions offer event-driven execution with minimal setup.",
          "explanation": "### Why Use Cloud Function Events?\n1. **Near Real-time Execution**:\n   - Cloud Functions are triggered by events, ensuring timely compliance checks for changes in both Cloud Storage and Firestore.\n\n2. **Minimal Setup**:\n   - Cloud Functions are serverless, eliminating the need to manage infrastructure.\n   - Native integration with Cloud Storage and Firestore events simplifies configuration.\n\n3. **Compliance Use Case**:\n   - For Cloud Storage, you can trigger the function on events like object creation, deletion, or metadata updates using `onFinalize` or `onMetadataUpdate` triggers.\n   - For Firestore, you can trigger the function on document creation, updates, or deletion.\n\n### Implementation Steps\n1. **Create a Cloud Function for Cloud Storage**:\n   ```bash\n   gcloud functions deploy check-compliance-storage \\\n       --runtime python39 \\\n       --trigger-resource YOUR_BUCKET_NAME \\\n       --trigger-event google.storage.object.finalize\n   ```\n2. **Create a Cloud Function for Firestore**:\n   ```bash\n   gcloud functions deploy check-compliance-firestore \\\n       --runtime python39 \\\n       --trigger-event google.firestore.document.create \\\n       --trigger-resource \"projects/YOUR_PROJECT/databases/(default)/documents/YOUR_COLLECTION/{documentId}\"\n   ```\n3. **Add Security Script Logic**:\n   - Write your compliance verification logic in the function.\n\n\n### Why Other Options Are Not Ideal\n- **A. Trigger Mechanism in Each Datastore:**\n   - Firestore and Cloud Storage do not have an inherent \"trigger\" mechanism outside the event-based Cloud Functions.\n\n- **C. Redirect to App Engine:**\n   - Adds unnecessary complexity and requires managing infrastructure.\n\n- **D. Python Script with Logs:**\n   - Not near real-time and introduces latency due to log extraction and processing.\n\n🔗 Reference: [Cloud Functions for Event-driven Applications](https://cloud.google.com/functions/docs/concepts/events-triggers)"
        },
        {
          "id": 74,
          "question": "Your application needs to process a significant rate of transactions. The rate of transactions exceeds the processing capabilities of a single virtual machine (VM). You want to spread transactions across multiple servers in real time and in the most cost-effective manner. What should you do?",
          "type": "single",
          "options": [
            "Send transactions to BigQuery. On the VMs, poll for transactions that do not have the ‘processed’ key, and mark them ‘processed’ when done.",
            "Set up Cloud SQL with a memory cache for speed. On your multiple servers, poll for transactions that do not have the ‘processed’ key, and mark them ‘processed’ when done.",
            "Send transactions to Pub/Sub. Process them in VMs in a managed instance group.",
            "Record transactions in Cloud Bigtable, and poll for new transactions from the VMs."
          ],
          "correctAnswer": [
            "Send transactions to Pub/Sub. Process them in VMs in a managed instance group."
          ],
          "hint": "Focus on real-time message distribution and scalable processing capabilities.",
          "explanation": "### Why Pub/Sub with Managed Instance Group?\n\n1. **Pub/Sub for Real-time Message Distribution**:\n   - Pub/Sub acts as a globally distributed message queue, capable of handling high throughput and large burst rates.\n   - Automatically scales to handle increased transaction rates without additional setup.\n\n2. **Managed Instance Group (MIG) Processing Power**:\n   - MIG ensures scalability by automatically increasing or decreasing VM instances based on workload.\n   - Distributes messages across multiple VMs in parallel using pull or push subscriptions from Pub/Sub.\n\n3. **Cost Efficiency**:\n   - You only pay for the messages processed and the computing resources used.\n   - Avoids polling inefficiencies and unnecessary resource usage.\n\n### Implementation Steps\n1. **Setup Pub/Sub Topic and Subscription**:\n   ```bash\n   gcloud pubsub topics create transactions-topic\n   gcloud pubsub subscriptions create transactions-sub \\\n       --topic=transactions-topic\n   ```\n2. **Create a Managed Instance Group**:\n   - Use a startup script or application code within the VM template to pull and process messages from Pub/Sub:\n   ```bash\n   gcloud compute instance-templates create transaction-processor-template \\\n       --metadata=startup-script='python process_transactions.py'\n\n   gcloud compute instance-groups managed create transaction-processors \\\n       --base-instance-name=transaction-processor \\\n       --template=transaction-processor-template \\\n       --size=3\n   ```\n3. **Code to Pull Messages from Pub/Sub**:\n   Example Python logic:\n   ```python\n   from google.cloud import pubsub_v1\n\n   def callback(message):\n       print(f'Received message: {message.data}')\n       # Process transaction here\n       message.ack()\n\n   subscriber = pubsub_v1.SubscriberClient()\n   subscription_path = subscriber.subscription_path('your-project-id', 'transactions-sub')\n\n   streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)\n   try:\n       streaming_pull_future.result()\n   except KeyboardInterrupt:\n       streaming_pull_future.cancel()\n   ```\n\n### Why Other Options Are Not Ideal\n- **A: BigQuery**:\n   - Heavy query costs and not suited for high-frequency real-time transactional processing.\n\n- **B: Cloud SQL**:\n   - Polling introduces delays and inefficiencies.\n   - Scalability is more complex as Cloud SQL has limitations in terms of concurrent connections.\n\n- **D: Cloud Bigtable**:\n   - Designed for large analytical workloads, not real-time transaction polling.\n   - Requires extra code for polling, introducing delays.\n\n🔗 Reference: [Cloud Pub/Sub Overview](https://cloud.google.com/pubsub/docs/overview) | [Managed Instance Group Autohealing](https://cloud.google.com/compute/docs/instance-groups/autohealing-overview)"
        },
        {
          "id": 75,
          "question": "Your team needs to directly connect your on-premises resources to several virtual machines inside a virtual private cloud (VPC). You want to provide your team with fast and secure access to the VMs with minimal maintenance and cost. What should you do?",
          "type": "single",
          "options": [
            "Set up Cloud Interconnect.",
            "Use Cloud VPN to create a bridge between the VPC and your network.",
            "Assign a public IP address to each VM, and assign a strong password to each one.",
            "Start a Compute Engine VM, install a software router, and create a direct tunnel to each VM."
          ],
          "correctAnswer": [
            "Use Cloud VPN to create a bridge between the VPC and your network."
          ],
          "hint": "Consider secure connectivity with minimal maintenance and lower costs.",
          "explanation": "### Why Use Cloud VPN?\n\n1. **Secure Connectivity**:\n   - Cloud VPN establishes a secure IPSec encrypted tunnel between your on-premises network and the VPC.\n   - Ensures data in transit is secure.\n\n2. **Low Cost**:\n   - Provides a cost-effective solution compared to Cloud Interconnect, which requires higher setup and ongoing costs.\n\n3. **Minimal Maintenance**:\n   - Fully managed by Google, reducing the operational overhead.\n   - Easily configurable through the Google Cloud Console or `gcloud` CLI.\n\n### Implementation Steps\n1. **Create a VPN Gateway in the VPC**:\n   ```bash\n   gcloud compute vpn-gateways create my-vpn-gateway \\\n       --region=REGION \\\n       --network=NETWORK_NAME\n   ```\n2. **Set Up VPN Tunnel**:\n   ```bash\n   gcloud compute vpn-tunnels create my-vpn-tunnel \\\n       --peer-address=ON_PREM_VPN_IP \\\n       --region=REGION \\\n       --ike-version=2 \\\n       --shared-secret=MY_SECRET\n   ```\n3. **Configure On-Premises Gateway**:\n   - Follow the setup instructions for your on-premises VPN device to establish the connection.\n\n### Why Not the Other Options?\n- **A: Cloud Interconnect**:\n   - Best for high-bandwidth, low-latency use cases but comes with higher setup costs and complex management.\n   - Overkill if VPN can meet your requirements.\n\n- **C: Assign Public IPs with Strong Passwords**:\n   - Public IPs expose your VMs to the internet, increasing attack risks.\n   - A challenging approach to manage securely at scale.\n\n- **D: Software Router on Compute Engine**:\n   - Complex to configure and maintain.\n   - Lacks the scalability and reliability of Cloud VPN solutions.\n\n🔗 Reference: [Cloud VPN Overview](https://cloud.google.com/vpn/docs/overview)"
        },
        {
          "id": 76,
          "question": "You are implementing Cloud Storage for your organization. You need to follow your organization’s regulations. They include: 1) Archive data older than one year. 2) Delete data older than 5 years. 3) Use standard storage for all other data. You want to implement these guidelines automatically and in the simplest manner available. What should you do?",
          "type": "single",
          "options": [
            "Set up Object Lifecycle management policies.",
            "Run a script daily. Copy data that is older than one year to an archival bucket, and delete five-year-old data.",
            "Run a script daily. Set storage class to ARCHIVE for data that is older than one year, and delete five-year-old data.",
            "Set up default storage class for three buckets named: STANDARD, ARCHIVE, DELETED. Use a script to move the data in the appropriate bucket when its condition matches your company guidelines."
          ],
          "correctAnswer": ["Set up Object Lifecycle management policies."],
          "hint": "Leverage built-in Cloud Storage features for automated data management.",
          "explanation": "### Why Object Lifecycle Management?\n\n1. **Automation and Simplicity**:\n   - Object Lifecycle Management policies automate the transition of objects between storage classes and deletion based on age, eliminating the need for manual scripting or intervention.\n\n2. **Direct Control within Buckets**:\n   - Policies are applied at the bucket level, managing data directly within the storage environment. This avoids complexities of moving data between buckets or managing multiple buckets for different storage durations.\n\n3. **Cost Optimization**:\n   - Automatically transitions data to cheaper storage classes like Archive when no longer frequently accessed, aligning with the organization's regulation for archival.\n\n### Implementation Steps\n1. **Go to the Cloud Storage Browser in the Cloud Console**:\n   [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)\n2. **Select your bucket and navigate to the \"Lifecycle\" tab.**\n3. **Click \"Add rule\" and configure the following actions**:\n   - **Action:** Set Storage Class to Archive\n     - **Age:** 365 Days\n   - **Action:** Delete Object\n     - **Age:** 1825 Days (5 years)\n\n### Why Not the Other Options?\n- **B and C: Running Daily Scripts**:\n   - Introduces operational overhead and potential for errors.\n   - Less efficient than built-in lifecycle management.\n\n- **D: Multiple Buckets with Scripting**:\n   - Complex to manage and prone to errors.\n   - Increases storage management complexity.\n\n🔗 Reference: [Object Lifecycle Management](https://cloud.google.com/storage/docs/lifecycle)"
        },
        {
          "id": 77,
          "question": "You are creating a Cloud IOT application requiring data storage of up to 10 petabytes (PB). The application must support high-speed reads and writes of small pieces of data, but your data schema is simple. You want to use the most economical solution for data storage.",
          "type": "single",
          "options": [
            "Store the data in Cloud Spanner, and add an in-memory cache for speed.",
            "Store the data in Cloud Storage, and distribute the data through Cloud CDN for speed.",
            "Store the data in Cloud Bigtable, and implement the business logic in the programming language of your choice.",
            "Use BigQuery, and implement the business logic in SQL."
          ],
          "correctAnswer": [
            "Store the data in Cloud Bigtable, and implement the business logic in the programming language of your choice."
          ],
          "hint": "Think about the scale of data, the nature of the application (IoT), and the need for high-speed read/writes with a simple schema.",
          "explanation": "### Why Cloud Bigtable is the Best Fit:\n\n1. **Massive Scalability for IoT Data:**\n    - Cloud Bigtable excels at handling petabytes of data, making it ideal for the large-scale data ingestion common in IoT applications.\n\n2. **High-Speed Read/Write Performance:**\n    - Designed for low-latency read/writes of small data pieces (row-column structure), crucial for real-time IoT applications requiring quick data access.\n\n3. **Simple Schema and Cost Optimization:**\n    - A simple schema aligns well with Bigtable's data model.\n    - Cloud Bigtable offers a cost-effective solution compared to Spanner for scenarios with simple schemas.\n\n4. **Flexible Business Logic:**\n    - You can implement business logic using your preferred programming language (e.g., Java, Python) with Bigtable client libraries, providing flexibility.\n\n### Why Other Options Are Not As Suitable:\n\n- **A: Cloud Spanner:**\n    - Better for complex schemas and strong consistency needs.\n    - More expensive than Bigtable for this use case.\n    - Adding an in-memory cache adds complexity and cost.\n\n- **B: Cloud Storage with CDN:**\n    - Cloud Storage is ideal for object storage, not high-speed transactional data access.\n    - CDN is for content distribution, not real-time read/writes.\n\n- **D: BigQuery:**\n    - Excellent for analytical queries but not ideal for high-frequency, low-latency transactions typical in IoT scenarios.\n\n### Additional Considerations:\n\n- **Data Model:** Design your Bigtable schema carefully to optimize for read/write patterns.\n- **Client Libraries:** Leverage Bigtable client libraries for your chosen language to interact with the database efficiently.\n- **Monitoring:** Implement monitoring to track performance and ensure optimal resource utilization.\n\n🔗 Reference: [Cloud Bigtable Overview](https://cloud.google.com/bigtable/docs/overview)"
        },
        {
          "id": 78,
          "question": "You have created a Kubernetes deployment on Google Kubernetes Engine (GKE) that has a backend service. You also have pods that run the frontend service. You want to ensure that there is no interruption in communication between your frontend and backend service pods if they are moved or restarted. What should you do?",
          "type": "single",
          "options": [
            "Create a service that groups your pods in the backend service, and tell your frontend pods to communicate through that service.",
            "Create a DNS entry with a fixed IP address that the frontend service can use to reach the backend service.",
            "Assign static internal IP addresses that the frontend service can use to reach the backend pods.",
            "Assign static external IP addresses that the frontend service can use to reach the backend pods."
          ],
          "correctAnswer": [
            "Create a service that groups your pods in the backend service, and tell your frontend pods to communicate through that service."
          ],
          "hint": "Utilize Kubernetes' built-in service discovery mechanism.",
          "explanation": "### Why Create a Kubernetes Service?\n\nKubernetes Services provide a stable endpoint for communication between pods, even if those pods are moved or restarted. Here's why this is the best approach:\n\n* **Abstraction and Service Discovery:** Services abstract away the dynamic nature of pod IPs, providing a consistent point of access for your frontend.\n* **Load Balancing and Fault Tolerance:** Services distribute traffic across healthy backend pods, ensuring high availability.\n* **Dynamic Updates:** As your backend scales or pods are rescheduled, the service automatically reflects these changes.\n\n### Why Other Options Are Not Ideal:\n\n* **DNS with Fixed IP:**  Pod IPs are dynamic, making fixed DNS entries unreliable.\n* **Static Internal IPs:**  Kubernetes manages IP allocation; static assignments are not recommended.\n* **Static External IPs:**  Exposing backend pods directly to the internet is a security risk and bypasses Kubernetes' networking model.\n\nBy using a Kubernetes Service, you leverage the platform's features for service discovery, load balancing, and fault tolerance, ensuring seamless communication between your frontend and backend."
        },
        {
          "id": 79,
          "question": "You are responsible for the user-management service for your global company. The service will add, update, delete, and list addresses. Each of these operations is implemented by a Docker container microservice. The processing load can vary from low to very high. You want to deploy the service on Google Cloud for scalability and minimal administration. What should you do?",
          "type": "single",
          "options": [
            "Deploy your Docker containers into Cloud Run.",
            "Start each Docker container as a managed instance group.",
            "Deploy your Docker containers into Google Kubernetes Engine.",
            "Combine the four microservices into one Docker image, and deploy it to the App Engine instance."
          ],
          "correctAnswer": ["Deploy your Docker containers into Cloud Run."],
          "hint": "Focus on serverless solutions that abstract infrastructure management while enabling automatic scaling.",
          "explanation": "### Why Cloud Run is the Best Choice:\n\n1. **Fully Serverless & Scalable:**\n   - Cloud Run automatically scales from 0 to many instances based on incoming HTTP requests, handling variable workloads seamlessly.\n   - No infrastructure management required (no nodes, clusters, or VMs to manage).\n\n2. **Microservices Compatibility:**\n   - Deploy each operation (add/update/delete/list) as independent services with separate containers, preserving modularity.\n   - Cloud Run supports per-service scaling and versioning.\n\n3. **Cost Efficiency:**\n   - You only pay for the CPU and memory used during request processing.\n   - Zero cost when no traffic (scale-to-zero capability).\n\n4. **Simplified Operations:**\n   - Automatic TLS, logging, monitoring, and CI/CD integration.\n\n### Why Other Options Fail:\n\n- **B: Managed Instance Groups (MIGs):**\n   - Requires manual scaling configuration.\n   - Overkill for HTTP-based microservices; MIGs are better for stateful VM workloads.\n\n- **C: Google Kubernetes Engine (GKE):**\n   - Introduces cluster/node management overhead.\n   - Costly for small teams due to node pool administration.\n   - Uneconomical for low-traffic periods since nodes persist.\n\n- **D: Combining Services into One Image (App Engine):**\n   - Defeats the purpose of microservices by coupling functionalities.\n   - Limited flexibility for independent scaling and updates.\n\n### Implementation Steps:\n1. Build Docker images for each microservice.\n2. Deploy to Cloud Run with commands like:\n   ```bash\n   gcloud run deploy add-service \\\n       --image=gcr.io/PROJECT_ID/add-service \\\n       --platform=managed \\\n       --region=us-central1\n   ```\n3. Use concurrency settings and scaling parameters to fine-tune performance.\n\n🔗 Reference: [Cloud Run for Stateless Containers](https://cloud.google.com/run/docs/overview)"
        },
        {
          "id": 80,
          "question": "You provide a service that you need to open to everyone in your partner network. You have a server and an IP address where the application is located. You do not want to have to change the IP address on your DNS server if your server crashes or is replaced. You also want to avoid downtime and deliver a solution for minimal cost and setup. What should you do?",
          "type": "single",
          "options": [
            "Create a script that updates the IP address for the domain when the server crashes or is replaced.",
            "Reserve a static internal IP address, and assign it using Cloud DNS.",
            "Reserve a static external IP address, and assign it using Cloud DNS.",
            "Use the Bring Your Own IP (BYOIP) method to use your own IP address."
          ],
          "correctAnswer": [
            "Reserve a static external IP address, and assign it using Cloud DNS."
          ],
          "hint": "Focus on maintaining a consistent external IP address while minimizing manual updates.",
          "explanation": "### Why Reserve a Static External IP Address?\n\n1. **Consistent IP Address:**\n   - A static external IP address ensures that even if your server is replaced or restarted, the IP address remains constant, avoiding the need to update DNS records.\n\n2. **Integration with Cloud DNS:**\n   - Use the static external IP address in your DNS configuration, ensuring high availability and no downtime.\n\n3. **Minimal Cost and Setup:**\n   - Reserving a static IP address is straightforward and cost-efficient.\n   - No need for complex scripts or manual updates.\n\n### Implementation Steps:\n\n1. **Reserve the Static External IP Address:**\n   ```bash\n   gcloud compute addresses create my-static-ip \\\n       --region=REGION\n   ```\n\n2. **Assign the Static IP Address to the Server:**\n   ```bash\n   gcloud compute instances update my-instance \\\n       --address=MY-STATIC-IP\n   ```\n\n3. **Update DNS Records in Cloud DNS:**\n   - Point your domain’s DNS A records to the static external IP address.\n\n### Why Not the Other Options?\n\n- **A: Use a Script to Update IP:**\n   - This introduces unnecessary complexity and potential delays in DNS propagation.\n\n- **B: Reserve a Static Internal IP:**\n   - Internal IPs are only accessible within a VPC and are not usable for external partner access.\n\n- **D: Bring Your Own IP (BYOIP):**\n   - BYOIP is meant for organizations using their own, pre-registered IP blocks.\n   - It’s more complex and intended for advanced use cases.\n\n🔗 Reference: [Static External IP Address](https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address)"
        },
        {
          "id": 81,
          "question": "Your team is building the development, test, and production environments for your project deployment in Google Cloud. You need to efficiently deploy and manage these environments and ensure that they are consistent. You want to follow Google-recommended practices. What should you do?",
          "type": "single",
          "options": [
            "Create a Cloud Shell script that uses gcloud commands to deploy the environments.",
            "Create one Terraform configuration for all environments. Parameterize the differences between environments.",
            "For each environment, create a Terraform configuration. Use them for repeated deployment. Reconcile the templates periodically.",
            "Use the Cloud Foundation Toolkit to create one deployment template that will work for all environments, and deploy with Terraform."
          ],
          "correctAnswer": [
            "Use the Cloud Foundation Toolkit to create one deployment template that will work for all environments, and deploy with Terraform."
          ],
          "hint": "Leverage prebuilt deployment templates and parameterization to ensure consistency and efficiency.",
          "explanation": "### Why Use the Cloud Foundation Toolkit with Terraform?\n\n1. **Prebuilt and Reusable Templates:**\n   - The Cloud Foundation Toolkit provides modular and pretested Terraform templates, aligning with Google Cloud best practices for infrastructure deployment.\n   - Templates cover common patterns, reducing development time and ensuring consistency across environments (e.g., development, test, production).\n\n2. **Parameterization for Flexibility:**\n   - You can efficiently customize templates using variables for environment-specific differences, such as machine types or network configurations.\n   - Ensures uniform deployment processes while tailoring environments to specific needs.\n\n3. **Enhanced Efficiency:**\n   - Reduces complexity by avoiding starting from scratch.\n   - Streamlines the process of deploying infrastructure repeatedly and consistently.\n\n### Why Not the Other Options?\n\n- **A: Cloud Shell Script with gcloud Commands:**\n   - Hard to maintain and prone to human error; lacks reusability compared to Infrastructure-as-Code tools like Terraform.\n\n- **B: Single Terraform Configuration with Parameterization:**\n   - While this approach is valid, the Cloud Foundation Toolkit provides additional value by offering pretested modules consistent with Google Cloud practices, saving time and effort.\n\n- **C: Separate Terraform Configurations for Each Environment:**\n   - Manually managing multiple configurations results in duplication, inconsistency, and added maintenance effort.\n\n### How to Implement:\n\n1. **Use the Cloud Foundation Toolkit:**\n   - Visit the [Cloud Foundation Toolkit repository](https://github.com/GoogleCloudPlatform/cloud-foundation-toolkit) to choose templates that match your project requirements.\n\n2. **Adapt Templates:**\n   - Customize the templates to reflect project-specific parameters such as networks, machine types, and environment names.\n   - Example with Terraform:\n     ```hcl\n     module \"vm_instance\" {\n       source       = \"github.com/GoogleCloudPlatform/cloud-foundation-toolkit//modules/compute-vm\"\n       environment  = \"${var.environment}\"\n       machine_type = var.machine_type\n     }\n     ```\n\n3. **Deploy Environments:**\n   - Use Terraform commands to deploy the consistent and flexible infrastructure:\n     ```bash\n     terraform init\n     terraform apply -var environment=dev\n     terraform apply -var environment=prod\n     ```\n\n🔗 Reference: [Cloud Foundation Toolkit in Google Cloud](https://cloud.google.com/foundation-toolkit)"
        },
        {
          "id": 82,
          "question": "You receive an error message when you try to start a new VM: 'You have exhausted the IP range in your subnet.' You want to resolve the error with the least amount of effort. What should you do?",
          "type": "single",
          "options": [
            "Create a new subnet and start your VM there.",
            "Expand the CIDR range in your subnet, and restart the VM that issued the error.",
            "Create another subnet, and move several existing VMs into the new subnet.",
            "Restart the VM using exponential backoff until the VM starts successfully."
          ],
          "correctAnswer": [
            "Expand the CIDR range in your subnet, and restart the VM that issued the error."
          ],
          "hint": "Focus on modifying the existing subnet instead of creating new infrastructure.",
          "explanation": "### Why Expanding the CIDR Range Solves the Problem:\n\n1. **Direct Resolution**: Expanding the subnets IP address range (e.g., from /24 to /23) immediately increases available IP addresses without requiring VM relocation.\n\n2. **Minimal Effort**: This approach avoids:\n   - Creating new subnets\n   - Reconfiguring network routing\n   - Moving existing VMs to a new subnet\n\n### Steps to Implement:\n1. Navigate to Google Cloud Console > VPC Network > Subnets\n2. Edit the affected subnet\n3. Modify the CIDR block to a broader range (e.g., 10.0.0.0/22 → 10.0.0.0/21)\n4. Restart the VM requiring an IP\n\n### Why Other Options Are Less Efficient:\n- **A/C (New Subnet Creation/Migration)**: Requires updating firewall rules, routes, and VM configurations.\n- **D (Exponential Backoff)**: Does not resolve IP exhaustion - requests will continue failing.\n\n### Best Practice Tip**: Use Network Analyzer to proactively monitor IP utilization."
        },
        {
          "id": 83,
          "question": "You are running several related applications on Compute Engine virtual machine (VM) instances. You want to follow Google-recommended practices and expose each application through a DNS name. What should you do?",
          "type": "single",
          "options": [
            "Use the Compute Engine internal DNS service to assign DNS names to your VM instances, and make the names known to your users.",
            "Assign each VM instance an alias IP address range, and then make the internal DNS names public.",
            "Assign Google Cloud routes to your VM instances, assign DNS names to the routes, and make the DNS names public.",
            "Use Cloud DNS to translate your domain names into your IP addresses."
          ],
          "correctAnswer": [
            "Use Cloud DNS to translate your domain names into your IP addresses."
          ],
          "hint": "Focus on external visibility and user-friendly domain name resolution for related applications.",
          "explanation": "### Why Use Cloud DNS to Expose Applications on VMs?\n\n1. **Control and Flexibility**:\n   - Cloud DNS allows you to create and manage custom DNS records for translating your domain names to the IP addresses of your VM instances.\n   - Ideal for making your applications accessible with user-friendly names over the internet.\n\n2. **Google-Recommended Practice**:\n   - Cloud DNS is Google's fully managed DNS service, designed for consistent and reliable name resolution.\n   - It's the standard for exposing applications with custom domain names.\n\n3. **Public Accessibility**:\n   - Internal DNS in Compute Engine is specific to instances within the same VPC and cannot be used for public DNS resolution.\n   - Cloud DNS supports public DNS zones for external domains.\n\n### Steps to Implement:\n1. **Create a Public DNS Zone in Cloud DNS**:\n   - Use the Google Cloud Console or CLI:\n     ```bash\n     gcloud dns managed-zones create my-zone \\\n         --dns-name=\"example.com.\" \\\n         --description=\"Public zone for my applications\"\n     ```\n2. **Add DNS Records for Each Application:**\n   - Map the application's IP address to a DNS name (e.g., `app1.example.com`).\n     ```bash\n     gcloud dns record-sets transaction start --zone=my-zone\n\n     gcloud dns record-sets transaction add \"INSTANCE_IP\" \\\n         --name=\"app1.example.com.\" \\\n         --type=A \\\n         --ttl=300 \\\n         --zone=my-zone\n\n     gcloud dns record-sets transaction execute --zone=my-zone\n     ```\n3. **Point Your Domain's Name Servers to Cloud DNS:**\n   - Update your domain registrar's name server settings with the Cloud DNS name servers.\n\n### Why Not the Other Options?\n- **A: Internal DNS Service:**\n   - Internal DNS works only within the VPC and isn't exposed to public users. It is not meant for external DNS resolution.\n- **B: Alias IP Address Range:**\n   - Alias IP addresses are better suited for load balancing or container deployments, not for DNS purposes.\n- **C: Google Cloud Routes:**\n   - Routes control network traffic and are unrelated to DNS or name resolution.\n\n🔗 Reference: [Cloud DNS Documentation](https://cloud.google.com/dns/docs)"
        },
        {
          "id": 84,
          "question": "You are charged with optimizing Google Cloud resource consumption. Specifically, you need to investigate the resource consumption charges and present a summary of your findings. You want to do it in the most efficient way possible. What should you do?",
          "type": "single",
          "options": [
            "Rename resources to reflect the owner and purpose. Write a Python script to analyze resource consumption.",
            "Attach labels to resources to reflect the owner and purpose. Export Cloud Billing data into BigQuery, and analyze it with Data Studio.",
            "Assign tags to resources to reflect the owner and purpose. Export Cloud Billing data into BigQuery, and analyze it with Data Studio.",
            "Create a script to analyze resource usage based on the project to which the resources belong. In this script, use the IAM accounts and services accounts that control given resources."
          ],
          "correctAnswer": [
            "Attach labels to resources to reflect the owner and purpose. Export Cloud Billing data into BigQuery, and analyze it with Data Studio."
          ],
          "hint": "Labels allow efficient resource categorization and cost analysis, while BigQuery and Data Studio provide clear visual summaries.",
          "explanation": "### Why Use Labels and Cloud Billing Export to BigQuery?\n\n1. **Efficient Resource Categorization**:\n   - Labels are metadata key-value pairs that you attach to resources, making it easy to categorize and filter them (e.g., by owner or purpose).\n   - Google Cloud Billing supports grouping and filtering costs using these labels.\n\n2. **Data Analysis on BigQuery**:\n   - Exporting Cloud Billing data to BigQuery enables detailed analysis of resource consumption costs.\n   - You can use SQL queries to segment costs by labels, projects, or services.\n\n3. **Visual Reporting with Looker Studio (formerly Data Studio)**:\n   - Leverage Looker Studio to visualize billing data in easy-to-read dashboards and share the summary with stakeholders.\n\n### Implementation Steps:\n\n1. **Attach Labels to Resources**:\n   - Assign labels with information such as the owner, purpose, or environment.\n   - Example:\n     ```bash\n     gcloud compute instances update INSTANCE_NAME --update-labels=owner=teamA,purpose=testing\n     ```\n\n2. **Export Billing Data to BigQuery**:\n   - Enable Cloud Billing export to BigQuery to analyze your charges.\n     ```bash\n     gcloud beta billing accounts export BIGQUERY_DATASET_NAME --billing-account=ACCOUNT_ID\n     ```\n\n3. **Analyze and Visualize Billing Data**:\n   - Use SQL queries in BigQuery to identify areas of high cost based on labels and filter data.\n   - Connect BigQuery to Looker Studio for visualizations (e.g., charts summarizing costs by owner or purpose).\n\n### Why Not the Other Options?\n\n- **A: Rename Resources and Python Script**:\n   - Renaming resources lacks standardization and is less efficient compared to using labels.\n   - Writing a script for billing data parsing is redundant given BigQuery's native export capabilities.\n\n- **C: Tags instead of Labels**:\n   - Tags in Google Cloud are designed for policy enforcement, not for cost analytics or filtering.\n\n- **D: Custom Script Using IAM Accounts**:\n   - Writing a custom script to analyze resource ownership adds unnecessary complexity and lacks the precision of using labels coupled with BigQuery and visual reporting tools like Looker Studio.\n\n🔗 Reference: [Cloud Billing Export to BigQuery Documentation](https://cloud.google.com/billing/docs/how-to/export-data-bigquery)"
        },
        {
          "id": 85,
          "question": "You are creating an environment for researchers to run ad hoc SQL queries. The researchers work with large quantities of data. Although they will use the environment for an hour a day on average, the researchers need access to the functional environment at any time during the day. You need to deliver a cost-effective solution. What should you do?",
          "type": "single",
          "options": [
            "Store the data in Cloud Bigtable, and run SQL queries provided by Bigtable schema.",
            "Store the data in BigQuery, and run SQL queries in BigQuery.",
            "Create a Dataproc cluster, store the data in HDFS storage, and run SQL queries in Spark.",
            "Create a Dataproc cluster, store the data in Cloud Storage, and run SQL queries in Spark."
          ],
          "correctAnswer": [
            "Store the data in BigQuery, and run SQL queries in BigQuery."
          ],
          "hint": "Focus on simplicity, scalability, and cost efficiency for handling large datasets with SQL queries.",
          "explanation": "### Why BigQuery is the Best Fit:\n\n1. **Purpose-Built for Ad Hoc SQL Queries:**\n   - BigQuery is a serverless data warehouse designed for running ad hoc SQL queries on large datasets.\n   - Researchers can access the environment anytime without the need to manage infrastructure.\n\n2. **Cost-Effective Solution:**\n   - BigQuery uses on-demand pricing for queries, meaning you only pay for the data scanned.\n   - There are no costs when the environment is idle, which aligns with the researchers' hour-a-day usage pattern.\n\n3. **Scalability and Simplicity:**\n   - Handles petabyte-scale data effortlessly.\n   - Eliminates the need for manual management of clusters or storage systems.\n\n### Why Not the Other Options?\n\n- **A: Cloud Bigtable:**\n   - Bigtable is better suited for low-latency key-value or time-series workloads, not SQL-based analytics.\n\n- **C/D: Dataproc with HDFS or Cloud Storage:**\n   - Dataproc requires managing clusters, which may be overkill for infrequent, short-duration usage.\n   - Running and maintaining clusters adds complexity and additional costs.\n\n### Implementation Steps:\n1. Load the data into BigQuery using the `bq` command-line tool or the Cloud Console.\n   ```bash\n   bq load --source_format=CSV dataset.table gs://your-bucket/your-data.csv\n   ```\n2. Provide researchers with access to the BigQuery console and configure IAM permissions (e.g., `roles/bigquery.dataViewer`).\n3. Set up budgets and alerts to track and control query costs if needed.\n\n🔗 Reference: [BigQuery Pricing and Use Cases](https://cloud.google.com/bigquery/pricing)"
        },
        {
          "id": 86,
          "question": "You are migrating your workload from on-premises deployment to Google Kubernetes Engine (GKE). You want to minimize costs and stay within budget. What should you do?",
          "type": "single",
          "options": [
            "Configure Autopilot in GKE to monitor node utilization and eliminate idle nodes.",
            "Configure the needed capacity; the sustained use discount will make you stay within budget.",
            "Scale individual nodes up and down with the Horizontal Pod Autoscaler.",
            "Create several nodes using Compute Engine, add them to a managed instance group, and set the group to scale up and down depending on load."
          ],
          "correctAnswer": [
            "Configure Autopilot in GKE to monitor node utilization and eliminate idle nodes."
          ],
          "hint": "Look for a fully managed GKE solution that automatically optimizes resources to reduce costs.",
          "explanation": "### Why Configure GKE Autopilot?\n\n1. **Fully Managed Cost Optimization:**\n   - GKE Autopilot is a managed Kubernetes offering that automatically provisions and optimizes resources based on workload demands.\n   - It eliminates costs associated with idle nodes by scaling down and only running workloads when necessary.\n\n2. **Simplified Management:**\n   - No need to manually configure or manage node pools.\n   - GKE Autopilot eliminates the need to manage nodes, pods, or scaling policies directly, reducing administrative overhead.\n\n3. **Pay-per-Use Pricing:**\n   - Autopilot charges only for running workloads (CPU, memory, and disk resources actively used by pods).\n   - Idle resources are scaled automatically, keeping costs low when the workload is light.\n\n4. **Budget-Friendly:**\n   - By dynamically managing resources based on utilization, Autopilot helps stay within budget.\n   - Sustained use discounts apply without managing reserved capacity manually.\n\n### Why Not the Other Options?\n\n- **B: Sustained Use Discount with Fixed Capacity:**\n   - Configuring fixed node capacity could lead to overprovisioning, resulting in wasted resources and higher costs.\n\n- **C: Horizontal Pod Autoscaler (HPA):**\n   - HPA manages only pod scaling, not node-level scaling (which requires a separate setup with cluster autoscaler).\n   - Node management is still needed to prevent idle nodes.\n\n- **D: Use Compute Engine Instance Groups:**\n   - Compute Engine scales virtual machines, not containers. This approach requires additional engineering and management effort, making it inefficient for Kubernetes workloads.\n\n### Implementation Steps:\n1. **Create a GKE Autopilot Cluster:**\n   ```bash\n   gcloud container clusters create-auto my-cluster \\\n       --region=us-central1\n   ```\n2. **Deploy Your Workloads to the Cluster:**\n   - Use `kubectl` to configure and deploy your workloads as usual.\n3. **Monitor and Optimize Usage:**\n   - Use Cloud Monitoring and Cloud Cost Management tools to track costs.\n\n🔗 Reference: [GKE Autopilot Overview](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview)"
        },
        {
          "id": 87,
          "question": "Your application allows users to upload pictures. You need to convert each picture to your internal optimized binary format and store it. You want to use the most efficient, cost-effective solution. What should you do?",
          "type": "single",
          "options": [
            "Store uploaded files in Cloud Bigtable, monitor Bigtable entries, and then run a Cloud Function to convert the files and store them in Bigtable.",
            "Store uploaded files in Firestore, monitor Firestore entries, and then run a Cloud Function to convert the files and store them in Firestore.",
            "Store uploaded files in Filestore, monitor Filestore entries, and then run a Cloud Function to convert the files and store them in Filestore.",
            "Save uploaded files in a Cloud Storage bucket, and monitor the bucket for uploads. Run a Cloud Function to convert the files and to store them in a Cloud Storage bucket."
          ],
          "correctAnswer": [
            "Save uploaded files in a Cloud Storage bucket, and monitor the bucket for uploads. Run a Cloud Function to convert the files and to store them in a Cloud Storage bucket."
          ],
          "hint": "Consider which storage service is best suited for binary files and has native event triggers.",
          "explanation": "Cloud Storage with Cloud Functions is the most efficient and cost-effective solution because: 1) Cloud Storage is designed for object storage and binary files like images, 2) It has native event triggers that automatically invoke Cloud Functions on file uploads, 3) It provides high durability and automatic scalability. Other options like Bigtable, Firestore, and Filestore are either not designed for binary storage, have size limitations, or require more complex setups. Cloud Storage also offers better pricing for this use case and simpler implementation through direct event triggering."
        },
        {
          "id": 88,
          "question": "You are migrating your on-premises solution to Google Cloud. As a first step, the new cloud solution will need to ingest 100 TB of data. Your daily uploads will be within your current bandwidth limit of 100 Mbps. You want to follow Google-recommended practices for the most cost-effective way to implement the migration. What should you do?",
          "type": "single",
          "options": [
            "Set up Partner Interconnect for the duration of the first upload.",
            "Obtain a Transfer Appliance, copy the data to it, and ship it to Google.",
            "Set up Dedicated Interconnect for the duration of your first upload, and then drop back to regular bandwidth.",
            "Divide your data between 100 computers, and upload each data portion to a bucket. Then run a script to merge the uploads together."
          ],
          "correctAnswer": [
            "Obtain a Transfer Appliance, copy the data to it, and ship it to Google."
          ],
          "hint": "Consider offline transfer options for large data volumes when bandwidth is a limitation.",
          "explanation": "### Why Use Transfer Appliance?\n1. **Offline Transfer for Large Data Volumes:**\n   - For 100 TB, an offline transfer solution like Transfer Appliance is recommended for efficiency and cost-effectiveness.\n   - 100 Mbps bandwidth would take over *90 days* to upload 100 TB of data, which is neither practical nor efficient.\n\n2. **Cost-Effective Solution:**\n   - Transfer Appliance avoids the need to set up costly infrastructure like Dedicated or Partner Interconnect just for one-time use.\n   - Once data is loaded onto the appliance and shipped to Google's data center, it is securely uploaded to Cloud Storage.\n\n3. **Secure and Reliable:**\n   - Transfer Appliance offers secure data transfer with encryption and professional shipping methods.\n   - Ideal for large-scale migrations that exceed network capabilities.\n\n### Why Not the Other Options?\n- **A: Partner Interconnect:**\n   - Setting up Partner Interconnect requires partnerships with service providers and incurs costs for infrastructure.\n   - Not cost-effective for a one-off transfer.\n\n- **C: Dedicated Interconnect:**\n   - Dedicated Interconnect is a high-cost solution for long-term high-bandwidth needs, not for one-time transfers.\n   - The setup and teardown process is more complex than required for this use case.\n\n- **D: Dividing Data Across Multiple Computers:**\n   - This approach is impractical and would introduce significant operational overhead for managing uploads and merging data effectively.\n\n### Implementation Steps for Transfer Appliance:\n1. **Request a Transfer Appliance:**\n   - Go to the [Transfer Appliance Console](https://cloud.google.com/storage/transfer-appliance) and order an appliance.\n\n2. **Load Data onto the Appliance:**\n   - Use the Transfer Appliance tools to securely and quickly copy the 100 TB of data onto the device.\n\n3. **Ship the Appliance to Google:**\n   - Follow shipping instructions to send the appliance to Google.\n\n4. **Data Ingestion:**\n   - Google's team will ingest the data into your designated Cloud Storage bucket.\n\n5. **Verify the Data Integrity:**\n   - Confirm that the data is complete and accessible in the Cloud Storage bucket.\n\n🔗 Reference: [Transfer Appliance Documentation](https://cloud.google.com/storage/transfer-appliance/docs/)"
        },
        {
          "id": 89,
          "question": "You are setting up billing for your project. You want to prevent excessive consumption of resources due to an error or malicious attack and prevent billing spikes or surprises. What should you do?",
          "type": "single",
          "options": [
            "Set up budgets and alerts in your project.",
            "Set up quotas for the resources that your project will be using.",
            "Set up a spending limit on the credit card used in your billing account.",
            "Label all resources according to best practices, regularly export the billing reports, and analyze them with BigQuery."
          ],
          "correctAnswer": [
            "Set up quotas for the resources that your project will be using."
          ],
          "hint": "Focus on limiting the maximum resources your project can consume to prevent unexpected overages.",
          "explanation": "### Why Set Up Quotas?\n\n1. **Prevention of Excess Resource Usage:**\n   - Quotas act as a safeguard to limit the maximum resources (e.g., CPU, memory, API requests) your project can consume. This can prevent unexpected usage spikes due to errors or malicious attacks.\n\n2. **Proactive Cost Management:**\n   - By enforcing quotas, you ensure that your project does not exceed predetermined limits, thereby preventing billing surprises or spikes.\n\n3. **Google-Recommended Practice:**\n   - Setting up quotas is a built-in feature of Google Cloud, designed to help manage resource consumption and prevent over-allocation.\n\n### Why Not the Other Options?\n\n- **A: Set Up Budgets and Alerts:**\n   - Budgets and alerts notify you after a specific spend threshold is reached, but they do not stop resource consumption once the threshold is crossed.\n\n- **C: Spending Limit on Credit Card:**\n   - Google Cloud does not currently support setting spending limits directly on credit cards. Using this as your only method will not protect against resource overages effectively.\n\n- **D: Labels with BigQuery Analysis:**\n   - While this helps monitor and analyze costs, it does not prevent excess consumption in real-time.\n\n### Implementation Steps for Setting Up Quotas:\n1. **Navigate to the Quotas Page:**\n   - Go to the [Quotas settings in Google Cloud Console](https://console.cloud.google.com/iam-admin/quotas).\n\n2. **Specify Quotas for Key Resources:**\n   - Identify critical resources to limit (CPU, API usage, storage, etc.).\n   - Set maximum limits for each based on your expected usage.\n\n3. **Monitor and Adjust as Needed:**\n   - Monitor resource consumption regularly and adjust quotas based on new requirements or patterns.\n\n🔗 Reference: [Using Quotas in Google Cloud](https://cloud.google.com/iam/quotas)"
        },
        {
          "id": 90,
          "question": "Your project team needs to estimate the spending for your Google Cloud project for the next quarter. You know the project requirements. You want to produce your estimate as quickly as possible. What should you do?",
          "type": "single",
          "options": [
            "Build a simple machine learning model that will predict your next month’s spend.",
            "Estimate the number of hours of compute time required, and then multiply by the VM per-hour pricing.",
            "Use the Google Cloud Pricing Calculator to enter your predicted consumption for all groups of resources.",
            "Use the Google Cloud Pricing Calculator to enter your consumption for all groups of resources, and then adjust for volume discounts."
          ],
          "correctAnswer": [
            "Use the Google Cloud Pricing Calculator to enter your predicted consumption for all groups of resources."
          ],
          "hint": "Leverage tools made specifically for pricing estimation to streamline your process.",
          "explanation": "### Why Use Google Cloud Pricing Calculator?\n\n1. **Fast and Accurate Estimates:**\n   - The Google Cloud Pricing Calculator is specifically designed to estimate costs for various Google Cloud resources.\n   - By entering your predicted usage (e.g., VM hours, storage, network egress), you can generate detailed estimates quickly.\n\n2. **All Resource Groups Included:**\n   - The calculator accounts for all resources, ensuring a comprehensive projection of costs.\n   - Ideal for estimating complete multi-service deployments.\n\n3. **Google-Recommended Practice:**\n   - Using the Pricing Calculator is the most straightforward way to estimate costs accurately and align them with specific requirements.\n\n### Why Not the Other Options?\n- **A: Build a Simple Machine Learning Model:**\n   - While interesting, it is over-engineered for this use case and would take significant time compared to using a dedicated tool.\n\n- **B: Estimate Compute Costs Only:**\n   - This approach doesn't account for other resources like storage, networking, or additional services, leading to incomplete estimates.\n\n- **D: Adjust for Volume Discounts:**\n   - Volume discounts are usually applied automatically where applicable, so this step is unnecessary when using the Pricing Calculator.\n\n### Implementation Steps:\n1. **Access the Pricing Calculator:**\n   - Go to the [Google Cloud Pricing Calculator](https://cloud.google.com/products/calculator).\n\n2. **Input Consumption Details:**\n   - Enter your expected usage for each resource type (e.g., compute, storage, network).\n\n3. **Review Estimate:**\n   - Verify the total projected costs for the next quarter.\n   - Optionally share the estimate with the team.\n\n🔗 Reference: [Google Cloud Pricing Calculator](https://cloud.google.com/products/calculator)"
        },
        {
          "id": 91,
          "question": "You are creating a Google Kubernetes Engine (GKE) cluster with a cluster autoscaler feature enabled. You need to make sure that each node of the cluster will run a monitoring pod that sends container metrics to a third-party monitoring solution. What should you do?",
          "type": "single",
          "options": [
            "Deploy the monitoring pod in a StatefulSet object.",
            "Deploy the monitoring pod in a DaemonSet object.",
            "Reference the monitoring pod in a Deployment object.",
            "Reference the monitoring pod in a cluster initializer at the GKE cluster creation time."
          ],
          "correctAnswer": ["Deploy the monitoring pod in a DaemonSet object."],
          "hint": "Consider which Kubernetes object ensures a pod runs on every node in the cluster.",
          "explanation": "### Why Use a DaemonSet?\n\n1. **Guaranteed Pod per Node:**\n   - DaemonSet ensures that a copy of the pod runs on every node in the cluster\n   - When new nodes are added by the autoscaler, DaemonSet automatically deploys monitoring pods\n   - When nodes are removed, the DaemonSet pods are automatically cleaned up\n\n2. **Perfect for Monitoring Use Case:**\n   ```yaml\n   apiVersion: apps/v1\n   kind: DaemonSet\n   metadata:\n     name: monitoring-agent\n     namespace: monitoring\n   spec:\n     selector:\n       matchLabels:\n         app: monitoring-agent\n     template:\n       metadata:\n         labels:\n           app: monitoring-agent\n       spec:\n         containers:\n         - name: monitoring-agent\n           image: monitoring-image:latest\n   ```\n\n### Why Not Other Options?\n\n- **StatefulSet:**\n   - Designed for stateful applications with unique network identifiers\n   - Doesn't automatically ensure coverage across all nodes\n   - More suitable for databases or other stateful workloads\n\n- **Deployment:**\n   - Manages replicated pods\n   - Doesn't guarantee pod distribution across all nodes\n   - Better for stateless applications\n\n- **Cluster Initializer:**\n   - Deprecated feature\n   - Doesn't handle dynamic node additions/removals\n   - Not suitable for ongoing monitoring needs\n\n### Implementation Best Practices:\n\n1. **Resource Limits:**\n   ```yaml\n   resources:\n     limits:\n       cpu: 200m\n       memory: 256Mi\n     requests:\n       cpu: 100m\n       memory: 128Mi\n   ```\n\n2. **Node Affinity:**\n   ```yaml\n   affinity:\n     nodeAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: kubernetes.io/os\n             operator: In\n             values:\n             - linux\n   ```\n\n3. **Tolerations for System Nodes:**\n   ```yaml\n   tolerations:\n   - key: node-role.kubernetes.io/master\n     effect: NoSchedule\n   ```\n\n🔗 Reference: [DaemonSet Documentation](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)"
        },
        {
          "id": 92,
          "question": "You deployed an App Engine application using gcloud app deploy, but it did not deploy to the intended project. You want to find out why this happened and where the application deployed. What should you do?",
          "type": "single",
          "options": [
            "Check the app.yaml file for your application and check project settings.",
            "Check the web-application.xml file for your application and check project settings.",
            "Go to Deployment Manager and review settings for deployment of applications.",
            "Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment."
          ],
          "correctAnswer": [
            "Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment."
          ],
          "hint": "Consider which command shows the active configuration settings used by gcloud commands.",
          "explanation": "Using gcloud config list in Cloud Shell is the most direct way to identify the active project configuration that was used during deployment. This command shows the current account, project ID, compute region, and other relevant settings that affect where your application gets deployed. The app.yaml file contains application configurations but not the deployment target. Web-application.xml is irrelevant for deployment configuration, and Deployment Manager is not involved in App Engine deployments. After identifying the issue with gcloud config list, you can correct it using gcloud config set project if needed."
        },
        {
          "id": 92,
          "question": "You deployed an App Engine application using gcloud app deploy, but it did not deploy to the intended project. You want to find out why this happened and where the application deployed. What should you do?",
          "type": "single",
          "options": [
            "Check the app.yaml file for your application and check project settings.",
            "Check the web-application.xml file for your application and check project settings.",
            "Go to Deployment Manager and review settings for deployment of applications.",
            "Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment."
          ],
          "correctAnswer": [
            "Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment."
          ],
          "hint": "Consider which command shows the active configuration settings used by gcloud commands.",
          "explanation": "### Why Use gcloud config list?\n\n#### Immediate Visibility of Settings\n```bash\ngcloud config list\n```\nThis command shows:\n- Active project ID\n- Authenticated account\n- Default region/zone\n- Other relevant configurations\n\n#### Quick Problem Resolution\n1. Identifies wrong project configuration\n2. Verifies authentication\n3. Shows unexpected default settings\n\n#### Resolution Steps\n1. Check current configuration:\n```bash\ngcloud config list\n```\n\n2. Switch to correct project if needed:\n```bash\ngcloud config set project CORRECT-PROJECT-ID\n```\n\n3. Verify App Engine status:\n```bash\ngcloud app describe\n```\n\n### Summary of Wrong Options:\n\n**Option 1: Check app.yaml**\n- ❌ Only contains application configuration\n- ❌ Doesn't specify deployment project\n- ❌ Won't show actual deployment location\n\n**Option 2: Check web-application.xml**\n- ❌ Java web configuration file only\n- ❌ Not related to deployment targets\n- ❌ Irrelevant for project selection\n\n**Option 3: Go to Deployment Manager**\n- ❌ Not used for App Engine deployments\n- ❌ Infrastructure as code tool only\n- ❌ Wrong service for this purpose\n\n🔗 [Reference: gcloud Configuration Guide](https://cloud.google.com/sdk/gcloud/reference/config)"
        },
        {
          "id": 93,
          "question": "You've deployed a microservice called myapp1 to a Google Kubernetes Engine cluster using the YAML file specified below. You need to refactor this configuration so that the database password is not stored in plain text. You want to follow Google-recommended practices. ![YAML Configuration](https://github.com/Ditectrev/Google-Cloud-Platform-GCP-Associate-Cloud-Engineer-Practice-Tests-Exams-Questions-Answers/raw/main/images/yaml.jpg)",
          "type": "single",
          "options": [
            "Store the database password inside the Docker image of the container, not in the YAML file.",
            "Store the database password inside a Secret object. Modify the YAML file to populate the DB_PASSWORD environment variable from the Secret.",
            "Store the database password inside a ConfigMap object. Modify the YAML file to populate the DB_PASSWORD environment variable from the ConfigMap.",
            "Store the database password in a file inside a Kubernetes persistent volume, and use a persistent volume claim to mount the volume to the container."
          ],
          "correctAnswer": [
            "Store the database password inside a Secret object. Modify the YAML file to populate the DB_PASSWORD environment variable from the Secret."
          ],
          "hint": "Look for options that ensure sensitive information is handled securely while adhering to best practices.",
          "explanation": "### Why Use Kubernetes Secrets?\n\n#### Enhanced Security\nStoring sensitive information such as database passwords inside a Secret object provides several benefits:\n- Encoded storage of sensitive data to mitigate risk.\n- Access control and auditing capabilities for Secrets through Kubernetes RBAC (Role-Based Access Control).\n\n#### Example Implementation Steps:\n1. **Create a Secret:**\n   You can create a Secret using the following command:\n   ```bash\n   kubectl create secret generic myapp1-db-secret --from-literal=DB_PASSWORD='your_password'\n   ```\n\n2. **Modify Your YAML Configuration:**\n   In your deployment YAML, reference the Secret as follows:\n   ```yaml\n   env:\n     - name: DB_PASSWORD\n       valueFrom:\n         secretKeyRef:\n           name: myapp1-db-secret\n           key: DB_PASSWORD\n   ```\n\n### Summary of Wrong Options:\n\n**Option 1: Store inside Docker image**\n- ❌ This practice can lead to leaking secrets if the image is shared.\n- ❌ Breaks the immutable infrastructure principle.\n\n**Option 3: Store inside ConfigMap**\n- ❌ ConfigMaps are not designed for sensitive information and are not encoded.\n\n**Option 4: Use Persistent Volume**\n- ❌ This is not a recommended practice as it still creates a risk of exposing secrets.\n\nBy following the recommended practice of using Secrets, you ensure the security and reliability of sensitive credential management in your GKE deployment."
        },
        {
          "id": 94,
          "question": "You are using Container Registry to centrally store your company's container images in a separate project. In another project, you want to create a Google Kubernetes Engine (GKE) cluster. You want to ensure that Kubernetes can download images from Container Registry. What should you do?",
          "type": "single",
          "options": [
            "In the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes.",
            "When you create the GKE cluster, choose the Allow full access to all Cloud APIs option under 'Access scopes'.",
            "Create a service account, and give it access to Cloud Storage. Create a P12 key for this service account and use it as an imagePullSecrets in Kubernetes.",
            "Configure the ACLs on each image in Cloud Storage to give read-only access to the default Compute Engine service account."
          ],
          "correctAnswer": [
            "In the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes."
          ],
          "hint": "Think about how to grant permissions for accessing container images stored in a separate project.",
          "explanation": "### Why Grant Storage Object Viewer Role?\n\n#### Access Control\nGranting the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes will allow the nodes to pull images from Container Registry securely, without exposing unnecessary permissions.\n\n#### Steps to Implement:\n1. **Identify the Service Account:**\n   Find out which service account your GKE cluster nodes are using. Typically, it could be something like `PROJECT_NUMBER-compute@developer.gserviceaccount.com`.\n\n2. **Grant Permissions:**\n   In the project where the Container Registry is located, execute the command:\n   ```bash\n   gcloud projects add-iam-policy-binding PROJECT_ID --member='serviceAccount:SERVICE_ACCOUNT_EMAIL' --role='roles/storage.objectViewer'\n   ```\n\n### Summary of Wrong Options:\n\n**Option 2: Allow full access to all Cloud APIs**\n- ❌ This is too broad and increases security risks; least privilege principle should be followed.\n\n**Option 3: Create a service account and use imagePullSecrets**\n- ❌ This is unnecessary complexity when the IAM role can suffice, as Kubernetes can seamlessly manage service accounts for image pulling.\n\n**Option 4: Configure ACLs**\n- ❌ Managing ACLs on individual images can be cumbersome and is not the preferred way to handle access control; IAM roles are more manageable.\n\nBy correctly granting the Storage Object Viewer IAM role, you enable your Kubernetes nodes to access the container images they need without introducing excessive privileges."
        },
        {
          "id": 95,
          "question": "You deployed a new application inside your Google Kubernetes Engine cluster using the YAML file specified below. You check the status of the deployed pods and notice that one of them is still in PENDING status. You want to find out why the pod is stuck in pending status. What should you do? ![YAML Configuration](https://github.com/Ditectrev/Google-Cloud-Platform-GCP-Associate-Cloud-Engineer-Practice-Tests-Exams-Questions-Answers/raw/main/images/yaml2.png) ![Pod Status](https://github.com/Ditectrev/Google-Cloud-Platform-GCP-Associate-Cloud-Engineer-Practice-Tests-Exams-Questions-Answers/raw/main/images/pods.png)",
          "type": "single",
          "options": [
            "Review details of the myapp-service Service object and check for error messages.",
            "Review details of the myapp-deployment Deployment object and check for error messages.",
            "Review details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages.",
            "View logs of the container in myapp-deployment-58ddbbb995-lp86m pod and check for warning messages."
          ],
          "correctAnswer": [
            "Review details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages."
          ],
          "hint": "Investigate the specific pod's status to find out why it is not scheduling correctly.",
          "explanation": "### Why Check Pod Details?\n\n#### Understanding Pod Status\nWhen a pod is in a PENDING status, it usually indicates that the Kubernetes scheduler cannot find a suitable node for it to run on. This might be due to a variety of reasons, such as:\n- Insufficient resources (CPU, memory).\n- Taints and tolerations preventing the pod from being scheduled.\n- Node selector constraints or affinities that cannot be met.\n\n#### Steps to Diagnose:\n1. **Get Pod Details:**\n   You can obtain detailed information about the pod and its status with the command:\n   ```bash\n   kubectl describe pod myapp-deployment-58ddbbb995-lp86m\n   ```\n   This command will show events related to the pod that may indicate why it is stuck in PENDING, including any specific warning messages or reasons.\n\n### Summary of Wrong Options:\n\n**Option 1: Review myapp-service**\n- ❌ The Service object is used for networking and might not provide information on pod scheduling issues.\n\n**Option 2: Review myapp-deployment**\n- ❌ While deployments manage pods, checking the deployment won't provide specific details about the pod's issues in this case.\n\n**Option 4: View logs of the container**\n- ❌ The pod is not yet running; thus, there are no logs available to review until it starts.\n\nBy focusing on the specific pod's details and events, you can accurately diagnose the underlying issue preventing it from being scheduled."
        },
        {
          "id": 96,
          "question": "You are setting up a Windows VM on Compute Engine and want to make sure you can log in to the VM via RDP. What should you do?",
          "type": "single",
          "options": [
            "After the VM has been created, use your Google Account credentials to log in into the VM.",
            "After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.",
            "When creating the VM, add metadata to the instance using 'windows-password' as the key and a password as the value.",
            "After the VM has been created, download the JSON Private Key for the default Compute Engine service account. Use the credentials in the JSON file to log in to the VM."
          ],
          "correctAnswer": [
            "After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM."
          ],
          "hint": "Look for a method that ensures you securely obtain the login credentials for the Windows VM.",
          "explanation": "### Why Use gcloud compute reset-windows-password?\n\n#### Secure Credential Management\nThe `gcloud compute reset-windows-password` command is specifically designed for Google Cloud Windows instances and ensures the VM's login credentials are securely set up. This command generates a random password and associates it with the specified user account, ensuring that you have secure access to the VM once it is created.\n\n#### Steps to Implement:\n1. **Create the Windows VM:**\n   Go through the usual process of creating your Windows VM on Compute Engine.\n\n2. **Retrieve Login Credentials:**\n   Use the command:\n   ```bash\n   gcloud compute reset-windows-password [INSTANCE_NAME] --zone [ZONE]\n   ```\n   Replace `[INSTANCE_NAME]` with your VM's name and `[ZONE]` with the appropriate zone. This will prompt you to specify the username and provide you with the password to log in via RDP.\n\n### Summary of Wrong Options:\n\n**Option 1: Use Google Account credentials**\n- ❌ Google Account credentials are not a method for accessing Windows VMs through RDP; you need specific Windows credentials.\n\n**Option 3: Add metadata with 'windows-password'**\n- ❌ While you can set passwords in metadata, managing passwords through this method is less secure and does not follow the best practices for handling sensitive information.\n\n**Option 4: Use JSON Private Key**\n- ❌ The JSON Private Key is used for service account authentication, not for logging into Windows VMs. It does not provide user credentials needed for RDP.\n\nBy utilizing `gcloud compute reset-windows-password`, you are following the best practices for securely managing your Windows VM login credentials."
        },
        {
          "id": 97,
          "question": "You want to configure an SSH connection to a single Compute Engine instance for users in the dev1 group. This instance is the only resource in this particular Google Cloud Platform project that the dev1 users should be able to connect to. What should you do?",
          "type": "single",
          "options": [
            "Set metadata to enable-oslogin=true for the instance. Grant the dev1 group the compute.osLogin role. Direct them to use the Cloud Shell to ssh to that instance.",
            "Set metadata to enable-oslogin=true for the instance. Set the service account to no service account for that instance. Direct them to use the Cloud Shell to ssh to that instance.",
            "Enable block project wide keys for the instance. Generate an SSH key for each user in the dev1 group. Distribute the keys to dev1 users and direct them to use their third-party tools to connect.",
            "Enable block project wide keys for the instance. Generate an SSH key and associate the key with that instance. Distribute the key to dev1 users and direct them to use their third-party tools to connect."
          ],
          "correctAnswer": [
            "Set metadata to enable-oslogin=true for the instance. Grant the dev1 group the compute.osLogin role. Direct them to use the Cloud Shell to ssh to that instance."
          ],
          "hint": "Consider using Google Cloud's OS Login feature for managing SSH access securely and efficiently.",
          "explanation": "### Why Use OS Login?\n\n#### Enhanced Security and Management\nUsing OS Login allows you to manage SSH access through IAM roles, which simplifies permission management, enhances security, and removes the need for handling SSH keys manually.\n\n#### Steps to Configure:\n1. **Enable OS Login:**\n   Set the metadata for the instance:\n   ```bash\n   gcloud compute instances add-metadata [INSTANCE_NAME] --metadata enable-oslogin=true\n   ```\n2. **Grant IAM Role:**\n   Assign the `compute.osLogin` role to the `dev1` group:\n   ```bash\n   gcloud iam service-accounts add-iam-policy-binding [YOUR_SA] --member='group:dev1@example.com' --role='roles/compute.osLogin'\n   ```\n3. **SSH via Cloud Shell:**\n   Instruct the users to access the VM using Cloud Shell (or their preferred SSH client) and utilize their Google credentials for authentication.\n\n### Summary of Wrong Options:\n\n**Option 2: No service account**\n- ❌ This does not pertain to SSH access management and does not improve security.\n\n**Option 3: Block project-wide keys and generate keys**\n- ❌ Manually handling SSH keys increases complexity and management overhead, which OS Login avoids.\n\n**Option 4: Generate and distribute a key**\n- ❌ Similar to Option 3, this method adds unnecessary complexity; leveraging OS Login is far more secure and manageable.\n\nBy leveraging OS Login and IAM roles, you create a cleaner and more secure approach to manage access to your Compute Engine instances."
        },
        {
          "id": 98,
          "question": "You need to produce a list of the enabled Google Cloud Platform APIs for a GCP project using the gcloud command line in the Cloud Shell. The project name is my-project. What should you do?",
          "type": "single",
          "options": [
            "Run gcloud projects list to get the project ID, and then run gcloud services list --project .",
            "Run gcloud init to set the current project to my-project, and then run gcloud services list --available.",
            "Run gcloud info to view the account value, and then run gcloud services list --account .",
            "Run gcloud projects describe to verify the project value, and then run gcloud services list --available."
          ],
          "correctAnswer": [
            "Run gcloud projects list to get the project ID, and then run gcloud services list --project my-project."
          ],
          "hint": "You need to identify the correct command to list enabled services in a specific project.",
          "explanation": "### Correct Command Usage\n\nTo list the enabled APIs in a specific GCP project (in this case, 'my-project'), you will need to use the following commands:\n1. **Get the Project ID:**\n   The command `gcloud projects list` will provide a list of all your projects along with their IDs. \n   ```bash\n   gcloud projects list\n   ```\n2. **List Enabled APIs:**\n   Once you have the correct project ID from the previous command, use it in conjunction with the `gcloud services list` command to view the enabled APIs:\n   ```bash\n   gcloud services list --enabled --project=my-project\n   ```\n\n### Summary of Wrong Options:\n\n**Option 2: gcloud init**\n- ❌ Although `gcloud init` can set your active project, it does not directly help to list enabled services.\n\n**Option 3: gcloud info**\n- ❌ This command retrieves account information but does not help in listing the APIs for a specific project.\n\n**Option 4: gcloud projects describe**\n- ❌ While this command can give you project details, it does not show enabled APIs. You need to list them using `gcloud services list` directly.\n\nBy following the first option, you access the necessary information to display all enabled APIs correctly."
        },
        {
          "id": 99,
          "question": "You are building a new version of an application hosted in an App Engine environment. You want to test the new version with 1% of users before you completely switch your application over to the new version. What should you do?",
          "type": "single",
          "options": [
            "Deploy a new version of your application in Google Kubernetes Engine instead of App Engine and then use GCP Console to split traffic.",
            "Deploy a new version of your application in a Compute Engine instance instead of App Engine and then use GCP Console to split traffic.",
            "Deploy a new version as a separate app in App Engine. Then configure App Engine using GCP Console to split traffic between the two apps.",
            "Deploy a new version of your application in App Engine. Then go to App Engine settings in GCP Console and split traffic between the current version and newly deployed versions accordingly."
          ],
          "correctAnswer": [
            "Deploy a new version of your application in App Engine. Then go to App Engine settings in GCP Console and split traffic between the current version and newly deployed versions accordingly."
          ],
          "hint": "Consider how traffic splitting is directly supported within App Engine for version management.",
          "explanation": "### Why Use Traffic Splitting in App Engine?\n\n#### Native Support for Versions\nGoogle App Engine natively supports version control and traffic splitting which allows you to gradually roll out a new version of your application. This feature helps you manage risks by testing new changes with a small percentage of users before a full deployment.\n\n#### Steps to Implement:\n1. **Deploy New Version:**\n   Deploy the new version of your application to App Engine by using the command:\n   ```bash\n   gcloud app deploy\n   ```\n2. **Split Traffic:**\n   After deployment, navigate to the App Engine section in the GCP Console: \n   - Go to the **Traffic splitting** section.\n   - You can then specify to direct 1% of the traffic to the new version while the rest continues to use the current version.\n\n### Summary of Wrong Options:\n\n**Option 1: Use Google Kubernetes Engine**\n- ❌ While Kubernetes is a robust deployment tool, it is not necessary for traffic splitting in App Engine and does not leverage the built-in features of App Engine.\n\n**Option 2: Use Compute Engine**\n- ❌ Similar to Option 1, this is not the preferred method, as it lacks direct integration for feature testing like App Engine does.\n\n**Option 3: Separate App in App Engine**\n- ❌ Creating a separate app complicates management and deployment; using versions within the same application is more effective.\n\nBy deploying the new version in App Engine and using its traffic splitting capabilities, you maintain a streamlined and effective testing process."
        },
        {
          "id": 100,
          "question": "You need to provide a cost estimate for a Kubernetes cluster using the GCP pricing calculator for Kubernetes. Your workload requires high IOPs, and you will also be using disk snapshots. You start by entering the number of nodes, average hours, and average days. What should you do next?",
          "type": "single",
          "options": [
            "Fill in local SSD. Fill in persistent disk storage and snapshot storage.",
            "Fill in local SSD. Add estimated cost for cluster management.",
            "Select Add GPUs. Fill in persistent disk storage and snapshot storage.",
            "Select Add GPUs. Add estimated cost for cluster management."
          ],
          "correctAnswer": [
            "Fill in local SSD. Fill in persistent disk storage and snapshot storage."
          ],
          "hint": "Consider the storage requirements for high IOPs and the need for snapshots.",
          "explanation": "### Why Include Local SSD and Persistent Disk?\n\n#### High IOPs Requirement\nFor workloads requiring high IOPS, local SSDs are recommended because they offer significantly higher performance compared to persistent disks. Local SSDs deliver extremely low latency and high throughput for applications that need fast access to disk storage.\n\n1. **Fill in Local SSD Details:**\n   - Specify the amount of local SSD needed based on your expected workload requirements. \n\n2. **Fill in Persistent Disk Storage:**\n   - Enter the size of the persistent disk you intend to use for more storage; even if you use local SSDs, persistent disks are useful for data that needs to be preserved through instance restarts.\n\n3. **Fill in Snapshot Storage:**\n   - Since you mentioned using disk snapshots, it’s essential to estimate the amount you expect to use for snapshot storage as well. This will help you gauge the costs associated with backing up your persistent disks.\n\n### Summary of Wrong Options:\n\n**Option 2: Add estimated cost for cluster management**\n- ❌ While cluster management costs may be important, you first need to address the storage specificities for your high IOPS needs before estimating management costs.\n\n**Option 3: Select Add GPUs**\n- ❌ GPUs are not necessary for all workloads, and unless your application explicitly requires them for calculation or processing tasks, there’s no need to include them just for high IOPS.\n\n**Option 4: Add estimated cost for cluster management**\n- ❌ Similar to Option 2; the focus should remain on the storage and performance aspects of your workload before estimating additional management costs.\n\nBy prioritizing local SSDs, persistent disks, and snapshot storage, you are effectively addressing the main requirements for high IOPS workloads and budgeting accurately for your Kubernetes cluster."
        },
        {
          "id": 101,
          "question": "You are using Google Kubernetes Engine with autoscaling enabled to host a new application. You want to expose this new application to the public, using HTTPS on a public IP address. What should you do?",
          "type": "single",
          "options": [
            "Create a Kubernetes Service of type NodePort for your application, and a Kubernetes Ingress to expose this Service via a Cloud Load Balancer.",
            "Create a Kubernetes Service of type ClusterIP for your application. Configure the public DNS name of your application using the IP of this Service.",
            "Create a Kubernetes Service of type NodePort to expose the application on port 443 of each node of the Kubernetes cluster. Configure the public DNS name of your application with the IP of every node of the cluster to achieve load-balancing.",
            "Create a HAProxy pod in the cluster to load-balance the traffic to all the pods of the application. Forward the public traffic to HAProxy with an iptable rule. Configure the DNS name of your application using the public IP of the node HAProxy is running on."
          ],
          "correctAnswer": [
            "Create a Kubernetes Service of type NodePort for your application, and a Kubernetes Ingress to expose this Service via a Cloud Load Balancer."
          ],
          "hint": "Think about how to expose a service securely and scale it with HTTPS.",
          "explanation": "### Why Use Ingress with HTTPS?\n\n#### Ingress Controller\nThe Ingress controller manages external access to the services in a Kubernetes cluster, typically HTTP and HTTPS. \n- Provides SSL termination \n- Balances traffic across services\n\n#### Load Balancer\nA Cloud Load Balancer can be associated with an Ingress, providing a single public IP address for accessing your application securely. \n\n### Summary of Wrong Options:\n\n**Option 2: Use ClusterIP**\n- ❌ ClusterIP is not accessible from outside the cluster.\n\n**Option 3: Expose via NodePort**\n- ❌ This requires configuring public DNS for each node, which is not efficient.\n\n**Option 4: HAProxy Pod**\n- ❌ This adds unnecessary complexity for load balancing in a managed service.\n\n🔗 [Reference: Kubernetes Ingress Documentation](https://kubernetes.io/docs/concepts/services-networking/ingress/)"
        },
        {
          "id": 102,
          "question": "You need to enable traffic between multiple groups of Compute Engine instances that are currently running two different GCP projects. Each group of Compute Engine instances is running in its own VPC. What should you do?",
          "type": "single",
          "options": [
            "Verify that both projects are in a GCP Organization. Create a new VPC and add all instances.",
            "Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC.",
            "Verify that you are the Project Administrator of both projects. Create two new VPCs and add all instances.",
            "Verify that you are the Project Administrator of both projects. Create a new VPC and add all instances."
          ],
          "correctAnswer": [
            "Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC."
          ],
          "hint": "Consider how to leverage VPC sharing features in GCP.",
          "explanation": "### Why Use VPC Sharing?\n\n#### Organizational Structure\nVPC sharing allows projects within the same organization to share network resources like VPCs across different GCP projects.\n\n#### Benefits\n- Simplifies network interconnectivity\n- Reduces the need to create and manage multiple subnets.\n\n### Summary of Wrong Options:\n\n**Option 1: Create a new VPC**\n- ❌ This doesn't enable communication between existing instances.\n\n**Option 3 and 4: Creating new VPCs**\n- ❌ Creating new VPCs won't connect existing instances across projects effectively.\n\n🔗 [Reference: Shared VPC Overview](https://cloud.google.com/vpc/docs/shared-vpc)"
        },
        {
          "id": 103,
          "question": "You want to add a new auditor to a Google Cloud Platform project. The auditor should be allowed to read, but not modify, all project items. How should you configure the auditor's permissions?",
          "type": "single",
          "options": [
            "Create a custom role with view-only project permissions. Add the user's account to the custom role.",
            "Create a custom role with view-only service permissions. Add the user's account to the custom role.",
            "Select the built-in IAM project Viewer role. Add the user's account to this role.",
            "Select the built-in IAM service Viewer role. Add the user's account to this role."
          ],
          "correctAnswer": [
            "Select the built-in IAM project Viewer role. Add the user's account to this role."
          ],
          "hint": "Consider the predefined IAM roles for viewing project resources.",
          "explanation": "### Why Use Project Viewer Role?\n\n#### Predefined Role\nThe IAM project Viewer role is a predefined role that grants read-only access to all project resources. \n- Ensures auditors can review configurations and resources without making changes.\n\n### Summary of Wrong Options:\n\n**Option 1: Custom Role with project permissions**\n- ❌ More complex if a predefined role accomplishes the requirement.\n\n**Option 2: Custom Role with service permissions**\n- ❌ This does not cover all project resources.\n\n**Option 4: Service Viewer Role**\n- ❌ Limited to service-related permissions only.\n\n🔗 [Reference: IAM Roles Documentation](https://cloud.google.com/iam/docs/understanding-roles)"
        },
        {
          "id": 104,
          "question": "You are operating a Google Kubernetes Engine (GKE) cluster for your company where different teams can run non-production workloads. Your Machine Learning (ML) team needs access to Nvidia Tesla P100 GPUs to train their models. You want to minimize effort and cost. What should you do?",
          "type": "single",
          "options": [
            "Ask your ML team to add the accelerator: gpu annotation to their pod specification.",
            "Recreate all the nodes of the GKE cluster to enable GPUs on all of them.",
            "Create your own Kubernetes cluster on top of Compute Engine with nodes that have GPUs. Dedicate this cluster to your ML team.",
            "Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke-accelerator: nvidia-tesla-p100 nodeSelector to their pod specification."
          ],
          "correctAnswer": [
            "Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke-accelerator: nvidia-tesla-p100 nodeSelector to their pod specification."
          ],
          "hint": "Think about how to efficiently add GPU resources to existing clusters.",
          "explanation": "### Why Add a New Node Pool?\n\n#### Specific Resource Needs\nAdding a dedicated GPU-enabled node pool allows the GKE cluster to manage resources better for workloads requiring GPUs.\n\n#### Cost Efficiency\n- Only the required nodes are created, reducing overall costs.\n- Vertical scaling can be applied for individual workloads.\n\n### Summary of Wrong Options:\n\n**Option 1: Pod Annotation**\n- ❌ Annotation alone does not provision GPU nodes.\n\n**Option 2: Recreating all nodes**\n- ❌ This is resource intensive and not cost-effective.\n\n**Option 3: Separate Kubernetes cluster**\n- ❌ More complex and inefficient use of resources.\n\n🔗 [Reference: GKE GPU Documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus)"
        },
        {
          "id": 105,
          "question": "Your VMs are running in a subnet that has a subnet mask of 255.255.255.240. The current subnet has no more free IP addresses and you require an additional 10 IP addresses for new VMs. The existing and new VMs should all be able to reach each other without additional routes. What should you do?",
          "type": "single",
          "options": [
            "Use gcloud to expand the IP range of the current subnet.",
            "Delete the subnet, and recreate it using a wider range of IP addresses.",
            "Create a new project. Use Shared VPC to share the current network with the new project.",
            "Create a new subnet with the same starting IP but a wider range to overwrite the current subnet."
          ],
          "correctAnswer": [
            "Use gcloud to expand the IP range of the current subnet."
          ],
          "hint": "Consider the options for modifying existing subnets in Google Cloud.",
          "explanation": "### Why Expand the Subnet?\n\n#### Efficient Management\nUsing `gcloud`, you can expand the subnet without disrupting existing resources or changing the configuration significantly.\n\n#### Avoids Downtime\n- Prevents the need to recreate the subnet, which would involve downtime.\n- Keeps all configurations intact.\n\n### Summary of Wrong Options:\n\n**Option 2: Delete and recreate**\n- ❌ This causes downtime and potential configuration loss.\n\n**Option 3: Shared VPC**\n- ❌ This does not solve the immediate issue of IP shortages in the existing subnet.\n\n**Option 4: New subnet**\n- ❌ This complicates routing as it creates multiple subnets.\n\n🔗 [Reference: VPC Docs](https://cloud.google.com/vpc/docs/overview)"
        },
        {
          "id": 106,
          "question": "Your organization uses G Suite for communication and collaboration. All users in your organization have a G Suite account. You want to grant some G Suite users access to your Cloud Platform project. What should you do?",
          "type": "single",
          "options": [
            "Enable Cloud Identity in the GCP Console for your domain.",
            "Grant them the required IAM roles using their G Suite email address.",
            "Create a CSV sheet with all users' email addresses. Use the gcloud command line tool to convert them into Google Cloud Platform accounts.",
            "In the G Suite console, add the users to a special group called cloud-console-users@yourdomain.com. Rely on the default behavior of the Cloud Platform to grant users access if they are members of this group."
          ],
          "correctAnswer": [
            "Grant them the required IAM roles using their G Suite email address."
          ],
          "hint": "Think about how to assign IAM roles effectively using G Suite accounts.",
          "explanation": "### Why Grant IAM Roles?\n\n#### Direct Access\nGranting IAM roles directly to G Suite accounts allows users to access GCP resources with predefined permissions.\n- Tailored access control\n- Simplified management for existing users in G Suite.\n\n### Summary of Wrong Options:\n\n**Option 1: Enable Cloud Identity**\n- ❌ This requires additional setup and may not be necessary if users are already in G Suite.\n\n**Option 3: CSV Conversion**\n- ❌ Unnecessary complexity for existing accounts.\n\n**Option 4: Special Group in G Suite**\n- ❌ This method doesn't provide fine-grained IAM roles.\n\n🔗 [Reference: IAM Documentation](https://cloud.google.com/iam/docs/overview)"
        },
        {
          "id": 107,
          "question": "You have a Google Cloud Platform account with access to both production and development projects. You need to create an automated process to list all compute instances in development and production projects on a daily basis. What should you do?",
          "type": "single",
          "options": [
            "Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.",
            "Create two configurations using gsutil config. Write a script that sets configurations as active, individually. For each configuration, use gsutil compute instances list to get a list of compute resources.",
            "Go to Cloud Shell and export this information to Cloud Storage on a daily basis.",
            "Go to GCP Console and export this information to Cloud SQL on a daily basis."
          ],
          "correctAnswer": [
            "Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources."
          ],
          "hint": "Consider the best command-line tool for managing GCP resources and scripting.",
          "explanation": "### Why Use gcloud Configurations?\n\n#### Automation and Flexibility\nUsing `gcloud config` allows you to switch contexts between different projects easily and automate tasks using scripts.\n- Supports multiple configurations for different environments.\n- Easily integrates with CI/CD pipelines.\n\n### Summary of Wrong Options:\n\n**Option 2: Using gsutil config**\n- ❌ gsutil is meant for Cloud Storage, not for Compute Engine resources.\n\n**Option 3: Cloud Shell Export**\n- ❌ This does not automate the daily reporting process effectively.\n\n**Option 4: GCP Console Export**\n- ❌ This is a manual process and does not provide automation.\n\n🔗 [Reference: gcloud CLI Documentation](https://cloud.google.com/sdk/gcloud/reference)"
        },
        {
          "id": 108,
          "question": "You have a large 5-TB AVRO file stored in a Cloud Storage bucket. Your analysts are proficient only in SQL and need access to the data stored in this file. You want to find a cost-effective way to complete their request as soon as possible. What should you do?",
          "type": "single",
          "options": [
            "Load data in Cloud Datastore and run a SQL query against it.",
            "Create a BigQuery table and load data in BigQuery. Run a SQL query on this table and drop this table after you complete your request.",
            "Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.",
            "Create a Hadoop cluster and copy the AVRO file to NDFS by compressing it. Load the file in a hive table and provide access to your analysts so that they can run SQL queries."
          ],
          "correctAnswer": [
            "Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request."
          ],
          "hint": "Consider how to leverage existing data without duplicating it.",
          "explanation": "### Why Use External Tables in BigQuery?\n\n#### Cost-Effective Access\nUsing external tables in BigQuery allows you to query data directly from Cloud Storage without copying it, saving time and costs related to data loading.\n\n#### SQL Proficiency\nAnalysts can work with the data using SQL, minimizing the need for extensive training or adjustments to their workflow.\n\n### Summary of Wrong Options:\n\n**Option 1: Load data in Cloud Datastore**\n- ❌ Datastore is not optimized for SQL querying and would increase complexity.\n\n**Option 2: Create a BigQuery table**\n- ❌ This incurs additional costs for storage and loading the data unnecessarily.\n\n**Option 4: Create a Hadoop cluster**\n- ❌ This adds significant overhead and management complexity compared to using BigQuery.\n\n🔗 [Reference: Querying External Data](https://cloud.google.com/bigquery/docs/querying-external-data)"
        },
        {
          "id": 109,
          "question": "You need to verify that a Google Cloud Platform service account was created at a particular time. What should you do?",
          "type": "single",
          "options": [
            "Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account.",
            "Filter the Activity log to view the Configuration category. Filter the Resource type to Google Project.",
            "Filter the Activity log to view the Data Access category. Filter the Resource type to Service Account.",
            "Filter the Activity log to view the Data Access category. Filter the Resource type to Google Project."
          ],
          "correctAnswer": [
            "Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account."
          ],
          "hint": "Think about what logs capture resource creation events in GCP.",
          "explanation": "### Why Filter the Activity Log by Configuration?\n\n#### Configuration Changes\nThe Activity log captures events related to changes in configuration, including the creation of service accounts.\n- **Service Accounts** specifically fall under this log category.\n\n### Summary of Wrong Options:\n\n**Option 2: Filter by Google Project**\n- ❌ This does not specify the specific resource you are looking for.\n\n**Option 3: Data Access Category**\n- ❌ This logs data interactions but not resource creation.\n\n**Option 4: Data Access for Google Project**\n- ❌ This is irrelevant for examining creation timestamps.\n\n🔗 [Reference: Activity Logs](https://cloud.google.com/logging/docs/view/overview)"
        },
        {
          "id": 110,
          "question": "You deployed an LDAP server on Compute Engine that is reachable via TLS through port 636 using UDP. You want to make sure it is reachable by clients over that port. What should you do?",
          "type": "single",
          "options": [
            "Add the network tag allow-udp-636 to the VM instance running the LDAP server.",
            "Create a route called allow-udp-636 and set the next hop to be the VM instance running the LDAP server.",
            "Add a network tag of your choice to the instance. Create a firewall rule to allow ingress on UDP port 636 for that network tag.",
            "Add a network tag of your choice to the instance running the LDAP server. Create a firewall rule to allow egress on UDP port 636 for that network tag."
          ],
          "correctAnswer": [
            "Add a network tag of your choice to the instance. Create a firewall rule to allow ingress on UDP port 636 for that network tag."
          ],
          "hint": "Consider how to properly configure access to your services in GCP.",
          "explanation": "### Why Create a Firewall Rule for Ingress?\n\n#### Network Security\nTo allow clients to communicate with the LDAP server on port 636 using UDP, you must configure ingress firewall rules to permit traffic.\n\n#### Network Tags\nApplying a network tag allows you to target specific instances with firewall rules without exposing all instances in a network.\n\n### Summary of Wrong Options:\n\n**Option 1: Add network tag only**\n- ❌ Without the firewall rule, adding a tag does not allow traffic.\n\n**Option 2: Create a route**\n- ❌ Routes do not control access; firewall rules are needed.\n\n**Option 4: Egress Firewall Rule**\n- ❌ This only regulates outgoing traffic, which is not relevant for incoming client requests.\n\n🔗 [Reference: Firewall Rules](https://cloud.google.com/vpc/docs/firewalls)"
        },
        {
          "id": 111,
          "question": "You need to set a budget alert for use of Compute Engine services on one of the three Google Cloud Platform projects that you manage. All three projects are linked to a single billing account. What should you do?",
          "type": "single",
          "options": [
            "Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project.",
            "Verify that you are the project billing administrator. Select the associated billing account and create a budget and a custom alert.",
            "Verify that you are the project administrator. Select the associated billing account and create a budget for the appropriate project.",
            "Verify that you are project administrator. Select the associated billing account and create a budget and a custom alert."
          ],
          "correctAnswer": [
            "Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project."
          ],
          "hint": "Consider the roles required for managing budgets in GCP.",
          "explanation": "### Why Be the Billing Administrator?\n\n#### Necessary Permissions\nOnly project billing administrators have the necessary permissions to manage budgets and alerts associated with billing accounts in GCP.\n\n#### Monthly Budget Management\nCreating budget alerts helps in tracking costs proactively, ensuring spending aligns with expectations.\n\n### Summary of Wrong Options:\n\n**Option 2: Custom alert**\n- ❌ Misleading as creating alerts requires billing administrator permissions regardless.\n\n**Option 3: Project Administrator**\n- ❌ This role lacks the ability to manage budgets effectively.\n\n**Option 4: Project Administrator for Budgets**\n- ❌ Similar to option 3 regarding lack of permissions.\n\n🔗 [Reference: Budgeting in GCP](https://cloud.google.com/billing/docs/how-to/budgets)"
        },
        {
          "id": 112,
          "question": "You are migrating a production-critical on-premises application that requires 96 vCPUs to perform its task. You want to make sure the application runs in a similar environment on GCP. What should you do?",
          "type": "single",
          "options": [
            "When creating the VM, use machine type n1-standard-96.",
            "When creating the VM, use Intel Skylake as the CPU platform.",
            "Create the VM using Compute Engine default settings. Use gcloud to modify the running instance to have 96 vCPUs.",
            "Start the VM using Compute Engine default settings, and adjust as you go based on Rightsizing Recommendations."
          ],
          "correctAnswer": [
            "When creating the VM, use machine type n1-standard-96."
          ],
          "hint": "Consider the VM specifications required to match production needs.",
          "explanation": "### Why Use n1-standard-96?\n\n#### Performance Requirements\nThis machine type is specifically designed to meet high vCPU requirements, providing a suitable environment for production-critical applications.\n\n#### Efficient Migration\nBy selecting the correct machine type initially, you ensure performance stability without needing subsequent adjustments.\n\n### Summary of Wrong Options:\n\n**Option 2: Intel Skylake**\n- ❌ This focuses on a CPU type but does not ensure the right number of vCPUs.\n\n**Option 3: Modify Instance After Creation**\n- ❌ This can lead to downtime and resource misconfiguration.\n\n**Option 4: Default Settings**\n- ❌ Using defaults may not meet the stringent requirements of a production-critical application.\n\n🔗 [Reference: Google Cloud Machine Types](https://cloud.google.com/compute/docs/machine-types)"
        },
        {
          "id": 113,
          "question": "You want to configure a solution for archiving data in a Cloud Storage bucket. The solution must be cost-effective. Data with multiple versions should be archived after 30 days. Previous versions are accessed once a month for reporting. This archive data is also occasionally updated at month-end. What should you do?",
          "type": "single",
          "options": [
            "Add a bucket lifecycle rule that archives data with newer versions after 30 days to Coldline Storage.",
            "Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.",
            "Add a bucket lifecycle rule that archives data from regional storage after 30 days to Coldline Storage.",
            "Add a bucket lifecycle rule that archives data from regional storage after 30 days to Nearline Storage."
          ],
          "correctAnswer": [
            "Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage."
          ],
          "hint": "Think about the storage class most suitable for infrequently accessed data.",
          "explanation": "### Why Use Nearline Storage?\n\n#### Cost Efficiency\nNearline Storage is designed for data that is accessed less than once a month, making it ideal for archives where data is infrequently accessed.\n\n#### Lifecycle Management\nUsing lifecycle rules can help automate the movement of data to more cost-effective storage options based on access patterns.\n\n### Summary of Wrong Options:\n\n**Option 1: Coldline Storage**\n- ❌ Best for data that is rarely accessed, typically less than once a year, not just monthly like your case.\n\n**Option 3: Regional to Coldline**\n- ❌ Not suitable since you need near monthly access.\n\n**Option 4: Regional to Nearline**\n- ❌ This is essentially the same as the correct answer and should be ignored since the context is already mentioned in the previous correct option.\n\n🔗 [Reference: Cloud Storage Classes](https://cloud.google.com/storage/docs/storage-classes)"
        },
        {
          "id": 114,
          "question": "Your company's infrastructure is on-premises, but all machines are running at maximum capacity. You want to burst to Google Cloud. The workloads on Google Cloud must be able to directly communicate to the workloads on-premises using a private IP range. What should you do?",
          "type": "single",
          "options": [
            "In Google Cloud, configure the VPC as a host for Shared VP.",
            "In Google Cloud, configure the VPC for VPC Network Peering.",
            "Create bastion hosts both in your on-premises environment and on Google Cloud. Configure both as proxy servers using their public IP addresses.",
            "Set up Cloud VPN between the infrastructure on-premises and Google Cloud."
          ],
          "correctAnswer": [
            "Set up Cloud VPN between the infrastructure on-premises and Google Cloud."
          ],
          "hint": "Consider how to securely connect on-premises infrastructure with GCP.",
          "explanation": "### Why Set Up Cloud VPN?\n\n#### Secure Communication\nCloud VPN allows you to create a secure connection between your on-premises network and Google Cloud, enabling private IP communication.\n\n#### Cost-Effective Solution\nIt’s generally more straightforward and cost-effective to set up a VPN for this type of connectivity than managing complex peering arrangements or additional infrastructure.\n\n### Summary of Wrong Options:\n\n**Option 1: Configure as a host for Shared VP**\n- ❌ Shared VPC is for sharing resources between projects, not for direct private communication with local networks.\n\n**Option 2: VPC Network Peering**\n- ❌ Peering doesn't support on-premises connectivity directly; it is for peering between GCP VPCs.\n\n**Option 3: Bastion Hosts**\n- ❌ This adds unnecessary complexity and doesn't provide the direct private connectivity needed.\n\n🔗 [Reference: Cloud VPN Documentation](https://cloud.google.com/network-connectivity/docs/vpn)"
        },
        {
          "id": 115,
          "question": "You want to select and configure a solution for storing and archiving data on Google Cloud Platform. You need to support compliance objectives for data from one geographic location. This data is archived after 30 days and needs to be accessed annually. What should you do?",
          "type": "single",
          "options": [
            "Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.",
            "Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.",
            "Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.",
            "Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage."
          ],
          "correctAnswer": [
            "Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage."
          ],
          "hint": "Consider the best storage options for infrequent access requirements.",
          "explanation": "### Why Use Regional Storage with Coldline?\n\n#### Storage Class\nRegional Storage optimally supports compliance objectives by keeping data localized while Coldline is designed for long-term storage of infrequently accessed data. This setup meets the annual access requirement efficiently.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Multi-Regional Storage**\n- ❌ More expensive; unnecessary for compliance objectives that allow for regional storage.\n\n**Option 3: Regional with Nearline**\n- ❌ Nearline is for data accessed less than once a month, which is not suitable for annual access.\n\n🔗 [Reference: Cloud Storage Classes](https://cloud.google.com/storage/docs/storage-classes)"
        },
        {
          "id": 116,
          "question": "Your company uses BigQuery for data warehousing. Over time, many different business units in your company have created 1000+ datasets across hundreds of projects. Your CIO wants you to examine all datasets to find tables that contain an employee_ssn column. You want to minimize effort in performing this task. What should you do?",
          "type": "single",
          "options": [
            "Go to Data Catalog and search for employee_ssn in the search box.",
            "Write a shell script that uses the bq command line tool to loop through all the projects in your organization.",
            "Write a script that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find the employee_ssn column.",
            "Write a Cloud Dataflow job that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find the employee_ssn column."
          ],
          "correctAnswer": [
            "Go to Data Catalog and search for employee_ssn in the search box."
          ],
          "hint": "Think about tools within GCP that optimize metadata management.",
          "explanation": "### Why Use Data Catalog?\n\n#### Efficient Metadata Management\nData Catalog allows users to search for metadata, including schema information, across many datasets quickly, providing a streamlined way to find the specific columns you need, thereby minimizing manual coding or scripting efforts.\n\n### Summary of Wrong Options:\n\n**Options 2 and 3: Scripting**\n- ❌ Writing scripts requires a substantial amount of manual effort, and might not efficiently utilize GCP's built-in capabilities.\n\n**Option 4: Cloud Dataflow job**\n- ❌ Unnecessarily complex for the task, which can be done with simpler tools like Data Catalog.\n\n🔗 [Reference: Data Catalog Overview](https://cloud.google.com/data-catalog/docs/overview)"
        },
        {
          "id": 117,
          "question": "You create a Deployment with 2 replicas in a Google Kubernetes Engine cluster that has a single preemptible node pool. After a few minutes, you use kubectl to examine the status of your Pod and observe that one of them is still in Pending status. What is the most likely cause?\n\n![Deployed pods status](https://github.com/Ditectrev/Google-Cloud-Platform-GCP-Associate-Cloud-Engineer-Practice-Tests-Exams-Questions-Answers/raw/main/images/pods.png)",
          "type": "single",
          "options": [
            "The pending Pod's resource requests are too large to fit on a single node of the cluster.",
            "Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.",
            "The node pool is configured with a service account that does not have permission to pull the container image used by the pending Pod.",
            "The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node."
          ],
          "correctAnswer": [
            "The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node."
          ],
          "hint": "Consider the characteristics of preemptible VM instances.",
          "explanation": "### Why Was the Pod Preempted?\n\n#### Preemptible VMs\nPreemptible nodes can be terminated at any time when GCP needs to reclaim resources, which means the scheduled Pods would also be affected. In this case, the pending Pod is in the process of rescheduling on a different node.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Resource Issues**\n- ❌ The cluster might support both replicas when space allows, especially with a preemptible pool where resource allocation can be dynamic in nature.\n\n**Option 3: Service Account Permissions**\n- ❌ This would not cause a Pod to be stuck in Pending status but rather in Failed or CrashLoopBackOff state.\n\n🔗 [Reference: Kubernetes Pod Scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling/)"
        },
        {
          "id": 118,
          "question": "You want to find out when users were added to Cloud Spanner Identity Access Management (IAM) roles on your Google Cloud Platform (GCP) project. What should you do in the GCP Console?",
          "type": "single",
          "options": [
            "Open the Cloud Spanner console to review configurations.",
            "Open the IAM & admin console to review IAM policies for Cloud Spanner roles.",
            "Go to the Stackdriver Monitoring console and review information for Cloud Spanner.",
            "Go to the Stackdriver Logging console, review admin activity logs, and filter them for Cloud Spanner IAM roles."
          ],
          "correctAnswer": [
            "Go to the Stackdriver Logging console, review admin activity logs, and filter them for Cloud Spanner IAM roles."
          ],
          "hint": "Think about how IAM role changes are logged in GCP.",
          "explanation": "### Why Use Stackdriver Logging Console?\n\n#### Admin Activity Logs\nAdmin Activity logs are specifically designed to track changes and activity regarding IAM roles and permissions, allowing you to trace who added which users and when.\n\n### Summary of Wrong Options:\n\n**Option 1: Cloud Spanner console**\n- ❌ This interface is not focused on logging IAM-related changes.\n\n**Option 2: IAM Policies Review**\n- ❌ This only shows current permissions, not when changes were made.\n\n**Option 3: Stackdriver Monitoring**\n- ❌ Monitoring does not typically include audit and activity logging relevant to IAM role changes.\n\n🔗 [Reference: Logging IAM Changes](https://cloud.google.com/logging/docs/audit#admin_activity_logs)"
        },
        {
          "id": 119,
          "question": "Your company implemented BigQuery as an enterprise data warehouse. Users from multiple business units run queries on this data warehouse. However, you notice that query costs for BigQuery are very high, and you need to control costs. Which two methods should you use? (Choose two.)",
          "type": "multiple",
          "options": [
            "Split the users from business units to multiple projects.",
            "Apply a user- or project-level custom query quota for BigQuery data warehouse.",
            "Create separate copies of your BigQuery data warehouse for each business unit.",
            "Split your BigQuery data warehouse into multiple data warehouses for each business unit.",
            "Change your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project."
          ],
          "correctAnswer": [
            "Apply a user- or project-level custom query quota for BigQuery data warehouse.",
            "Change your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project."
          ],
          "hint": "Consider cost control strategies relevant to data warehousing in GCP.",
          "explanation": "### Why Use Custom Query Quotas and Flat Rate?\n\n#### Cost Management\nSetting quotas on queries helps prevent overuse and high costs by controlling how much each user or project can consume.\nSwitching to a flat-rate model helps control costs more predictably by allocating a set number of slots for parallel processing of queries, making it easier to budget spending.\n\n### Summary of Wrong Options:\n\n**Option 1: Split Users by Projects**\n- ❌ This does not address the cost problem and might complicate data access for business units.\n\n**Option 3: Creating Separate Copies**\n- ❌ This would increase storage costs significantly.\n\n**Option 4: Splitting Data Warehouses**\n- ❌ Similar to option 3, which does not effectively deal with cost issues.\n\n🔗 [Reference: Managing BigQuery Costs](https://cloud.google.com/bigquery/docs/how-to-manage-costs)"
        },
        {
          "id": 120,
          "question": "You are building a product on top of Google Kubernetes Engine (GKE). You have a single GKE cluster. For each of your customers, a Pod is running in that cluster, and your customers can run arbitrary code inside their Pod. You want to maximize the isolation between your customers' Pods. What should you do?",
          "type": "single",
          "options": [
            "Use Binary Authorization and whitelist only the container images used by your customers' Pods.",
            "Use the Container Analysis API to detect vulnerabilities in the containers used by your customers' Pods.",
            "Create a GKE node pool with a sandbox type configured to gvisor. Add the parameter runtimeClassName: gvisor to the specification of your customers' Pods.",
            "Use the cos_containerd image for your GKE nodes. Add a nodeSelector with the value cloud.google.com/gke-os-distribution: cos_containerd to the specification of your customers' Pods."
          ],
          "correctAnswer": [
            "Create a GKE node pool with a sandbox type configured to gvisor. Add the parameter runtimeClassName: gvisor to the specification of your customers' Pods."
          ],
          "hint": "Think about how to isolate workloads in a shared cluster environment.",
          "explanation": "### Why Use gVisor for Isolation?\n\n#### Enhanced Security\ngVisor provides a secure isolation boundary for running applications in containers, preventing malicious activities within a Pod from affecting others. It is specifically designed to support scenarios where arbitrary code execution is allowed.\n\n### Summary of Wrong Options:\n\n**Option 1: Binary Authorization**\n- ❌ While this secures deployment, it does not provide runtime isolation for arbitrary code execution.\n\n**Option 2: Container Analysis API**\n- ❌ This only evaluates vulnerabilities but does not enforce isolation between Pods.\n\n**Option 4: Cos_containerd image**\n- ❌ This does not offer the same level of isolation as using gVisor.\n\n🔗 [Reference: gVisor Documentation](https://gvisor.dev/gvisor/)"
        },
        {
          "id": 121,
          "question": "Your customer has implemented a solution that uses Cloud Spanner and notices some read latency-related performance issues on one table. This table is accessed only by their users using a primary key. The table schema is shown below.\n\n![Database table](https://github.com/Ditectrev/Google-Cloud-Platform-GCP-Associate-Cloud-Engineer-Practice-Tests-Exams-Questions-Answers/raw/main/images/db_table.png)\n\n![DDL](https://github.com/Ditectrev/Google-Cloud-Platform-GCP-Associate-Cloud-Engineer-Practice-Tests-Exams-Questions-Answers/raw/main/images/ddl.png)",
          "type": "single",
          "options": [
            "Remove the profile_picture field from the table.",
            "Add a secondary index on the person_id column.",
            "Change the primary key to not have monotonically increasing values.",
            "Create a secondary index using the following Data Definition Language (DDL):"
          ],
          "correctAnswer": [
            "Change the primary key to not have monotonically increasing values."
          ],
          "hint": "Consider how the primary key structure affects performance in Cloud Spanner.",
          "explanation": "### Why Avoid Monotonically Increasing Values?\n\n#### Performance Implications\nMonotonically increasing primary keys can lead to performance bottlenecks as they can result in hotspots where many writes go to the same location. Distributing keys more evenly can help balance the load and improve read performance.\n\n### Summary of Wrong Options:\n\n**Option 1: Remove profile_picture**\n- ❌ This does not directly address read latency issues.\n\n**Option 2: Add secondary index**\n- ❌ While secondary indexes can help with certain queries, they won't necessarily resolve the performance issues tied to the primary key structure.\n\n**Option 4: Create a secondary index with DDL**\n- ❌ Creating an index on a poorly designed primary key won't alleviate existing latency issues.\n\n🔗 [Reference: Cloud Spanner Performance Optimization](https://cloud.google.com/spanner/docs/performance)"
        },
        {
          "id": 122,
          "question": "Your finance team wants to view the billing report for your projects. You want to make sure that the finance team does not get additional permissions to the project. What should you do?",
          "type": "single",
          "options": [
            "Add the group for the finance team to roles/billing user role.",
            "Add the group for the finance team to roles/billing admin role.",
            "Add the group for the finance team to roles/billing viewer role.",
            "Add the group for the finance team to roles/billing project/Manager role."
          ],
          "correctAnswer": [
            "Add the group for the finance team to roles/billing viewer role."
          ],
          "hint": "Consider the principle of least privilege when granting access.",
          "explanation": "### Why Use the Billing Viewer Role?\n\n#### Minimal Permissions\nThe roles/billing viewer role allows users to view billing data without providing permissions to alter project settings. This aligns with the principle of least privilege, ensuring the finance team can access necessary reports without additional permissions.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Billing User/Admin Role**\n- ❌ These roles provide more permissions than necessary, enabling users to modify billing settings or information.\n\n**Option 4: Billing Project Manager**\n- ❌ This role grants permissions to manage resources and projects, which is not advisable for the finance team's needs.\n\n🔗 [Reference: IAM Roles](https://cloud.google.com/iam/docs/understanding-roles)"
        },
        {
          "id": 123,
          "question": "Your organization has strict requirements to control access to Google Cloud projects. You need to enable your Site Reliability Engineers (SREs) to approve requests from the Google Cloud support team when an SRE opens a support case. You want to follow Google-recommended practices. What should you do?",
          "type": "single",
          "options": [
            "Add your SREs to roles/iam.roleAdmin role.",
            "Add your SREs to roles/accessapproval.approver role.",
            "Add your SREs to a group and then add this group to roles/iam.roleAdmin.role.",
            "Add your SREs to a group and then add this group to roles/accessapproval.approver role."
          ],
          "correctAnswer": [
            "Add your SREs to a group and then add this group to roles/accessapproval.approver role."
          ],
          "hint": "Think about how to manage approval workflows securely.",
          "explanation": "### Why Use the Access Approval Role?\n\n#### Role-Specific Permissions\nThe roles/accessapproval.approver role is specifically intended for users who need to approve access requests and is conducive to governance and strict access controls. This role ensures that only designated SREs can approve sensitive access requests.\n\n### Summary of Wrong Options:\n\n**Option 1: IAM Role Admin**\n- ❌ This role gives excessive permissions beyond what is necessary for approving requests.\n\n**Option 2: Granting the role directly**\n- ❌ Adding SREs directly to the approver role bypasses the group management approach, which is preferable for easier access management.\n\n**Option 3: Group without Access Approval Role**\n- ❌ Using the role Admin again gives more permissions than are needed, which goes against best practices.\n\n🔗 [Reference: Access Approval](https://cloud.google.com/access-approval/docs/overview)"
        },
        {
          "id": 124,
          "question": "You need to host an application on a Compute Engine instance in a project shared with other teams. You want to prevent the other teams from accidentally causing downtime on that application. Which feature should you use?",
          "type": "single",
          "options": [
            "Use a Shielded VM.",
            "Use a Preemptible VM.",
            "Use a sole-tenant node.",
            "Enable deletion protection on the instance."
          ],
          "correctAnswer": ["Enable deletion protection on the instance."],
          "hint": "Consider ways to safeguard instances in a shared environment.",
          "explanation": "### Why Enable Deletion Protection?\n\n#### Prevent Accidental Deletion\nEnabling deletion protection on a Compute Engine instance is crucial in a shared environment as it safeguards the instance from being accidentally deleted by users from other teams, thereby minimizing the risk of unintentional downtime or service disruptions.\n\n### Summary of Wrong Options:\n\n**Option 1: Shielded VM**\n- ❌ This provides advanced security features but does not specifically protect against deletion.\n\n**Option 2: Preemptible VM**\n- ❌ These instances are ephemeral and can be terminated by GCP at any time, not suitable for production workloads.\n\n**Option 3: Sole-Tenant Node**\n- ❌ While this offers dedicated resources, it does not specifically prevent deletion of instances hosted on it.\n\n🔗 [Reference: Deletion Protection](https://cloud.google.com/compute/docs/instances/deletion-protection)"
        },
        {
          "id": 125,
          "question": "Your organization needs to grant users access to query datasets in BigQuery but prevent them from accidentally deleting the datasets. You want a solution that follows Google-recommended practices. What should you do?",
          "type": "single",
          "options": [
            "Add users to roles/bigquery user role only, instead of roles/bigquery dataOwner.",
            "Add users to roles/bigquery dataEditor role only, instead of roles/bigquery dataOwner.",
            "Create a custom role by removing delete permissions, and add users to that role only.",
            "Create a custom role by removing delete permissions. Add users to the group, and then add the group to the custom role."
          ],
          "correctAnswer": [
            "Create a custom role by removing delete permissions. Add users to the group, and then add the group to the custom role."
          ],
          "hint": "Think about customizing roles to align with access control needs.",
          "explanation": "### Why Create a Custom Role?\n\n#### Customization for Access Control\nCreating a custom role that excludes delete permissions allows you to tailor user access specifically to querying datasets without granting them the ability to modify or delete important data, which aligns with best practices for security and data integrity.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Default Roles**\n- ❌ These roles provide permissions that may include undesired capabilities that could lead to accidental deletions.\n\n**Option 3: Custom Role Only**\n- ❌ Adding users to a custom role without a group may complicate access management and scalability.\n\n🔗 [Reference: Custom Roles](https://cloud.google.com/iam/docs/creating-custom-roles)"
        },
        {
          "id": 126,
          "question": "You have a developer laptop with the Cloud SDK installed on Ubuntu. The Cloud SDK was installed from the Google Cloud Ubuntu package repository. You want to test your application locally on your laptop with Cloud Datastore. What should you do?",
          "type": "single",
          "options": [
            "Export Cloud Datastore data using gcloud datastore export.",
            "Create a Cloud Datastore index using gcloud datastore indexes create.",
            "Install the google-cloud-sdk-datastore-emulator component using the apt get install command.",
            "Install the cloud-datastore-emulator component using the gcloud components install command."
          ],
          "correctAnswer": [
            "Install the google-cloud-sdk-datastore-emulator component using the apt get install command."
          ],
          "hint": "Think about how to emulate GCP services locally.",
          "explanation": "### Why Install the Datastore Emulator?\n\n#### Local Development Environment\nThe `google-cloud-sdk-datastore-emulator` allows developers to run a local instance of Cloud Datastore, facilitating the testing of applications without incurring costs or needing to connect to production.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2**\n- ❌ These actions are focused on exporting data or creating indexes and do not directly aid in local testing.\n\n**Option 4: gcloud components install**\n- ❌ This command does not specifically reference the emulator; the accurate command for Ubuntu uses apt packages.\n\n🔗 [Reference: Datastore Emulator](https://cloud.google.com/datastore/docs/emulator)"
        },
        {
          "id": 127,
          "question": "Your company set up a complex organizational structure on Google Cloud. The structure includes hundreds of folders and projects. Only a few team members should be able to view the hierarchical structure. You need to assign minimum permissions to these team members, and you want to follow Google-recommended practices. What should you do?",
          "type": "single",
          "options": [
            "Add the users to roles/browser role.",
            "Add the users to roles/iam.roleViewer role.",
            "Add the users to a group, and add this group to roles/browser.",
            "Add the users to a group, and add this group to roles/iam.roleViewer role."
          ],
          "correctAnswer": [
            "Add the users to a group, and add this group to roles/browser."
          ],
          "hint": "Consider the most appropriate roles for viewing organizational structures.",
          "explanation": "### Why Use the Roles/Browser Role?\n\n#### Appropriate for Viewing Structure\nThe roles/browser role allows users to view the hierarchy without granting any permissions to manage or alter the organization, aligning with the principle of least privilege.\n\n### Summary of Wrong Options:\n\n**Option 2: IAM Role Viewer**\n- ❌ This role is intended for viewing IAM roles and permissions, not the organizational structure generally.\n\n**Option 3: Group as Browser Role**\n- ❌ This would work, but it is less efficient than defining role assignments to specific groups that allow for better management of permissions.\n\n**Option 4: Group as IAM Role Viewer**\n- ❌ This role doesn't align with the viewing needs concerning organizational hierarchy.\n\n🔗 [Reference: IAM Roles](https://cloud.google.com/iam/docs/understanding-roles)"
        },
        {
          "id": 128,
          "question": "Your company has a single sign-on (SSO) identity provider that supports Security Assertion Markup Language (SAML) integration with service providers. Your company has users in Cloud Identity. You would like users to authenticate using your company's SSO provider. What should you do?",
          "type": "single",
          "options": [
            "In Cloud Identity, set up SSO with Google as an identity provider to access custom SAML apps.",
            "In Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider.",
            "Obtain OAuth 2.0 credentials, configure the user consent screen, and set up OAuth 2.0 for Mobile & Desktop Apps.",
            "Obtain OAuth 2.0 credentials, configure the user consent screen, and set up OAuth 2.0 for Web Server Applications."
          ],
          "correctAnswer": [
            "In Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider."
          ],
          "hint": "Think about the correct integration approach for SSO.",
          "explanation": "### Why Use a Third-Party Identity Provider?\n\n#### SAML Integration\nSetting up SSO with a third-party identity provider is essential when your SSO provider supports SAML. This configuration allows users to authenticate seamlessly against your existing identity provider, enhancing security and user experience.\n\n### Summary of Wrong Options:\n\n**Option 1: Google Identity Provider**\n- ❌ This setup is not suitable for integrating with your third-party identity provider.\n\n**Options 3 and 4: OAuth 2.0 Credentials**\n- ❌ These options are irrelevant to SAML-based SSO configurations and serve different authentication methods, not SSO.\n\n🔗 [Reference: SSO Setup in Cloud Identity](https://support.google.com/cloudidentity/answer/9112677?hl=en)"
        },
        {
          "id": 129,
          "question": "Your organization has a dedicated person who creates and manages all service accounts for Google Cloud projects. You need to assign this person the minimum role for projects. What should you do?",
          "type": "single",
          "options": [
            "Add the user to roles/iam.roleAdmin role.",
            "Add the user to roles/iam.securityAdmin role.",
            "Add the user to roles/iam.serviceAccountUser role.",
            "Add the user to roles/iam.serviceAccountAdmin role."
          ],
          "correctAnswer": [
            "Add the user to roles/iam.serviceAccountAdmin role."
          ],
          "hint": "Think about which role provides the necessary permissions for managing service accounts.",
          "explanation": "### Why Use the Service Account Admin Role?\n\n#### Required Permissions\nThe roles/iam.serviceAccountAdmin role grants the necessary permissions to create and manage service accounts without giving excessive privileges over other IAM roles or security settings in the project.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Role Admin/Security Admin**\n- ❌ These roles provide far more permissions than needed for simply managing service accounts.\n\n**Option 3: Service Account User**\n- ❌ This role only allows the user to act as a service account but doesn’t provide the ability to create or manage them.\n\n🔗 [Reference: IAM Roles for Service Accounts](https://cloud.google.com/iam/docs/service-accounts#roles)"
        },
        {
          "id": 130,
          "question": "You are building an archival solution for your data warehouse and have selected Cloud Storage to archive your data. Your users need to be able to access this archived data once a quarter for some regulatory requirements. You want to select a cost-efficient option. Which storage option should you use?",
          "type": "single",
          "options": [
            "Cold Storage.",
            "Nearline Storage.",
            "Regional Storage.",
            "Multi-Regional Storage."
          ],
          "correctAnswer": ["Nearline Storage."],
          "hint": "Consider the access frequency required for your archived data.",
          "explanation": "### Why Use Nearline Storage?\n\n#### Cost Efficiency for Infrequent Access\nNearline Storage is designed for data that is accessed less than once a month, which fits perfectly with the quarterly access requirement for regulatory compliance at a lower cost than Cold Storage, which is optimized for less frequent access.\n\n### Summary of Wrong Options:\n\n**Option 1: Cold Storage**\n- ❌ Best suited for data stored long-term and accessed less than once a year, which may incur higher costs for quarterly access.\n\n**Options 3 and 4: Regional and Multi-Regional**\n- ❌ These options are primarily for frequently accessed data and would not be cost-effective if the data is only accessed quarterly.\n\n🔗 [Reference: Cloud Storage Classes](https://cloud.google.com/storage/docs/storage-classes)"
        },
        {
          "id": 131,
          "question": "A team of data scientists infrequently needs to use a Google Kubernetes Engine (GKE) cluster that you manage. They require GPUs for some long-running, nonrestartable jobs. You want to minimize cost. What should you do?",
          "type": "single",
          "options": [
            "Enable node auto-provisioning on the GKE cluster.",
            "Create a VerticalPodAutoscaler for those workloads.",
            "Create a node pool with preemptible VMs and GPUs attached to those VMs.",
            "Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1."
          ],
          "correctAnswer": [
            "Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1."
          ],
          "hint": "Think about managing costs while still providing necessary resources.",
          "explanation": "### Why Enable Autoscaling on a GPU Node Pool?\n\n#### Cost Management\nBy creating a dedicated node pool for GPU instances and enabling autoscaling, you ensure that resources are provisioned only when needed, thereby minimizing costs while still accommodating the data scientists' requirements for GPUs during their infrequent jobs.\n\n### Summary of Wrong Options:\n\n**Option 1: Auto-provisioning**\n- ❌ While useful, this may not specifically optimize costs related to GPU usage as it could lead to unnecessary resources being provisioned.\n\n**Option 2: Vertical Pod Autoscaler**\n- ❌ This adjusts pod resource requests but does not manage the underlying node resources effectively for infrequent GPU needs.\n\n**Option 3: Preemptible VMs**\n- ❌ While cost-effective, preemptible VMs can be suddenly terminated, which isn't suitable for long-running, nonrestartable jobs.\n\n🔗 [Reference: GKE Autoscaling](https://cloud.google.com/kubernetes-engine/docs/concepts/autoscaler)"
        },
        {
          "id": 132,
          "question": "Your organization has user identities in Active Directory. Your organization wants to use Active Directory as their source of truth for identities. Your organization wants to have full control over the Google accounts used by employees for all Google services, including your Google Cloud Platform (GCP) organization. What should you do?",
          "type": "single",
          "options": [
            "Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.",
            "Use the cloud Identity APIs and write a script to synchronize users to Cloud Identity.",
            "Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.",
            "Ask each employee to create a Google account using self signup. Require that each employee use their company email address and password."
          ],
          "correctAnswer": [
            "Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity."
          ],
          "hint": "Consider the best method for integrating existing user directories with Google services.",
          "explanation": "### Why Use Google Cloud Directory Sync?\n\n#### Automated Synchronization\nGCDS allows for seamless synchronization of user accounts from Active Directory into Cloud Identity, keeping user data in sync and ensuring that user authentication is consistent across systems without requiring manual intervention.\n\n### Summary of Wrong Options:\n\n**Option 2: Cloud Identity APIs**\n- ❌ While possible, writing scripts for user synchronization is much more complex and error-prone compared to using GCDS.\n\n**Option 3: CSV Export/Import**\n- ❌ This approach is manual and not scalable over time for continuous synchronization, leading to potential data inconsistencies.\n\n**Option 4: Self Signup**\n- ❌ This method loses control over identity management and does not integrate well with existing Active Directory data.\n\n🔗 [Reference: Google Cloud Directory Sync](https://support.google.com/a/gts/9673004?hl=en)"
        },
        {
          "id": 133,
          "question": "You have successfully created a development environment in a project for an application. This application uses Compute Engine and Cloud SQL. Now you need to create a production environment for this application. The security team has forbidden the existence of network routes between these 2 environments and has asked you to follow Google-recommended practices. What should you do?",
          "type": "single",
          "options": [
            "Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment.",
            "Create a new production subnet in the existing VPC and a new production Cloud SQL instance in your existing project, and deploy your application using those resources.",
            "Create a new project, modify your existing VPC to be a Shared VPC, share that VPC with your new project, and replicate the setup you have in the development environment in that new project in the Shared VPC.",
            "Ask the security team to grant you the Project Editor role in an existing production project used by another division of your company. Once they grant you that role, replicate the setup you have in the development environment in that project."
          ],
          "correctAnswer": [
            "Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment."
          ],
          "hint": "Think about how to maintain strict network isolation between environments.",
          "explanation": "### Why Create a New Project?\n\n#### Enforcing Network Isolation\nCreating a new project ensures complete network isolation between the development and production environments, complying with security requirements while allowing independent management of resources.\n\n### Summary of Wrong Options:\n\n**Option 2: Same VPC for Production**\n- ❌ This directly violates the security team's requirement for no network routes between environments.\n\n**Option 3: Shared VPC**\n- ❌ While this could work, it still implies a shared network context between dev and prod, which is against the security team’s directives.\n\n**Option 4: Use Another Division’s Project**\n- ❌ This poses administrative issues and violates best practices for environment segregation.\n\n🔗 [Reference: Project Structure Best Practices](https://cloud.google.com/docs/overview-architecture-best-practices#project_structure)"
        },
        {
          "id": 134,
          "question": "Your management has asked an external auditor to review all the resources in a specific project. The security team has enabled the Organization Policy called Domain Restricted Sharing on the organization node by specifying only your Cloud Identity domain. You want the auditor to only be able to view, but not modify, the resources in that project. What should you do?",
          "type": "single",
          "options": [
            "Ask the auditor for their Google account, and give them the Viewer role on the project.",
            "Ask the auditor for their Google account, and give them the Security Reviewer role on the project.",
            "Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.",
            "Create a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project."
          ],
          "correctAnswer": [
            "Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project."
          ],
          "hint": "Consider the best way to grant audit capabilities without compromising security.",
          "explanation": "### Why Use a Temporary Account?\n\n#### Controlled Access\nCreating a temporary account exclusively for the auditor and assigning the Viewer role minimizes the risk of unauthorized modifications while allowing comprehensive review capabilities. This follows security protocols for maintaining a controlled environment.\n\n### Summary of Wrong Options:\n\n**Option 1: Auditor's Personal Account**\n- ❌ Cannot guarantee access compliance with the domain restrictions set by the organization policy.\n\n**Option 2: Security Reviewer Role**\n- ❌ The Security Reviewer role may confer additional permissions that are unnecessary for simply viewing resources.\n\n**Option 4: Auditor Account with Security Reviewer**\n- ❌ This again provides more permissions than needed and violates the principle of least privilege.\n\n🔗 [Reference: Managing Access with IAM](https://cloud.google.com/iam/docs/overview)"
        },
        {
          "id": 135,
          "question": "You have a workload running on Compute Engine that is critical to your business. You want to ensure that the data on the boot disk of this workload is backed up regularly. You need to be able to restore a backup as quickly as possible in case of disaster. You also want older backups to be cleaned automatically to save on cost. You want to follow Google-recommended practices. What should you do?",
          "type": "single",
          "options": [
            "Create a Cloud Function to create an instance template.",
            "Create a snapshot schedule for the disk using the desired interval.",
            "Create a cron job to create a new disk from the disk using gcloud.",
            "Create a Cloud Task to create an image and export it to Cloud Storage."
          ],
          "correctAnswer": [
            "Create a snapshot schedule for the disk using the desired interval."
          ],
          "hint": "Think about the best way to automate backups in Google Cloud.",
          "explanation": "### Why Use a Snapshot Schedule?\n\n#### Automated Backups\nCreating a snapshot schedule for the disk allows you to automate regular backups of your critical workload, ensuring quick recovery in case of a failure while also enabling automatic deletion of older snapshots based on retention policies, thus managing costs efficiently.\n\n### Summary of Wrong Options:\n\n**Option 1: Cloud Function**\n- ❌ This is unnecessary complexity when snapshot schedules can accomplish the same goal effectively.\n\n**Options 3 and 4: Cron jobs and Cloud Tasks**\n- ❌ These approaches require more manual setup and maintenance without the simplicity and reliability of snapshot schedules.\n\n🔗 [Reference: Managing Snapshots](https://cloud.google.com/compute/docs/disks/scheduling-snapshots)"
        },
        {
          "id": 136,
          "question": "You need to assign a Cloud Identity and Access Management (Cloud IAM) role to an external auditor. The auditor needs to have permissions to review your Google Cloud Platform (GCP) Audit Logs and also to review your Data Access logs. What should you do?",
          "type": "single",
          "options": [
            "Assign the auditor the IAM role roles/logging.privateLogViewer. Perform the export of logs to Cloud Storage.",
            "Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy.",
            "Assign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Perform the export of logs to Cloud Storage.",
            "Assign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Direct the auditor to also review the logs for changes to Cloud IAM policy."
          ],
          "correctAnswer": [
            "Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy."
          ],
          "hint": "Consider the specific needs for auditing logs and the permissions required.",
          "explanation": "### Why Use roles/logging.privateLogViewer?\n\n#### Required Audit Permissions\nthe IAM role `roles/logging.privateLogViewer` provides the necessary permissions for the auditor to view GCP audit logs, including Data Access logs, and ensures compliance with security protocols while allowing for review of changes to IAM policies.\n\n### Summary of Wrong Options:\n\n**Options 1 and 3: Export Logs**\n- ❌ Exporting logs unnecessarily complicates access when the viewer role suffices for audit purposes.\n\n**Option 4: Custom Role**\n- ❌ Creating a custom role for this task is overly complex and does not provide the guaranteed access of using predefined roles provided by GCP.\n\n🔗 [Reference: IAM Roles for Logging](https://cloud.google.com/logging/docs/access-control)"
        },
        {
          "id": 137,
          "question": "You are managing several Google Cloud Platform (GCP) projects and need access to all logs for the past 60 days. You want to be able to explore and quickly analyze the log contents. You want to follow Google-recommended practices to obtain the combined logs for all projects. What should you do?",
          "type": "single",
          "options": [
            "Navigate to Stackdriver Logging and select resource.labels.project_id=\"*\".",
            "Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days.",
            "Create a Stackdriver Logging Export with a Sink destination to Cloud Storage. Create a lifecycle rule to delete objects after 60 days.",
            "Configure a Cloud Scheduler job to read from Stackdriver and store the logs in BigQuery. Configure the table expiration to 60 days."
          ],
          "correctAnswer": [
            "Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days."
          ],
          "hint": "Think about the best way to aggregate logs from multiple projects for analysis.",
          "explanation": "### Why Use a BigQuery Sink?\n\n#### Efficient Log Analysis\nCreating a Stackdriver Logging Export that directs logs to BigQuery allows for flexible querying and analysis of log data across multiple projects, while setting a table expiration keeps storage costs manageable for logs that are no longer needed after 60 days.\n\n### Summary of Wrong Options:\n\n**Option 1: Inefficient Manual Approach**\n- ❌ Manually selecting resources does not facilitate log aggregation for analysis across projects.\n\n**Option 3: Cloud Storage Export**\n- ❌ This setup lacks the robust querying capabilities of BigQuery and could lead to higher costs and inefficiency.\n\n**Option 4: Cloud Scheduler Job**\n- ❌ An unnecessary complexity when a direct export to BigQuery is simpler and more effective for log management.\n\n🔗 [Reference: Logging Export to BigQuery](https://cloud.google.com/logging/docs/export)"
        },
        {
          "id": 138,
          "question": "You need to reduce GCP service costs for a division of your company using the fewest possible steps. You need to turn off all configured services in an existing GCP project. What should you do?",
          "type": "single",
          "options": [
            "1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID.",
            "1. Verify that you are assigned the Project Owners IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them.",
            "1. Verify that you are assigned the Organizational Administrator IAM role for this project. 2. Locate the project in the GCP console, enter the project ID and then click Shut down.",
            "1. Verify that you are assigned the Organizational Administrators IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them."
          ],
          "correctAnswer": [
            "1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID."
          ],
          "hint": "Think about a complete shutdown procedure versus other options.",
          "explanation": "### Why Shut Down the Project?\n\n#### Complete Service Termination\nShutting down the project is a more efficient way to disable all services and stop all associated costs instantly compared to deleting resources individually, which may still incur some costs if not managed properly. This method ensures that the project and its resources can easily be revived if needed later.\n\n### Summary of Wrong Options:\n\n**Options 2 and 4**\n- ❌ Deleting resources individually can be time-consuming and could lead to potential leftover costs or orphaned services that may not be apparent immediately.\n\n**Option 3: Organizational Admin Role**\n- ❌ While this role allows project modifications, it is not the most efficient way to terminate service costs compared to the Project Owner role.\n\n🔗 [Reference: Managing Projects](https://cloud.google.com/resource-manager/docs/projects#shutting-down-a-project)"
        },
        {
          "id": 139,
          "question": "You are configuring service accounts for an application that spans multiple projects. Virtual Machines (VMs) running in the web-applications project need access to BigQuery datasets in crm-databases-proj. You want to follow Google-recommended practices to give access to the service account in the web-applications project. What should you do?",
          "type": "single",
          "options": [
            "Give project owner for web-applications appropriate roles to crm-databases-proj.",
            "Give project owner role to crm-databases-proj and the web-applications project.",
            "Give project owner role to crm-databases-proj and bigquery.dataViewer role to web-applications.",
            "Give bigquery.dataViewer role to crm-databases-proj and appropriate roles to web-applications."
          ],
          "correctAnswer": [
            "Give bigquery.dataViewer role to crm-databases-proj and appropriate roles to web-applications."
          ],
          "hint": "Think about the principle of least privilege when granting permissions.",
          "explanation": "### Why Use bigquery.dataViewer Role?\n\n#### Principle of Least Privilege\nAssigning the `bigquery.dataViewer` role to the `crm-databases-proj` allows the VMs in the `web-applications` project to query the necessary datasets without granting excessive permissions, thus following the principle of least privilege while ensuring functionality across projects.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Project Owner Role**\n- ❌ Granting the Owner role gives more permissions than necessary, which is not a recommended practice for security compliance.\n\n**Option 3: Multiple Owner Roles**\n- ❌ This unnecessary duplication of ownership roles complicates permissions and does not serve the least privilege requirement effectively.\n\n🔗 [Reference: IAM Roles for BigQuery](https://cloud.google.com/bigquery/docs/access-control)"
        },
        {
          "id": 140,
          "question": "An employee was terminated, but their access to Google Cloud Platform (GCP) was not removed until 2 weeks later. You need to find out if this employee accessed any sensitive customer information after their termination. What should you do?",
          "type": "single",
          "options": [
            "View System Event Logs in Stackdriver. Search for the user's email as the principal.",
            "View System Event Logs in Stackdriver. Search for the service account associated with the user.",
            "View Data Access audit logs in Stackdriver. Search for the user's email as the principal.",
            "View the Admin Activity log in Stackdriver. Search for the service account associated with the user."
          ],
          "correctAnswer": [
            "View Data Access audit logs in Stackdriver. Search for the user's email as the principal."
          ],
          "hint": "Think about which logs track access to sensitive data.",
          "explanation": "### Why Use Data Access Audit Logs?\n\n#### Detailed Access Tracking\nThe Data Access audit logs specifically log reads and modifications to sensitive data and resources, making them the most appropriate logs to check for any unauthorized access by the terminated employee after their account was supposed to be deactivated.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: System Event Logs**\n- ❌ These logs provide information about system activity but do not track access to sensitive resources or data directly.\n\n**Option 4: Admin Activity Logs**\n- ❌ While these logs are useful for tracking administrative actions, they do not provide details on data access by users.\n\n🔗 [Reference: Understanding Audit Logs](https://cloud.google.com/logging/docs/audit-logs)"
        },
        {
          "id": 141,
          "question": "You need to create a custom IAM role for use with a GCP service. All permissions in the role must be suitable for production use. You also want to clearly share with your organization the status of the custom role. This will be the first version of the custom role. What should you do?",
          "type": "single",
          "options": [
            "Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to ALPHA while testing the role permissions.",
            "Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to BETA while testing the role permissions.",
            "Use permissions in your role that use the 'testing' support level for role permissions. Set the role stage to ALPHA while testing the role permissions.",
            "Use permissions in your role that use the 'testing' support level for role permissions. Set the role stage to BETA while testing the role permissions."
          ],
          "correctAnswer": [
            "Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to ALPHA while testing the role permissions."
          ],
          "hint": "Consider the recommended practices for staging custom roles.",
          "explanation": "### Why Use Supported Permissions with Alpha Stage?\n\n#### Best Practices for Rollout\nUsing 'supported' permissions ensures compatibility and reliability in production use. Setting the role stage to ALPHA indicates that it is still undergoing testing and feedback can be collected before moving to a more stable stage like BETA, which is ideal for initial development phases.\n\n### Summary of Wrong Options:\n\n**Option 2: BETA Stage**\n- ❌ This is more appropriate for roles that are ready for broader use, not for initial development and testing.\n\n**Options 3 and 4: Testing Level**\n- ❌ Using 'testing' permissions is not advisable for production scenarios and could lead to security issues.\n\n🔗 [Reference: Custom Roles](https://cloud.google.com/iam/docs/creating-custom-roles)"
        },
        {
          "id": 142,
          "question": "Your company has a large quantity of unstructured data in different file formats. You want to perform ETL transformations on the data. You need to make the data accessible on Google Cloud so it can be processed by a Dataflow job. What should you do?",
          "type": "single",
          "options": [
            "Upload the data to BigQuery using the bq command line tool.",
            "Upload the data to Cloud Storage using the gsutil command line tool.",
            "Upload the data into Cloud SQL using the import function in the console.",
            "Upload the data into Cloud Spanner using the import function in the console."
          ],
          "correctAnswer": [
            "Upload the data to Cloud Storage using the gsutil command line tool."
          ],
          "hint": "Think about the optimal storage solution for unstructured data before processing.",
          "explanation": "### Why Use Cloud Storage?\n\n#### Ideal for Unstructured Data\nCloud Storage is designed for scalable storage of various file types and is the best option for storing unstructured data before processing it through Dataflow. It provides flexibility in handling different file formats and is both cost-effective and efficient for ETL processes.\n\n### Summary of Wrong Options:\n\n**Option 1: BigQuery Upload**\n- ❌ BigQuery is more suited for structured data analysis and not optimal for initial storage of unstructured data.\n\n**Options 3 and 4: Cloud SQL and Cloud Spanner**\n- ❌ These databases are not ideal solutions for temporary storage of unstructured files and could complicate the ETL process.\n\n🔗 [Reference: Using Cloud Storage](https://cloud.google.com/storage/docs/)"
        },
        {
          "id": 143,
          "question": "You need to manage multiple Google Cloud projects in the fewest steps possible. You want to configure the Google Cloud SDK command line interface (CLI) so that you can easily manage multiple projects. What should you do?",
          "type": "single",
          "options": [
            "1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects.",
            "1. Create a configuration for each project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project.",
            "1. Use the default configuration for one project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects.",
            "1. Use the default configuration for one project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project."
          ],
          "correctAnswer": [
            "1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects."
          ],
          "hint": "Think about efficiently managing multiple projects without redundant steps.",
          "explanation": "### Why Create Configurations per Project?\n\n#### Streamlined Management\nBy creating a separate configuration for each project, you keep settings and context separate, allowing for quick toggling between projects without the need to remember specific configurations for each session.\n\n### Summary of Wrong Options:\n\n**Options 2 and 4: gcloud init**\n- ❌ While `gcloud init` is useful, it's not suited for regular switching between projects during ongoing work sessions.\n\n**Options 3: Default Configuration**\n- ❌ Relying on the default configuration may lead to confusion and mistakes when working with multiple projects.\n\n🔗 [Reference: Managing Configurations](https://cloud.google.com/sdk/docs/configurations)"
        },
        {
          "id": 144,
          "question": "Your Managed Instance Group raised an alert stating that new instance creation has failed to create new instances. You need to maintain the number of running instances specified by the template to be able to process expected application traffic. What should you do?",
          "type": "single",
          "options": [
            "Create an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names.",
            "Create an instance template that contains valid syntax that will be used by the instance group. Verify that the instance name and persistent disk name values are not the same in the template.",
            "Verify that the instance template being used by the instance group contains valid syntax. Delete any persistent disks with the same name as instance names. Set the disks.autoDelete property to true in the instance template.",
            "Delete the current instance template and replace it with a new instance template. Verify that the instance name and persistent disk name values are not the same in the template. Set the disks.autoDelete property to true in the instance template."
          ],
          "correctAnswer": [
            "Create an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names."
          ],
          "hint": "Think about the requirements of instance templates and persistent disks.",
          "explanation": "### Why Create a Valid Instance Template?\n\n#### Syntax Validation and Naming Conflicts\nCreating an instance template with valid syntax ensures the Managed Instance Group can successfully launch instances. Deleting persistent disks with the same names as instances prevents naming conflicts that lead to failure in instance creation.\n\n### Summary of Wrong Options:\n\n**Option 2: Verify Syntax Only**\n- ❌ Validation alone does not resolve existing naming conflicts with persistent disks.\n\n**Option 3: Modify Existing Template**\n- ❌ It's crucial to avoid potential conflicts instead of modifying an already problematic template.\n\n**Option 4: Replace Template**\n- ❌ While a new template is a solution, it is unnecessary if proper validation and conflict resolution steps are taken in the original template.\n\n🔗 [Reference: Instance Groups](https://cloud.google.com/compute/docs/instance-groups)"
        },
        {
          "id": 145,
          "question": "Your company is moving from an on-premises environment to Google Cloud. You have multiple development teams that use Cassandra environments as backend databases. They all need a development environment that is isolated from other Cassandra instances. You want to move to Google Cloud quickly and with minimal support effort. What should you do?",
          "type": "single",
          "options": [
            "1. Build an instruction guide to install Cassandra on Google Cloud. 2. Make the instruction guide accessible to your developers.",
            "1. Advise your developers to go to Cloud Marketplace. 2. Ask the developers to launch a Cassandra image for their development work.",
            "1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Use the snapshot to create instances for your developers.",
            "1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Upload the snapshot to Cloud Storage and make it accessible to your developers. 3. Build instructions to create a Compute Engine instance from the snapshot so that developers can do it themselves."
          ],
          "correctAnswer": [
            "1. Advise your developers to go to Cloud Marketplace. 2. Ask the developers to launch a Cassandra image for their development work."
          ],
          "hint": "Consider the quickest and least supportive approach for developers to access Cassandra.",
          "explanation": "### Why Use Cloud Marketplace Images?\n\n#### Speed and Isolation\nUsing pre-built Cassandra images from the Cloud Marketplace enables rapid deployment without the need for extensive setup instructions and allows each team to have isolated development environments while minimizing support overheads.\n\n### Summary of Wrong Options:\n\n**Option 1: Instruction Guide**\n- ❌ This approach may slow down implementation, requiring additional troubleshooting and support.\n\n**Options 3 and 4: Custom Instances**\n- ❌ Building and managing custom instances adds complexity and delays when solutions are readily available.\n\n🔗 [Reference: Cloud Marketplace](https://cloud.google.com/marketplace)"
        },
        {
          "id": 146,
          "question": "You have a Compute Engine instance hosting a production application. You want to receive an email if the instance consumes more than 90% of its CPU resources for more than 15 minutes. You want to use Google services. What should you do?",
          "type": "single",
          "options": [
            "1. Create a consumer Gmail account. 2. Write a script that monitors the CPU usage. 3. When the CPU usage exceeds the threshold, have that script send an email using the Gmail account and smtp.gmail.com on port 25 as SMTP server.",
            "1. Create a Stackdriver Workspace, and associate your Google Cloud Platform (GCP) project with it. 2. Create an Alerting Policy in Stackdriver that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel.",
            "1. Create a Stackdriver Workspace, and associate your GCP project with it. 2. Write a script that monitors the CPU usage and sends it as a custom metric to Stackdriver. 3. Create an uptime check for the instance in Stackdriver.",
            "1. In Stackdriver Logging, create a logs-based metric to extract the CPU usage by using this regular expression: CPU Usage: ([0-9]{1,3})% 2. In Stackdriver Monitoring, create an Alerting Policy based on this metric. 3. Configure your email address in the notification channel."
          ],
          "correctAnswer": [
            "1. Create a Stackdriver Workspace, and associate your Google Cloud Platform (GCP) project with it. 2. Create an Alerting Policy in Stackdriver that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel."
          ],
          "hint": "Think about leveraging existing Google services for monitoring and notifications.",
          "explanation": "### Why Use Stackdriver Alerting?\n\n#### Built-in Monitoring Capabilities\nUsing Stackdriver to create an alerting policy directly monitors CPU usage and provides an efficient way to send notifications via emails without custom scripting, making it an optimal solution for production environments.\n\n### Summary of Wrong Options:\n\n**Options 1, 3, and 4: Custom Solutions**\n- ❌ These involve unnecessary complexity and maintenance when the built-in capabilities of Stackdriver Monitoring can handle the requirements seamlessly.\n\n🔗 [Reference: Monitoring and Logging](https://cloud.google.com/monitoring/docs/alerting)"
        },
        {
          "id": 147,
          "question": "You have an application that uses Cloud Spanner as a backend database. The application has a very predictable traffic pattern. You want to automatically scale up or down the number of Spanner nodes depending on traffic. What should you do?",
          "type": "single",
          "options": [
            "Create a cron job that runs on a scheduled basis to review Cloud Monitoring metrics, and then resize the Spanner instance accordingly.",
            "Create a Cloud Monitoring alerting policy to send an alert to oncall SRE emails when Cloud Spanner CPU exceeds the threshold. SREs would scale resources up or down accordingly.",
            "Create a Cloud Monitoring alerting policy to send an alert to Google Cloud Support email when Cloud Spanner CPU exceeds your threshold. Google support would scale resources up or down accordingly.",
            "Create a Cloud Monitoring alerting policy to send an alert to webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly."
          ],
          "correctAnswer": [
            "Create a Cloud Monitoring alerting policy to send an alert to webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly."
          ],
          "hint": "Consider a solution that allows for automated scaling based on traffic patterns.",
          "explanation": "### Why Use Cloud Monitoring with Webhook?\n\n#### Automated Response to Traffic Changes\nSetting up an alerting policy with a webhook combined with a Cloud Function automates the resizing of Cloud Spanner nodes based on traffic patterns without manual intervention, ensuring efficient use of resources.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Manual Interventions**\n- ❌ These approaches require SRE involvement for scaling operations, leading to potential delays and inefficiencies.\n\n**Option 3: Dependency on Support**\n- ❌ Relying on Google Cloud Support for scaling is impractical and does not adhere to agile methodologies.\n\n🔗 [Reference: Automatic Scaling for Spanner](https://cloud.google.com/spanner/docs/scaling)"
        },
        {
          "id": 148,
          "question": "Your company publishes large files on an Apache web server that runs on a Compute Engine instance. The Apache web server is not the only application running in the project. You want to receive an email when the egress network costs for the server exceed 100 dollars for the current month as measured by Google Cloud. What should you do?",
          "type": "single",
          "options": [
            "Set up a budget alert on the project with an amount of 100 dollars, a threshold of 100%, and notification type of email.",
            "Set up a budget alert on the billing account with an amount of 100 dollars, a threshold of 100%, and notification type of email.",
            "Export the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly.",
            "Use the Cloud Logging Agent to export the Apache web server logs to Cloud Logging. Create a Cloud Function that uses BigQuery to parse the HTTP response log data in Cloud Logging for the current month and sends an email if the size of all HTTP responses, multiplied by current Google Cloud egress prices, totals over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly."
          ],
          "correctAnswer": [
            "Export the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly."
          ],
          "hint": "Think about the best way to calculate egress costs directly from billing data.",
          "explanation": "### Why Export Billing Data to BigQuery?\n\n#### Detailed Analysis and Flexibility\nExporting billing data to BigQuery allows for granular analysis and calculations of egress costs, enabling automated tracking of costs associated specifically with the Apache web server on a scheduled basis and highly relevant alerts based on cost thresholds.\n\n### Summary of Wrong Options:\n\n**Options 1 and 2: Simple Budget Alerts**\n- ❌ These alerts may notify you when the budget threshold is hit but do not provide detailed analysis specifically tied to the Apache web server.\n\n**Option 4: Parsing Logs**\n- ❌ This approach introduces unnecessary complexity and overhead as it uses log parsing rather than direct billing analysis.\n\n🔗 [Reference: Monitoring Costs](https://cloud.google.com/billing/docs/how-to/budgets)"
        },
        {
          "id": 149,
          "question": "You have designed a solution on Google Cloud that uses multiple Google Cloud products. Your company has asked you to estimate the costs of the solution. You need to provide estimates for the monthly total cost. What should you do?",
          "type": "single",
          "options": [
            "For each Google Cloud product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product.",
            "For each Google Cloud product in the solution, review the pricing details on the products pricing page. Create a Google Sheet that summarizes the expected monthly costs for each product.",
            "Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Navigate to the Billing Report page in the Cloud Console. Multiply the 1 week cost to determine the monthly costs.",
            "Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Use Cloud Monitoring to determine the provisioned and used resource amounts. Multiply the 1 week cost to determine the monthly costs."
          ],
          "correctAnswer": [
            "For each Google Cloud product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product."
          ],
          "hint": "Think about the most accurate method to estimate costs without incurring extra charges.",
          "explanation": "### Why Use the Pricing Calculator?\n\n#### No Cost for Estimates\nUsing the pricing calculator provides accurate estimates based on current pricing without needing to provision resources and risk incurring charges for unused infrastructure, making it an efficient method for budgeting and cost forecasting.\n\n### Summary of Wrong Options:\n\n**Options 3 and 4: Actual Provisioning**\n- ❌ Provisioning leads to unnecessary costs and does not guarantee accurate projections for future costs.\n\n**Option 2: Google Sheet Summary**\n- ❌ While useful for internal tracking, this approach lacks the accuracy of the pricing calculator's automated calculations and detailed breakdowns.\n\n🔗 [Reference: Pricing Calculator](https://cloud.google.com/products/calculator)"
        },
        {
          "id": 150,
          "question": "You have an application that receives SSL-encrypted TCP traffic on port 443. Clients for this application are located all over the world. You want to minimize latency for the clients. Which load balancing option should you use?",
          "type": "single",
          "options": [
            "HTTPS Load Balancer.",
            "Network Load Balancer.",
            "SSL Proxy Load Balancer.",
            "Internal TCP/UDP Load Balancer. Add a firewall rule allowing ingress traffic from 0.0.0.0/0 on the target instances."
          ],
          "correctAnswer": ["SSL Proxy Load Balancer."],
          "hint": "Consider the best load balancing solution for handling encrypted connections and global clients.",
          "explanation": "### Why Use SSL Proxy Load Balancer?\n\n#### Optimized for SSL Traffic\nThe SSL Proxy Load Balancer is designed specifically for handling SSL-encrypted traffic efficiently and can reduce latency for global clients by terminating SSL at the edge of Google's network, allowing for optimized routing of traffic thereafter, which is crucial for applications receiving TCP traffic over SSL.\n\n### Summary of Wrong Options:\n\n**Option 1: HTTPS Load Balancer**\n- ❌ This is primarily designed for HTTP(S) traffic, potentially adding unnecessary overhead for TCP traffic.\n\n**Option 2: Network Load Balancer**\n- ❌ While effective for TCP, it does not provide the SSL termination benefits necessary for efficient management of SSL-encrypted traffic.\n\n**Option 4: Internal Load Balancer**\n- ❌ Internal load balancing is suited for private traffic and would not be applicable for global client access over the internet. \n\n🔗 [Reference: Load Balancing Options](https://cloud.google.com/load-balancing/docs/overview)"
        }
      ]
    ]
  ]
}
